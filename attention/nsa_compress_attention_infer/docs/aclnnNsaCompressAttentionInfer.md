## aclnnNsaCompressAttentionInfer

[ğŸ“„ æŸ¥çœ‹æºç ](https://gitcode.com/cann/ops-transformer/tree/master/attention/nsa_compress_attention_infer)

## äº§å“æ”¯æŒæƒ…å†µ

|äº§å“      | æ˜¯å¦æ”¯æŒ |
|:----------------------------|:-----------:|
|<term>æ˜‡è…¾910_95 AIå¤„ç†å™¨</term>|      Ã—     |
|<term>Atlas A3 è®­ç»ƒç³»åˆ—äº§å“/Atlas A3 æ¨ç†ç³»åˆ—äº§å“</term>|      âˆš     |
|<term>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“/Atlas 800I A2 æ¨ç†äº§å“/A200I A2 Box å¼‚æ„ç»„ä»¶</term>|      âˆš     |
|<term>Atlas 200I/500 A2 æ¨ç†äº§å“</term>|      Ã—     |
|<term>Atlas æ¨ç†ç³»åˆ—äº§å“</term>|      Ã—     |
|<term>Atlas è®­ç»ƒç³»åˆ—äº§å“</term>|      Ã—     |
|<term>Atlas 200I/300/500 æ¨ç†äº§å“</term>|      Ã—     |

## åŠŸèƒ½è¯´æ˜

- æ¥å£åŠŸèƒ½ï¼šNative Sparse Attentionæ¨ç†è¿‡ç¨‹ä¸­ï¼ŒCompress Attentionçš„è®¡ç®—ã€‚
- è®¡ç®—å…¬å¼ï¼š

<center>

  $$
  P_{cmp}= Softmax(scale * query Â· key^T) \\
  attentionOut = P_{cmp} Â· value\\
  P_{slc}[j] = \sum\limits_{m=0}^{l'/d -1} \sum\limits_{n = 0}^{l/d -1} P_{cmp} [l'/d * j -m - n]\\
  P_{slc'} = \sum\limits_{g=1}^{G}  P_{slc} ^g,\quad 
  \text{å…¶ä¸­ } G = \text{GroupSizeï¼ˆåˆ†ç»„å¤§å°ï¼‰ï¼Œå³ï¼š} G = \frac{\text{numHeads}}{\text{numKeyValueHeads}} \\
  topkIndices = topk(P_{slc'})\\
  $$

</center>

## å‡½æ•°åŸå‹

æ¯ä¸ªç®—å­åˆ†ä¸º[ä¸¤æ®µå¼æ¥å£](../../../docs/zh/context/ä¸¤æ®µå¼æ¥å£.md)ï¼Œå¿…é¡»å…ˆè°ƒç”¨â€œaclnnNsaCompressAttentionInferGetWorkspaceSizeâ€æ¥å£è·å–è®¡ç®—æ‰€éœ€workspaceå¤§å°ä»¥åŠåŒ…å«äº†ç®—å­è®¡ç®—æµç¨‹çš„æ‰§è¡Œå™¨ï¼Œå†è°ƒç”¨â€œaclnnNsaCompressAttentionInferâ€æ¥å£æ‰§è¡Œè®¡ç®—ã€‚

```cpp
aclnnStatus aclnnNsaCompressAttentionInferGetWorkspaceSize(
    const aclTensor    *query,
    const aclTensor    *key,
    const aclTensor    *value,
    const aclTensor    *attentionMaskOptional,
    const aclTensor    *blockTableOptional,
    const aclIntArray  *actualQSeqLenOptional,
    const aclIntArray  *actualCmpKvSeqLenOptional,
    const aclIntArray  *actualSelKvSeqLenOptional,
    const aclTensor    *topKMaskOptional,
    int64_t             numHeads,
    int64_t             numKeyValueHeads,
    int64_t             selectBlockSize,
    int64_t             selectBlockCount,
    int64_t             compressBlockSize,
    int64_t             compressBlockStride,
    double              scaleValue,
    char               *layoutOptional,
    int64_t             pageBlockSize,
    int64_t             sparseMode,
    const aclTensor    *output,
    const aclTensor    *topKOutput,
    uint64_t           *workspaceSize,
    aclOpExecutor     **executor
)
```

```cpp
aclnnStatus aclnnNsaCompressAttentionInfer(
    void          *workspace,
    uint64_t       workspaceSize,
    aclOpExecutor *executor,
    aclrtStream    stream
)
```

## aclnnNsaCompressAttentionInferGetWorkspaceSize

- **å‚æ•°è¯´æ˜**
  
  <table style="undefined;table-layout: fixed; width: 1567px">
  <colgroup>
    <col style="width: 170px">  <!-- å‚æ•°å -->
    <col style="width: 120px">  <!-- è¾“å…¥/è¾“å‡º -->
    <col style="width: 300px">  <!-- æè¿° -->
    <col style="width: 330px">  <!-- ä½¿ç”¨è¯´æ˜ -->
    <col style="width: 212px">  <!-- æ•°æ®ç±»å‹ -->
    <col style="width: 100px">  <!-- æ•°æ®æ ¼å¼ -->
    <col style="width: 190px">  <!-- ç»´åº¦(shape) -->
    <col style="width: 145px">  <!-- éè¿ç»­çš„tensor -->
  </colgroup>
  <thead>
    <tr>
      <th>å‚æ•°å</th>
      <th>è¾“å…¥/è¾“å‡º</th>
      <th>æè¿°</th>
      <th>ä½¿ç”¨è¯´æ˜</th>
      <th>æ•°æ®ç±»å‹</th>
      <th>æ•°æ®æ ¼å¼</th>
      <th>ç»´åº¦(shape)</th>
      <th>éè¿ç»­Tensor</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>query</td>
      <td>è¾“å…¥</td>
      <td>attentionç»“æ„çš„Queryè¾“å…¥ã€‚</td>
      <td>
        <ul style="list-style-type: circle;">
          <li>queryä¸­çš„Bæ˜¯[1, 10000]åŒºé—´å†…çš„æ•´æ•°ï¼Œä¸”ä¸blockTableOptionalä¸­çš„Bä»¥åŠactualCmpKvSeqLenOptionalæ•°ç»„çš„é•¿åº¦ç›¸ç­‰ã€‚</li>
          <li>queryçš„Sè½´å°äºç­‰äº4ã€‚</li>
          <li>queryä¸­çš„Nå’ŒnumHeadså€¼ç›¸ç­‰ï¼Œä¸”Nè½´å¿…é¡»æ˜¯key/valueçš„Nè½´ï¼ˆH/Dï¼‰çš„æ•´æ•°å€ï¼Œæ­¤å¤–ï¼Œqueryçš„Nè½´ä¸key/valueçš„Nè½´ï¼ˆH/Dï¼‰çš„æ¯”å€¼ï¼ˆå³GQAä¸­çš„groupå¤§å°ï¼‰å°äºç­‰äº128ï¼Œä¸”128æ˜¯groupçš„æ•´æ•°å€ã€‚</li>
          <li>queryä¸­çš„Då’Œkeyçš„D(H/numKeyValueHeads)å€¼ç›¸ç­‰ï¼Œå°äºç­‰äº192ä¸”å¤§äºç­‰äºvalueçš„Dè½´ã€‚</li>
          <li>queryï¼Œkeyï¼Œvalueè¾“å…¥çš„æ•°æ®ç±»å‹å®Œå…¨ç›¸åŒï¼Œä¸ºFLOAT16æˆ–BFLOAT16ã€‚</li>
        </ul>
      </td>
      <td>FLOAT16ã€BFLOAT16</td>
      <td>ND</td>
      <td>[BSND]ã€[TND]</td>
      <td>x</td>
    </tr>
    <tr>
      <td>key</td>
      <td>è¾“å…¥</td>
      <td>attentionç»“æ„çš„Keyè¾“å…¥ã€‚</td>
      <td>
        <ul style="list-style-type: circle;">
          <li>keyä¸­çš„numBlockså’Œå‚æ•°valueä¸­çš„numBlockså€¼ç›¸ç­‰ã€‚</li>
          <li>keyä¸­çš„blockSizeå’ŒpageBlockSizeå€¼ç›¸ç­‰ï¼Œä¸”blockSizeå°äºç­‰äº128ï¼Œä¸”æ˜¯16çš„æ•´æ•°å€ã€‚</li>
          <li>keyçš„Sè½´å°äºç­‰äº8192ã€‚</li>
          <li>keyçš„Nå’ŒnumKeyValueHeadså€¼ç›¸ç­‰ã€‚</li>
          <li>keyçš„Dè½´å°äºç­‰äº192ä¸”å¤§äºç­‰äºvalueçš„Dè½´ã€‚</li>
        </ul>
      </td>
      <td>FLOAT16ã€BFLOAT16</td>
      <td>ND</td>
      <td>[numBlocks, blockSize, numKeyValueHeads * headDimsQK]</td>
      <td>x</td>
    </tr>
    <tr>
      <td>value</td>
      <td>è¾“å…¥</td>
      <td>attentionç»“æ„çš„Valueè¾“å…¥ã€‚</td>
      <td>
        <ul style="list-style-type: circle;">
          <li>valueçš„Nå’ŒnumKeyValueHeadså€¼ç›¸ç­‰ã€‚</li>
          <li>valueçš„D(H/numKeyValueHeads)å’Œoutputçš„Då€¼ç›¸ç­‰ã€‚</li>
  	  <li>valueçš„Dè½´å°äºç­‰äº128ã€‚</li>
          <li>valueä¸­çš„blockSizeå’ŒpageBlockSizeå€¼ç›¸ç­‰ã€‚</li>
          <li>valueçš„Sè½´å°äºç­‰äº8192ã€‚</li>
        </ul>
      </td>
      <td>FLOAT16ã€BFLOAT16</td>
      <td>ND</td>
      <td>[numBlocks, blockSize, numKeyValueHeads * headDimsV]</td>
      <td>x</td>
    </tr>
    <tr>
      <td>attentionMaskOptional</td>
      <td>å¯é€‰è¾“å…¥</td>
      <td>attentionæ©ç çŸ©é˜µã€‚</td>
      <td>ä»…åœ¨Q_Så¤§äº1çš„æƒ…å†µä¸‹ç”Ÿæ•ˆ</td>
      <td>-</td>
      <td>ND</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>blockTableOptional</td>
      <td>è¾“å…¥</td>
      <td>paged attentionä¸­KVå­˜å‚¨ä½¿ç”¨çš„blockæ˜ å°„è¡¨ã€‚</td>
      <td>å½“å‰åªæ”¯æŒpaged attentionï¼Œå› æ­¤è¯¥å‚æ•°å¿…é¡»ä¼ å…¥ã€‚</td>
      <td>INT32</td>
      <td>ND</td>
      <td>-</td>
      <td>x</td>
    </tr>
    <tr>
      <td>actualQSeqLenOptional</td>
      <td>å¯é€‰è¾“å…¥</td>
      <td>queryçš„Sè½´å®é™…é•¿åº¦ã€‚</td>
      <td>-</td>
      <td>INT64</td>
      <td>ND</td>
      <td>[B]</td>
      <td>-</td>
    </tr>
    <tr>
      <td>actualCmpKvSeqLenOptional</td>
      <td>å¯é€‰è¾“å…¥</td>
      <td>ç»è¿‡å‹ç¼©åçš„keyå’Œvalueçš„Sè½´å®é™…é•¿åº¦ï¼Œä¹Ÿå³è¯¥ç®—å­å¤„ç†çš„keyå’Œvalueçš„Sè½´å®é™…é•¿åº¦ã€‚</td>
      <td>å½“å‰åªæ”¯æŒpaged attentionï¼Œå› æ­¤è¯¥å‚æ•°å¿…é¡»ä¼ å…¥ã€‚</td>
      <td>INT64</td>
      <td>ND</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>actualSelKvSeqLenOptional</td>
      <td>å¯é€‰è¾“å…¥</td>
      <td>å‹ç¼©å‰çš„keyå’Œvalueçš„Sè½´å®é™…é•¿åº¦ã€‚</td>
      <td>-</td>
      <td>INT64</td>
      <td>ND</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>topKMaskOptional</td>
      <td>å¯é€‰è¾“å…¥</td>
      <td>topKè®¡ç®—ä¸­çš„æ©ç çŸ©é˜µã€‚</td>
      <td>é¢„ç•™å‚æ•°æš‚æœªä½¿ç”¨ã€‚</td>
      <td>-</td>
      <td>ND</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>numHeads</td>
      <td>è¾“å…¥</td>
      <td>headä¸ªæ•°ã€‚</td>
      <td>numHeadsæ˜¯numKeyValueHeadsçš„å€æ•°å…³ç³»ã€‚</td>
      <td>INT64</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>numKeyValueHeads</td>
      <td>è¾“å…¥</td>
      <td>kvHeadä¸ªæ•°ã€‚</td>
      <td>-</td>
      <td>INT64</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>selectBlockSize</td>
      <td>è¾“å…¥</td>
      <td>selecté˜¶æ®µçš„blockå¤§å°ï¼Œåœ¨è®¡ç®—importance scoreæ—¶ä½¿ç”¨ã€‚</td>
      <td>ä»…æ”¯æŒselectBlockSizeå–å€¼16ã€32ã€48ã€64ã€80ã€96ã€112ã€128ï¼Œä¸”selectBlockSizeå¤§äºç­‰äºcompressBlockSizeï¼Œå¹¶ä¸”æ˜¯compressBlockStrideçš„æ•´æ•°å€ã€‚</td>
      <td>INT64</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>selectBlockCount</td>
      <td>è¾“å…¥</td>
      <td>topKé˜¶æ®µéœ€è¦ä¿ç•™çš„blockæ•°é‡ã€‚</td>
      <td>-</td>
      <td>INT64</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>compressBlockSize</td>
      <td>è¾“å…¥</td>
      <td>å‹ç¼©æ—¶çš„æ»‘çª—å¤§å°ã€‚</td>
      <td>ä»…æ”¯æŒcompressBlockSizeå–å€¼16ã€32ã€48ã€64ã€80ã€96ã€112ã€128ï¼Œä¸”éœ€è¦å¤§äºç­‰äºcompressBlockStrideã€‚</td>
      <td>INT64</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>compressBlockStride</td>
      <td>è¾“å…¥</td>
      <td>ä¸¤æ¬¡å‹ç¼©é—´çš„æ»‘çª—é—´éš”å¤§å°ã€‚</td>
      <td>ä»…æ”¯æŒcompressBlockStrideå–å€¼16ã€32ã€48ã€64ã€‚</td>
      <td>INT64</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>scaleValue</td>
      <td>è¾“å…¥</td>
      <td>ç¼©æ”¾ç³»æ•°ï¼Œä½œä¸ºè®¡ç®—æµä¸­Mulsçš„scalarå€¼ã€‚</td>
      <td>-</td>
      <td>DOUBLE</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>layoutOptional</td>
      <td>è¾“å…¥</td>
      <td>è¾“å…¥queryã€keyã€valueçš„æ•°æ®æ’å¸ƒæ ¼å¼ã€‚</td>
      <td>å½“å‰æ”¯æŒå–å€¼â€œTNDâ€å’Œâ€œBSNDâ€ã€‚</td>
      <td>CHAR</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>pageBlockSize</td>
      <td>è¾“å…¥</td>
      <td>blockTableOptionalä¸­ä¸€ä¸ªblockçš„å¤§å°ã€‚</td>
      <td>-</td>
      <td>INT64</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>sparseMode</td>
      <td>è¾“å…¥</td>
      <td>sparseçš„æ¨¡å¼ï¼Œæ§åˆ¶æœ‰attentionMaskOptionalè¾“å…¥æ—¶çš„ç¨€ç–è®¡ç®—ã€‚</td>
      <td>-</td>
      <td>INT64</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>output</td>
      <td>è¾“å‡º</td>
      <td>attentionçš„è¾“å‡ºã€‚</td>
      <td>-</td>
      <td>FLOAT16ã€BFLOAT16</td>
      <td>ND</td>
      <td>[BSND]ã€[TND]</td>
      <td>-</td>
    </tr>
    <tr>
      <td>topKOutput</td>
      <td>è¾“å‡º</td>
      <td>topKçš„è¾“å‡ºã€‚</td>
      <td>-</td>
      <td>INT32</td>
      <td>ND</td>
      <td>[T, N, selectBlockCount]ã€[B, S, N, selectBlockCount]</td>
      <td>-</td>
    </tr>
    <tr>
      <td>workspaceSize</td>
      <td>è¾“å‡º</td>
      <td>è¿”å›éœ€è¦åœ¨Deviceä¾§ç”³è¯·çš„workspaceå¤§å°ã€‚</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>executor</td>
      <td>è¾“å‡º</td>
      <td>è¿”å›opæ‰§è¡Œå™¨ï¼ŒåŒ…å«äº†ç®—å­è®¡ç®—æµç¨‹ã€‚</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
  </tbody>
  
  </table>
- **è¿”å›å€¼**

  aclnnStatusï¼šè¿”å›çŠ¶æ€ç ï¼Œå…·ä½“å‚è§[aclnnè¿”å›ç ](../../../docs/zh/context/aclnnè¿”å›ç .md)ã€‚
  
  ç¬¬ä¸€æ®µæ¥å£å®Œæˆå…¥å‚æ ¡éªŒï¼Œå‡ºç°ä»¥ä¸‹åœºæ™¯æ—¶æŠ¥é”™ï¼š
  
  <table style="undefined;table-layout: fixed; width: 1030px">
  <colgroup>
  <col style="width: 250px">
  <col style="width: 130px">
  <col style="width: 650px">
  </colgroup>
  <thead>
    <tr>
      <th>è¿”å›å€¼</th>
      <th>é”™è¯¯ç </th>
      <th>æè¿°</th>
    </tr></thead>
  <tbody>
    <tr>
      <td>ACLNN_ERR_PARAM_NULLPTR</td>
      <td>161001</td>
      <td>ä¼ å…¥å‚æ•°æ˜¯å¿…é€‰è¾“å…¥ï¼Œè¾“å‡ºæˆ–è€…å¿…é€‰å±æ€§ï¼Œä¸”æ˜¯ç©ºæŒ‡é’ˆã€‚</td>
    </tr>
    <tr>
      <td>ACLNN_ERR_PARAM_INVALID</td>
      <td>161002</td>
      <td>queryã€keyã€valueã€blockTableOptionalã€attentionOutã€topKOutputçš„æ•°æ®ç±»å‹ä¸åœ¨æ”¯æŒçš„èŒƒå›´å†…ã€‚</td>
    </tr>
    <tr>
      <td>ACLNN_ERR_RUNTIME_ERROR</td>
      <td>361001</td>
      <td>APIå†…å­˜è°ƒç”¨npu runtimeçš„æ¥å£å¼‚å¸¸ã€‚</td>
    </tr>
  </tbody>
  </table>

## aclnnNsaCompressAttentionInfer

- **å‚æ•°è¯´æ˜**
  
  <table><thead>
    <tr>
      <th>å‚æ•°å</th>
      <th>è¾“å…¥/è¾“å‡º</th>
      <th>æè¿°</th>
    </tr></thead>
  <tbody>
    <tr>
      <td>workspace</td>
      <td>è¾“å…¥</td>
      <td>åœ¨Deviceä¾§ç”³è¯·çš„workspaceå†…å­˜åœ°å€ã€‚</td>
    </tr>
    <tr>
      <td>workspaceSize</td>
      <td>è¾“å…¥</td>
      <td>åœ¨Deviceä¾§ç”³è¯·çš„workspaceå¤§å°ï¼Œç”±ç¬¬ä¸€æ®µæ¥å£aclnnNsaCompressAttentionInferè·å–ã€‚</td>
    </tr>
    <tr>
      <td>executor</td>
      <td>è¾“å…¥</td>
      <td>opæ‰§è¡Œå™¨ï¼ŒåŒ…å«äº†ç®—å­è®¡ç®—æµç¨‹ã€‚</td>
    </tr>
    <tr>
      <td>stream</td>
      <td>è¾“å…¥</td>
      <td>æŒ‡å®šæ‰§è¡Œä»»åŠ¡çš„Streamã€‚</td>
    </tr>
  </tbody>
  </table>
- **è¿”å›å€¼**

  aclnnStatusï¼šè¿”å›çŠ¶æ€ç ï¼Œå…·ä½“å‚è§[aclnnè¿”å›ç ](../../../docs/zh/context/aclnnè¿”å›ç .md)ã€‚

## çº¦æŸè¯´æ˜

- ç¡®å®šæ€§è®¡ç®—ï¼š
  - aclnnNsaCompressAttentionInferé»˜è®¤ç¡®å®šæ€§å®ç°ã€‚
* å‚æ•°queryä»…æ”¯æŒTNDã€BSNDè¾“å…¥ã€‚Tæ˜¯Bå’ŒSåˆè½´ç´§å¯†æ’åˆ—çš„æ•°æ®ï¼ˆæ¯ä¸ªbatchçš„actualQSeqLenOptionalï¼‰ã€Bï¼ˆbatchï¼‰è¡¨ç¤ºè¾“å…¥æ ·æœ¬æ‰¹é‡å¤§å°ã€Sï¼ˆqSeqlenï¼‰è¡¨ç¤ºè¾“å…¥æ ·æœ¬åºåˆ—é•¿åº¦ã€Nï¼ˆnumHeadsï¼‰è¡¨ç¤ºå¤šå¤´æ•°ã€Dï¼ˆheadDimsQKï¼‰è¡¨ç¤ºéšè—å±‚æœ€å°çš„å•å…ƒå°ºå¯¸ã€‚
* å‹ç¼©å‰çš„kvSeqlençš„ä¸Šé™å¯ä»¥è¡¨ç¤ºä¸ºï¼šactualSelKvSeqLenCeil=(actualCmpKvSeqLenOptional-1)*compressBlockStride+compressBlockSizeï¼Œéœ€è¦æ»¡è¶³actualSelKvSeqLenCeil/selectBlockSize<=4096ï¼Œä¸”éœ€è¦æ»¡è¶³selectBlockCount<=actualSelKvSeqLenCeil/selectBlockSizeã€‚å¦‚æœactualSelKvSeqLenOptionalä¸æ»¡è¶³actualCmpKvSeqLenOptional=(actualSelKvSeqLenOptional-compressBlockSize)/compressBlockStride+1ï¼Œæˆ–è€…actualCmpKvSeqLenOptionalçš„é•¿åº¦å’ŒblockTableOptionalçš„batchç»´åº¦ä¸åŒï¼Œåˆ™ä¼šé»˜è®¤è¿›å…¥å•tokenæ¨ç†åœºæ™¯ã€‚
* å¤štokenæ¨ç†åœºæ™¯ä¸‹ï¼ŒactualQSeqLenOptionalå‚æ•°å¿…é¡»ä¼ å…¥ï¼ŒactualQSeqLenOptionalçš„é•¿åº¦å¿…é¡»å’ŒblockTableOptionalçš„batchç»´åº¦ç›¸ç­‰ï¼Œä»…æ”¯æŒqueryçš„Sè½´æœ€å¤§ç­‰äº4ï¼Œå¹¶ä¸”æ­¤æ—¶è¦æ±‚æ¯ä¸ªbatchå•ç‹¬çš„actualQSeqLenOptional<=actualSelKvSeqLenOptionalã€‚å¦‚æœactualQSeqLenOptionalçš„é•¿åº¦å’ŒblockTableOptionalçš„batchç»´åº¦ä¸åŒï¼Œæˆ–è€…actualQSeqLenOptionalçš„å€¼å°äº1æˆ–è€…å¤§äº4ï¼Œåˆ™ä¼šé»˜è®¤è¿›å…¥å•tokenæ¨ç†åœºæ™¯ã€‚

## è°ƒç”¨ç¤ºä¾‹

ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼Œä»…ä¾›å‚è€ƒï¼Œå…·ä½“ç¼–è¯‘å’Œæ‰§è¡Œè¿‡ç¨‹è¯·å‚è€ƒ[ç¼–è¯‘ä¸è¿è¡Œæ ·ä¾‹](../../../docs/zh/context/ç¼–è¯‘ä¸è¿è¡Œæ ·ä¾‹.md)ã€‚

```c++
#include <iostream>
#include <vector>
#include <cstring>
#include "acl/acl.h"
#include "aclnn/opdev/fp16_t.h"
#include "aclnnop/aclnn_nsa_compress_attention_infer.h"

using namespace std;

#define CHECK_RET(cond, return_expr) \
  do {                               \
    if (!(cond)) {                   \
    return_expr;                   \
    }                                \
  } while (0)

#define LOG_PRINT(message, ...)     \
  do {                              \
    printf(message, ##__VA_ARGS__); \
  } while (0)

int64_t GetShapeSize(const std::vector<int64_t>& shape) {
  int64_t shapeSize = 1;
  for (auto i : shape) {
    shapeSize *= i;
  }
  return shapeSize;
}

int Init(int32_t deviceId, aclrtStream* stream) {
  // å›ºå®šå†™æ³•ï¼Œèµ„æºåˆå§‹åŒ–
  auto ret = aclInit(nullptr);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclInit failed. ERROR: %d\n", ret); return ret);
  ret = aclrtSetDevice(deviceId);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtSetDevice failed. ERROR: %d\n", ret); return ret);
  ret = aclrtCreateStream(stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtCreateStream failed. ERROR: %d\n", ret); return ret);
  return 0;
}

template <typename T>
int CreateAclTensor(const std::vector<T>& hostData, const std::vector<int64_t>& shape, void** deviceAddr,
                    aclDataType dataType, aclTensor** tensor) {
  auto size = GetShapeSize(shape) * sizeof(T);
  // è°ƒç”¨aclrtMallocç”³è¯·deviceä¾§å†…å­˜
  auto ret = aclrtMalloc(deviceAddr, size, ACL_MEM_MALLOC_HUGE_FIRST);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtMalloc failed. ERROR: %d\n", ret); return ret);
  // è°ƒç”¨aclrtMemcpyå°†hostä¾§æ•°æ®æ‹·è´åˆ°deviceä¾§å†…å­˜ä¸Š
  ret = aclrtMemcpy(*deviceAddr, size, hostData.data(), size, ACL_MEMCPY_HOST_TO_DEVICE);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtMemcpy failed. ERROR: %d\n", ret); return ret);

  // è®¡ç®—è¿ç»­tensorçš„strides
  std::vector<int64_t> strides(shape.size(), 1);
  for (int64_t i = shape.size() - 2; i >= 0; i--) {
    strides[i] = shape[i + 1] * strides[i + 1];
  }

  // è°ƒç”¨aclCreateTensoræ¥å£åˆ›å»ºaclTensor
  *tensor = aclCreateTensor(shape.data(), shape.size(), dataType, strides.data(), 0, aclFormat::ACL_FORMAT_ND,
                shape.data(), shape.size(), *deviceAddr);
  return 0;
}

int main() {
  // 1. ï¼ˆå›ºå®šå†™æ³•ï¼‰device/streamåˆå§‹åŒ–ï¼Œå‚è€ƒAscendCLå¯¹å¤–æ¥å£åˆ—è¡¨
  // æ ¹æ®è‡ªå·±çš„å®é™…deviceå¡«å†™deviceId
  int32_t deviceId = 0;
  aclrtStream stream;
  auto ret = Init(deviceId, &stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("Init acl failed. ERROR: %d\n", ret); return ret);

  // 2. æ„é€ è¾“å…¥ä¸è¾“å‡ºï¼Œéœ€è¦æ ¹æ®APIçš„æ¥å£è‡ªå®šä¹‰æ„é€ 
  int32_t batchSize = 20;
  int32_t headDimsQK = 192;
  int32_t blockNum = 640;
  int32_t headDimsV = 128;
  int32_t sequenceLengthK = 4096;
  int32_t maxNumBlocksPerSeq = 32;
  // attr
    int64_t numHeads = 64;
    int64_t numKeyValueHeads = 4;
    int64_t selectBlockSize = 64;
    int64_t selectBlockCount = 16;
    int64_t compressBlockSize = 32;
    int64_t compressStride = 16;
    double scaleValue = 0.088388;
  string sLayerOut = "TND";
  char layOut[sLayerOut.length()];
  strcpy(layOut, sLayerOut.c_str());
    int64_t pageBlockSize = 128;
    int64_t sparseMod = 0;
  std::vector<int64_t> queryShape = {batchSize, numHeads, headDimsQK};
  std::vector<int64_t> keyShape = {blockNum, pageBlockSize, numKeyValueHeads * headDimsQK};
  std::vector<int64_t> valueShape = {blockNum, pageBlockSize, numKeyValueHeads * headDimsV};
  std::vector<int64_t> blockTableOptionalShape = {batchSize, maxNumBlocksPerSeq};
    std::vector<int64_t> outputShape = {batchSize, numHeads, headDimsV};
    std::vector<int64_t> topkIndicesShape = {batchSize, numKeyValueHeads, selectBlockCount};
  void *queryDeviceAddr = nullptr;
  void *keyDeviceAddr = nullptr;
  void *valueDeviceAddr = nullptr;
  void *blockTableOptionalDeviceAddr = nullptr;
  void *outputDeviceAddr = nullptr;
  void *topkIndicesDeviceAddr = nullptr;
  aclTensor *queryTensor = nullptr;
  aclTensor *keyTensor = nullptr;
  aclTensor *valueTensor = nullptr;
  aclTensor *blockTableOptionalTensor = nullptr;
  aclTensor *outputTensor = nullptr;
  aclTensor *topkIndicesTensor = nullptr;
  std::vector<op::fp16_t> queryHostData(batchSize * numHeads * headDimsQK, 1.0);
  std::vector<op::fp16_t> keyHostData(blockNum * pageBlockSize * numKeyValueHeads * headDimsQK, 1.0);
  std::vector<op::fp16_t> valueHostData(blockNum * pageBlockSize * numKeyValueHeads * headDimsV, 1.0);
  std::vector<int32_t> blockTableOptionalHostData(batchSize * maxNumBlocksPerSeq, 1);
  std::vector<op::fp16_t> outputHostData(batchSize * numHeads * headDimsV, 1.0);
  std::vector<int32_t> topkIndicesHostData(batchSize * numKeyValueHeads * selectBlockCount, 1);

  // åˆ›å»ºquery aclTensor
  ret = CreateAclTensor(queryHostData, queryShape, &queryDeviceAddr, aclDataType::ACL_FLOAT16, &queryTensor);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  // åˆ›å»ºkey aclTensor
  ret = CreateAclTensor(keyHostData, keyShape, &keyDeviceAddr, aclDataType::ACL_FLOAT16, &keyTensor);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  // åˆ›å»ºv aclTensor
  ret = CreateAclTensor(valueHostData, valueShape, &valueDeviceAddr, aclDataType::ACL_FLOAT16, &valueTensor);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  // åˆ›å»ºblockTableOptional aclTensor
  ret = CreateAclTensor(blockTableOptionalHostData, blockTableOptionalShape, &blockTableOptionalDeviceAddr, aclDataType::ACL_INT32, &blockTableOptionalTensor);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  // åˆ›å»ºoutput aclTensor
  ret = CreateAclTensor(outputHostData, outputShape, &outputDeviceAddr, aclDataType::ACL_FLOAT16, &outputTensor);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  // åˆ›å»ºtopkIndices aclTensor
  ret = CreateAclTensor(topkIndicesHostData, topkIndicesShape, &topkIndicesDeviceAddr, aclDataType::ACL_INT32, &topkIndicesTensor);
  CHECK_RET(ret == ACL_SUCCESS, return ret);

    std::vector<int64_t> actualCmpKvSeqLenVector(batchSize, sequenceLengthK);
    auto actualCmpKvSeqLen = aclCreateIntArray(actualCmpKvSeqLenVector.data(), actualCmpKvSeqLenVector.size());

  // 3. è°ƒç”¨CANNç®—å­åº“API
  uint64_t workspaceSize = 0;
  aclOpExecutor* executor;
  // è°ƒç”¨ç¬¬ä¸€æ®µæ¥å£
  ret = aclnnNsaCompressAttentionInferGetWorkspaceSize(queryTensor, keyTensor, valueTensor, nullptr, blockTableOptionalTensor, nullptr, actualCmpKvSeqLen,
        nullptr, nullptr,
        numHeads, numKeyValueHeads, selectBlockSize, selectBlockCount, compressBlockSize, compressStride,
        scaleValue, layOut, pageBlockSize, sparseMod, outputTensor, topkIndicesTensor, &workspaceSize, &executor);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclnnNsaCompressAttentionInferGetWorkspaceSize failed. ERROR: %d\n", ret); return ret);
  // æ ¹æ®ç¬¬ä¸€æ®µæ¥å£è®¡ç®—å‡ºçš„workspaceSizeç”³è¯·deviceå†…å­˜
  void* workspaceAddr = nullptr;
  if (workspaceSize > 0) {
    ret = aclrtMalloc(&workspaceAddr, workspaceSize, ACL_MEM_MALLOC_HUGE_FIRST);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("allocate workspace failed. ERROR: %d\n", ret); return ret);
  }
  // è°ƒç”¨ç¬¬äºŒæ®µæ¥å£
  ret = aclnnNsaCompressAttentionInfer(workspaceAddr, workspaceSize, executor, stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclnnNsaCompressAttentionInfer failed. ERROR: %d\n", ret); return ret);

  // 4. ï¼ˆå›ºå®šå†™æ³•ï¼‰åŒæ­¥ç­‰å¾…ä»»åŠ¡æ‰§è¡Œç»“æŸ
  ret = aclrtSynchronizeStream(stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtSynchronizeStream failed. ERROR: %d\n", ret); return ret);

  // 5. è·å–è¾“å‡ºçš„å€¼ï¼Œå°†deviceä¾§å†…å­˜ä¸Šçš„ç»“æœæ‹·è´è‡³hostä¾§ï¼Œéœ€è¦æ ¹æ®å…·ä½“APIçš„æ¥å£å®šä¹‰ä¿®æ”¹
  auto size = GetShapeSize(outputShape);
  std::vector<op::fp16_t> resultData(size, 0);
  ret = aclrtMemcpy(resultData.data(), resultData.size() * sizeof(resultData[0]), outputDeviceAddr,
            size * sizeof(resultData[0]), ACL_MEMCPY_DEVICE_TO_HOST);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("copy [attn] result from device to host failed. ERROR: %d\n", ret); return ret);
  uint64_t printNum = 10;
  for (int64_t i = 0; i < printNum; i++) {
    std::cout << "index: " << i << ": " << static_cast<float>(resultData[i]) << std::endl;
  }
    auto topksize = GetShapeSize(topkIndicesShape);
  std::vector<op::fp16_t> topkresultData(topksize, 0);
  ret = aclrtMemcpy(topkresultData.data(), topkresultData.size() * sizeof(topkresultData[0]), topkIndicesDeviceAddr,
            topksize * sizeof(topkresultData[0]), ACL_MEMCPY_DEVICE_TO_HOST);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("copy [top k] result from device to host failed. ERROR: %d\n", ret); return ret);
  for (int64_t i = 0; i < printNum; i++) {
    std::cout << "topk index: " << i << ": " << static_cast<int32_t>(topkresultData[i]) << std::endl;
  }

  // 6. é‡Šæ”¾èµ„æº
  aclDestroyTensor(queryTensor);
  aclDestroyTensor(keyTensor);
  aclDestroyTensor(valueTensor);
  aclDestroyTensor(blockTableOptionalTensor);
  aclDestroyIntArray(actualCmpKvSeqLen);
  aclDestroyTensor(outputTensor);
  aclDestroyTensor(topkIndicesTensor);
  aclrtFree(queryDeviceAddr);
  aclrtFree(keyDeviceAddr);
  aclrtFree(valueDeviceAddr);
  aclrtFree(blockTableOptionalDeviceAddr);
  aclrtFree(outputDeviceAddr);
  aclrtFree(topkIndicesDeviceAddr);
  if (workspaceSize > 0) {
    aclrtFree(workspaceAddr);
  }
  aclrtDestroyStream(stream);
  aclrtResetDevice(deviceId);
  aclFinalize();
  return 0;
}
```
