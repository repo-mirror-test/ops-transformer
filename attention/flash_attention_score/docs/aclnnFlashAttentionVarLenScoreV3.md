# aclnnFlashAttentionVarLenScoreV3

[ğŸ“„ æŸ¥çœ‹æºç ](https://gitcode.com/cann/ops-transformer/tree/master/attention/flash_attention_score)

## äº§å“æ”¯æŒæƒ…å†µ

|äº§å“      | æ˜¯å¦æ”¯æŒ |
|:----------------------------|:-----------:|
|<term>Atlas A3 è®­ç»ƒç³»åˆ—äº§å“/Atlas A3 æ¨ç†ç³»åˆ—äº§å“</term>|      âˆš     |
|<term>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</term>|      âˆš     |
|<term>Atlas A2 æ¨ç†ç³»åˆ—äº§å“</term>|      Ã—     |


## åŠŸèƒ½è¯´æ˜

- æ¥å£åŠŸèƒ½ï¼šè®­ç»ƒåœºæ™¯ä¸‹ï¼Œä½¿ç”¨FlashAttentionç®—æ³•å®ç°self-attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰çš„è®¡ç®—ã€‚è·Ÿ[aclnnFlashAttentionVarLenScoreV2](./aclnnFlashAttentionVarLenScoreV2.md)çš„åŒºåˆ«æ˜¯è¯¥æ¥å£æ”¯æŒquery/keyå¤šè¾“å…¥ï¼Œå³queryã€queryRopeã€keyå’ŒkeyRopeä½œä¸ºè¾“å…¥ã€‚éå¤šè¾“å…¥åœºæ™¯ä½¿ç”¨[aclnnFlashAttentionVarLenScoreV2](./aclnnFlashAttentionVarLenScoreV2.md)æˆ–å…¶ä»–æ¥å£ã€‚
- è®¡ç®—å…¬å¼ï¼š

   æ³¨æ„åŠ›çš„æ­£å‘è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

     $$
     attention\_out=Dropout(Softmax(Mask(scale*(query*key^T + queryRope*keyRope^T) + pse),atten\_mask),keep\_prob)*value
     $$


## å‡½æ•°åŸå‹

æ¯ä¸ªç®—å­åˆ†ä¸º[ä¸¤æ®µå¼æ¥å£](../../../docs/zh/context/ä¸¤æ®µå¼æ¥å£.md)ï¼Œå¿…é¡»å…ˆè°ƒç”¨â€œaclnnFlashAttentionVarLenScoreV3GetWorkspaceSizeâ€æ¥å£è·å–è®¡ç®—æ‰€éœ€workspaceå¤§å°ä»¥åŠåŒ…å«äº†ç®—å­è®¡ç®—æµç¨‹çš„æ‰§è¡Œå™¨ï¼Œå†è°ƒç”¨â€œaclnnFlashAttentionVarLenScoreV3â€æ¥å£æ‰§è¡Œè®¡ç®—ã€‚

```c++
aclnnStatus aclnnFlashAttentionVarLenScoreV3GetWorkspaceSize(
  const aclTensor   *query,
  const aclTensor   *queryRope,
  const aclTensor   *key,
  const aclTensor   *keyRope,
  const aclTensor   *value,
  const aclTensor   *realShiftOptional,
  const aclTensor   *dropMaskOptional,
  const aclTensor   *paddingMaskOptional,
  const aclTensor   *attenMaskOptional,
  const aclIntArray *prefixOptional,
  const aclIntArray *actualSeqQLenOptional,
  const aclIntArray *actualSeqKvLenOptional,
  const aclIntArray *qStartIdxOptional,
  const aclIntArray *kvStartIdxOptional,
  double             scaleValue,
  double             keepProb,
  int64_t            preTokens,
  int64_t            nextTokens,
  int64_t            headNum,
  char              *inputLayout,
  int64_t            innerPrecise,
  int64_t            sparseMode,
  int64_t            pseType,
  const aclTensor   *softmaxMaxOut,
  const aclTensor   *softmaxSumOut,
  const aclTensor   *softmaxOutOut,
  const aclTensor   *attentionOutOut,
  uint64_t          *workspaceSize,
  aclOpExecutor    **executor)
```
```c++

aclnnStatus aclnnFlashAttentionVarLenScoreV3(
  void              *workspace,
  uint64_t           workspaceSize,
  aclOpExecutor     *executor,
  const aclrtStream  stream)
```


## aclnnFlashAttentionVarLenScoreV3GetWorkspaceSize

- **å‚æ•°è¯´æ˜ï¼š**
  <table style="undefined;table-layout: fixed; width: 1452px"><colgroup>
    <col style="width: 174px">
    <col style="width: 121px">
    <col style="width: 253px">
    <col style="width: 262px">
    <col style="width: 213px">
    <col style="width: 115px">
    <col style="width: 169px">
    <col style="width: 145px">
    </colgroup>
    <thead>
      <tr>
        <th>å‚æ•°å</th>
        <th>è¾“å…¥/è¾“å‡º</th>
        <th>æè¿°</th>
        <th>ä½¿ç”¨è¯´æ˜</th>
        <th>æ•°æ®ç±»å‹</th>
        <th>æ•°æ®æ ¼å¼</th>
        <th>ç»´åº¦(shape)</th>
        <th>éè¿ç»­Tensor</th>
      </tr></thead>
    <tbody>
      <tr>
        <td>query</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„queryã€‚</td>
        <td>æ•°æ®ç±»å‹ä¸key/valueçš„æ•°æ®ç±»å‹ä¸€è‡´ã€‚</td>
        <td>BFLOAT16</td>
        <td>ND</td>
        <td>[TND]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>queryRope</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„queryRopeã€‚</td>
        <td>æ•°æ®ç±»å‹ä¸key/valueçš„æ•°æ®ç±»å‹ä¸€è‡´ã€‚</td>
        <td>BFLOAT16</td>
        <td>ND</td>
        <td>[TND]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>key</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„keyã€‚</td>
        <td>æ•°æ®ç±»å‹ä¸query/valueçš„æ•°æ®ç±»å‹ä¸€è‡´ã€‚</td>
        <td>BFLOAT16</td>
        <td>ND</td>
        <td>[TND]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>keyRope</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„keyRopeã€‚</td>
        <td>æ•°æ®ç±»å‹ä¸query/valueçš„æ•°æ®ç±»å‹ä¸€è‡´ã€‚</td>
        <td>BFLOAT16</td>
        <td>ND</td>
        <td>[TND]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>value</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„valueã€‚</td>
        <td>æ•°æ®ç±»å‹ä¸query/keyçš„æ•°æ®ç±»å‹ä¸€è‡´ã€‚</td>
        <td>BFLOAT16</td>
        <td>ND</td>
        <td>[TND]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>realShiftOptional</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„pseã€‚</td>
        <td>å¿…é¡»ä¸ºnullptrã€‚</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>dropMaskOptional</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„Dropoutã€‚</td>
        <td>å¿…é¡»ä¸ºnullptrã€‚</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>attenMaskOptional</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„atten_maskã€‚</td>
        <td>å–å€¼ä¸º1ä»£è¡¨è¯¥ä½ä¸å‚ä¸è®¡ç®—ï¼Œä¸º0ä»£è¡¨è¯¥ä½å‚ä¸è®¡ç®—ã€‚</td>
        <td>BOOLã€UINT8</td>
        <td>ND</td>
        <td>[B,N,Sq,Skv]ã€[B,1,Sq,Skv]ã€[1,1,Sq,Skv]ã€[Sq,Skv] </td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>prefixOptional</td>
        <td>è¾“å…¥</td>
        <td>ä»£è¡¨prefixç¨€ç–è®¡ç®—åœºæ™¯æ¯ä¸ªBatchçš„Nå€¼ã€‚</td>
        <td>-</td>
        <td>INT64</td>
        <td>ND</td>
        <td>0ã€1</td>
        <td>-</td>
      </tr>
      <tr>
        <td>actualSeqQLenOptional</td>
        <td>è¾“å…¥</td>
        <td>æè¿°äº†æ¯ä¸ªBatchå¯¹åº”çš„queryçš„sequence lengthã€‚</td>
        <td>-</td>
        <td>INT64</td>
        <td>ND</td>
        <td>0ã€1</td>
        <td>-</td>
      </tr>
      <tr>
        <td>actualSeqKvLenOptional</td>
        <td>è¾“å…¥</td>
        <td>æè¿°äº†æ¯ä¸ªBatchå¯¹åº”çš„key/valueçš„sequence lengthã€‚</td>
        <td>-</td>
        <td>INT64</td>
        <td>ND</td>
        <td>0ã€1</td>
        <td>-</td>
      </tr>
      <tr>
        <td>qStartIdxOptional</td>
        <td>è¾“å…¥</td>
        <td>ä»£è¡¨å¤–åˆ‡åœºæ™¯ï¼Œå½“å‰åˆ†å—çš„queryçš„sequenceåœ¨å…¨å±€ä¸­çš„èµ·å§‹ç´¢å¼•ã€‚</td>
        <td>-</td>
        <td>INT64</td>
        <td>ND</td>
        <td>0ã€1</td>
        <td>-</td>
      </tr>
      <tr>
        <td>kvStartIdxOptional</td>
        <td>è¾“å…¥</td>
        <td>ä»£è¡¨å¤–åˆ‡åœºæ™¯ï¼Œå½“å‰åˆ†å—çš„queryçš„sequenceåœ¨å…¨å±€ä¸­çš„èµ·å§‹ç´¢å¼•ã€‚</td>
        <td>-</td>
        <td>INT64</td>
        <td>ND</td>
        <td>0ã€1</td>
        <td>-</td>
      </tr>
      <tr>
        <td>scaleValue</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„scaleï¼Œä»£è¡¨ç¼©æ”¾ç³»æ•°ã€‚</td>
        <td>-</td>
        <td>DOUBLE</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>keepProb</td>
        <td>è¾“å…¥</td>
        <td>ä»£è¡¨dropMaskOptionalä¸­1çš„æ¯”ä¾‹ã€‚</td>
        <td>-</td>
        <td>DOUBLE</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>preTokens</td>
        <td>è¾“å…¥</td>
        <td>ç”¨äºç¨€ç–è®¡ç®— ï¼Œè¡¨ç¤ºslides windowçš„å·¦è¾¹ç•Œã€‚</td>
        <td>-</td>
        <td>INT64</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>nextTokens</td>
        <td>è¾“å…¥</td>
        <td>ç”¨äºç¨€ç–è®¡ç®—ï¼Œè¡¨ç¤ºslides windowçš„å³è¾¹ç•Œã€‚</td>
        <td>-</td>
        <td>INT64</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>headNum</td>
        <td>è¾“å…¥</td>
        <td>ä»£è¡¨å•å¡çš„headä¸ªæ•°ï¼Œå³è¾“å…¥queryçš„Nè½´é•¿åº¦ã€‚</td>
        <td>-</td>
        <td>INT64</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>inputLayout</td>
        <td>è¾“å…¥</td>
        <td>ä»£è¡¨è¾“å…¥queryã€keyã€valueçš„æ•°æ®æ’å¸ƒæ ¼å¼ã€‚</td>
        <td>æ”¯æŒTNDã€‚</td>
        <td>String</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>innerPrecise</td>
        <td>è¾“å…¥</td>
        <td>ç”¨äºæå‡ç²¾åº¦ã€‚</td>
        <td>æš‚æœªä½¿ç”¨ã€‚</td>
        <td>INT64</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>sparseMode</td>
        <td>è¾“å…¥</td>
        <td>è¡¨ç¤ºsparseçš„æ¨¡å¼ã€‚</td>
        <td>æ”¯æŒé…ç½®å€¼ä¸º0ã€1ã€2ã€3ã€4ã€7ã€8ã€‚</td>
        <td>INT64</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>pseType</td>
        <td>è¾“å…¥</td>
        <td>æ§åˆ¶mulä¸addè®¡ç®—é¡ºåºï¼Œä»…æ”¯æŒé…ç½®å€¼ä¸º1ã€‚</td>
        <td>-</td>
        <td>INT64</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>softmaxMaxOut</td>
        <td>è¾“å‡º</td>
        <td>Softmaxè®¡ç®—çš„Maxä¸­é—´ç»“æœï¼Œç”¨äºåå‘è®¡ç®—ã€‚</td>
        <td>-</td>
        <td>FLOAT</td>
        <td>ND</td>
        <td>[N,T,8]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>softmaxSumOut</td>
        <td>è¾“å‡º</td>
        <td>Softmaxè®¡ç®—çš„Sumä¸­é—´ç»“æœï¼Œç”¨äºåå‘è®¡ç®—ã€‚</td>
        <td>-</td>
        <td>FLOAT</td>
        <td>ND</td>
        <td>[N,T,8]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>attentionOutOut</td>
        <td>è¾“å‡º</td>
        <td>è®¡ç®—å…¬å¼çš„æœ€ç»ˆè¾“å‡ºã€‚</td>
        <td>æ•°æ®ç±»å‹å’Œshapeç±»å‹ä¸queryä¿æŒä¸€è‡´ã€‚</td>
        <td>BFLOAT16</td>
        <td>ND</td>
        <td>[TND]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>workspaceSize</td>
        <td>è¾“å‡º</td>
        <td>è¿”å›éœ€è¦åœ¨Deviceä¾§ç”³è¯·çš„workspaceå¤§å°ã€‚</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>executor</td>
        <td>è¾“å‡º</td>
        <td>è¿”å›opæ‰§è¡Œå™¨ï¼ŒåŒ…å«äº†ç®—å­è®¡ç®—æµç¨‹ã€‚</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
    </tbody>
  </table>  
  
- **è¿”å›å€¼ï¼š**

  aclnnStatusï¼šè¿”å›çŠ¶æ€ç ï¼Œå…·ä½“å‚è§[aclnnè¿”å›ç ](../../../docs/zh/context/aclnnè¿”å›ç .md)ã€‚

  ç¬¬ä¸€æ®µæ¥å£å®Œæˆå…¥å‚æ ¡éªŒï¼Œå‡ºç°ä»¥ä¸‹åœºæ™¯æ—¶æŠ¥é”™ï¼š
  <table style="undefined;table-layout: fixed;width: 1202px"><colgroup>
  <col style="width: 262px">
  <col style="width: 121px">
  <col style="width: 819px">
  </colgroup>
  <thead>
    <tr>
      <th>è¿”å›ç </th>
      <th>é”™è¯¯ç </th>
      <th>æè¿°</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ACLNN_ERR_PARAM_NULLPTR</td>
      <td>161001</td>
      <td>ä¼ å…¥å‚æ•°æ˜¯å¿…é€‰è¾“å…¥ï¼Œè¾“å‡ºæˆ–è€…å¿…é€‰å±æ€§ï¼Œä¸”æ˜¯ç©ºæŒ‡é’ˆã€‚</td>
    </tr>
    <tr>
      <td rowspan="2">ACLNN_ERR_PARAM_INVALID</td>
      <td rowspan="2">161002</td>
      <td>queryã€queryRopeã€keyã€keyRopeã€valueã€realShiftOptionalã€dropMaskOptionalã€paddingMaskOptionalã€attenMaskOptionalã€softmaxMaxOutã€softmaxSumOutã€softmaxOutOutã€attentionOutOutçš„æ•°æ®ç±»å‹ä¸åœ¨æ”¯æŒçš„èŒƒå›´å†…ã€‚</td>
    </tr>
    <tr>
      <td>queryã€queryRopeã€keyã€keyRopeã€valueã€realShiftOptionalã€dropMaskOptionalã€paddingMaskOptionalã€attenMaskOptionalã€softmaxMaxOutã€softmaxSumOutã€softmaxOutOutã€attentionOutOutçš„æ•°æ®æ ¼å¼ä¸åœ¨æ”¯æŒçš„èŒƒå›´å†…ã€‚</td>
    </tr>
  </tbody>
  </table>

## aclnnFlashAttentionVarLenScoreV3

-   **å‚æ•°è¯´æ˜ï¼š**
    <table style="undefined;table-layout: fixed; width: 1154px"><colgroup>
    <col style="width: 153px">
    <col style="width: 121px">
    <col style="width: 880px">
    </colgroup>
    <thead>
      <tr>
        <th>å‚æ•°å</th>
        <th>è¾“å…¥/è¾“å‡º</th>
        <th>æè¿°</th>
      </tr></thead>
    <tbody>
      <tr>
        <td>workspace</td>
        <td>è¾“å…¥</td>
        <td>åœ¨Deviceä¾§ç”³è¯·çš„workspaceå†…å­˜åœ°å€ã€‚</td>
      </tr>
      <tr>
        <td>workspaceSize</td>
        <td>è¾“å…¥</td>
        <td>åœ¨Deviceä¾§ç”³è¯·çš„workspaceå¤§å°ï¼Œç”±ç¬¬ä¸€æ®µæ¥å£aclnnFlashAttentionVarLenScoreV3GetWorkspaceSizeè·å–ã€‚</td>
      </tr>
      <tr>
        <td>executor</td>
        <td>è¾“å…¥</td>
        <td>opæ‰§è¡Œå™¨ï¼ŒåŒ…å«äº†ç®—å­è®¡ç®—æµç¨‹ã€‚</td>
      </tr>
      <tr>
        <td>stream</td>
        <td>è¾“å…¥</td>
        <td>æŒ‡å®šæ‰§è¡Œä»»åŠ¡çš„Streamã€‚</td>
      </tr>
    </tbody>
    </table>

-   **è¿”å›å€¼ï¼š**

    è¿”å›aclnnStatusçŠ¶æ€ç ï¼Œå…·ä½“å‚è§[aclnnè¿”å›ç ](../../../docs/zh/context/aclnnè¿”å›ç .md)ã€‚

## çº¦æŸè¯´æ˜

- ç¡®å®šæ€§è®¡ç®—ï¼š
  - aclnnFlashAttentionVarLenScoreV3é»˜è®¤ç¡®å®šæ€§å®ç°ã€‚
- è¯¥æ¥å£ä¸PyTorché…åˆä½¿ç”¨æ—¶ï¼Œéœ€è¦ä¿è¯CANNç›¸å…³åŒ…ä¸PyTorchç›¸å…³åŒ…çš„ç‰ˆæœ¬åŒ¹é…
- è¾“å…¥queryã€queryRopeã€keyã€keyRopeã€valueçš„Bï¼šbatchsizeå¿…é¡»ç›¸ç­‰ã€‚
- è¾“å…¥queryã€keyã€valueçš„Dï¼šHead-Dimå¿…é¡»æ»¡è¶³(qD == kD && kD >= vD)ï¼ŒDå¿…é¡»æ˜¯8çš„æ•´æ•°å€ã€‚
- è¾“å…¥queryRopeã€keyRopeçš„Dï¼šHead-Dimå¿…é¡»æ»¡è¶³(qRopeD == kRopeD)ï¼ŒDå¿…é¡»æ˜¯8çš„æ•´æ•°å€ï¼Œä¸”å¿…é¡»å°äºç­‰äºqueryã€keyå’Œvalueçš„Dã€‚
- è¾“å…¥queryã€keyã€valueçš„inputLayoutå¿…é¡»ä¸ºTNDã€‚
- å…³äºæ•°æ®shapeçš„çº¦æŸï¼Œå…¶ä¸­ï¼š
    -   Tï¼šå–å€¼èŒƒå›´ä¸º1\~1Mã€‚
    -   Nï¼šå–å€¼èŒƒå›´ä¸º1\~256ã€‚
    -   Dï¼šå–å€¼èŒƒå›´ä¸º1\~768ã€‚
    -   æ•°æ®shapeå¿…é¡»ä¸ºTNDã€‚
    -   KeepProbå¿…é¡»ä¸º1ã€‚  
- queryã€keyã€valueæ•°æ®æ’å¸ƒæ ¼å¼ä»…æ”¯æŒTNDï¼ŒTæ˜¯Bå’ŒSåˆè½´ç´§å¯†æ’åˆ—çš„æ•°æ®ï¼ˆæ¯ä¸ªbatchçš„SeqLenQå’ŒSeqLenKVï¼‰ï¼Œå…¶ä¸­Bï¼ˆBatchï¼‰è¡¨ç¤ºè¾“å…¥æ ·æœ¬æ‰¹é‡å¤§å°ã€Sï¼ˆSeq-Lengthï¼‰è¡¨ç¤ºè¾“å…¥æ ·æœ¬åºåˆ—é•¿åº¦ã€Hï¼ˆHead-Sizeï¼‰è¡¨ç¤ºéšè—å±‚çš„å¤§å°ã€Nï¼ˆHead-Numï¼‰è¡¨ç¤ºå¤šå¤´æ•°ã€Dï¼ˆHead-Dimï¼‰è¡¨ç¤ºéšè—å±‚æœ€å°çš„å•å…ƒå°ºå¯¸ï¼Œä¸”æ»¡è¶³D=H/Nã€‚
- sparseModeçš„çº¦æŸå¦‚ä¸‹: 
  - å½“æ‰€æœ‰çš„attenMaskOptionalçš„shapeå°äº2048ä¸”ç›¸åŒçš„æ—¶å€™ï¼Œå»ºè®®ä½¿ç”¨defaultæ¨¡å¼ï¼Œæ¥å‡å°‘å†…å­˜ä½¿ç”¨é‡ï¼›
  - é…ç½®ä¸º1ã€2ã€3ã€5æ—¶ï¼Œç”¨æˆ·é…ç½®çš„preTokensã€nextTokensä¸ä¼šç”Ÿæ•ˆï¼›
  - é…ç½®ä¸º0ã€4æ—¶ï¼Œé¡»ä¿è¯attenMaskOptionalä¸preTokensã€nextTokensçš„èŒƒå›´ä¸€è‡´ã€‚
  - ç”¨æˆ·ä¸ç‰¹æ„æŒ‡å®šæ—¶å»ºè®®ä¼ å…¥0ã€‚
  - sparseä¸åŒæ¨¡å¼çš„è¯¦ç»†è¯´æ˜è¯·å‚è§[sparseæ¨¡å¼è¯´æ˜](../../../docs/zh/context/sparse_modeå‚æ•°è¯´æ˜.md)ã€‚
  - é…ç½®ä¸º3æ—¶ï¼Œä¸æ”¯æŒæ— æ•ˆè¡Œè®¡ç®—ï¼Œéœ€è¦æ»¡è¶³æ¯ä¸ªbatchçš„Sq<=Skvã€‚
  - é…ç½®ä¸º7æ—¶ï¼Œä¸æ”¯æŒå¯é€‰è¾“å…¥realShiftOptionalã€‚
  - é…ç½®ä¸º8æ—¶ï¼Œå½“æ¯ä¸ªsequenceçš„qã€kvç­‰é•¿æ—¶æ”¯æŒå¯é€‰è¾“å…¥realShiftOptionalï¼Œé’ˆå¯¹å…¨å±€åšpseç”Ÿæˆã€‚æ”¯æŒqæ–¹å‘è¿›è¡Œå¤–åˆ‡ï¼Œéœ€è¦å¤–åˆ‡å‰æ¯ä¸ªsequenceçš„qã€kvç­‰é•¿ï¼Œå¤–åˆ‡åä¼ å…¥çš„actualSeqQLenOptionalã€‚
- éƒ¨åˆ†åœºæ™¯ä¸‹ï¼Œå¦‚æœè®¡ç®—é‡è¿‡å¤§å¯èƒ½ä¼šå¯¼è‡´ç®—å­æ‰§è¡Œè¶…æ—¶(aicore errorç±»å‹æŠ¥é”™ï¼ŒerrorSträ¸ºï¼štimeout or trap error)ï¼Œæ­¤æ—¶å»ºè®®åšè½´åˆ‡åˆ†å¤„ç†ï¼Œæ³¨ï¼šè¿™é‡Œçš„è®¡ç®—é‡ä¼šå—Bã€Sã€Nã€Dç­‰å‚æ•°çš„å½±å“ï¼Œå€¼è¶Šå¤§è®¡ç®—é‡è¶Šå¤§ã€‚
- bandåœºæ™¯ï¼ŒpreTokenså’ŒnextTokensä¹‹é—´å¿…é¡»è¦æœ‰äº¤é›†ã€‚
- prefixOptionalç¨€ç–è®¡ç®—åœºæ™¯å³sparseMode=6ï¼Œå½“Sq > Skvæ—¶ï¼Œprefixçš„Nå€¼å–å€¼èŒƒå›´\[0, Skv\]ï¼Œå½“Sq <= Skvæ—¶ï¼Œprefixçš„Nå€¼å–å€¼èŒƒå›´\[Skv-Sq, Skv\]ã€‚
[0] - actualSeqKvLenOptional[0] + qStartIdxOptional - kvStartIdxOptional == 0ï¼ˆæœ¬åŠŸèƒ½å±å®éªŒæ€§åŠŸèƒ½ï¼‰ã€‚
- actualSeqQLenOptionalè¾“å…¥æ”¯æŒæŸä¸ªBatchä¸Šçš„Sé•¿åº¦ä¸º0ï¼Œæ­¤æ—¶ä¸æ”¯æŒå¯é€‰è¾“å…¥realShiftOptionalã€‚actualSeqQLenOptionalçš„é•¿åº¦å–å€¼èŒƒå›´ä¸º1\~2Kã€‚å½“å­˜åœ¨prefixOptionalè¾“å…¥çš„æ—¶å€™ï¼Œå…¶é•¿åº¦æœ€å¤§æ”¯æŒ1Kã€‚
- attenMaskOptionalè¾“å…¥ä¸æ”¯æŒè¡¥padï¼Œå³attenMaskOptionalä¸­ä¸èƒ½å­˜åœ¨æŸä¸€è¡Œå…¨1çš„åœºæ™¯ã€‚
- æ”¯æŒactualSeqQLenOptionalä¸­æŸä¸ªBatchä¸Šçš„Sé•¿åº¦ä¸º0ï¼›å¦‚æœå­˜åœ¨Sä¸º0çš„æƒ…å†µï¼Œä¸æ”¯æŒpseè¾“å…¥ï¼Œ
  å‡è®¾çœŸå®çš„Sé•¿åº¦ä¸º\[2,2,0,2,2\]ï¼Œåˆ™ä¼ å…¥çš„actualSeqQLenOptionalä¸º\[2,4,4,6,8\]ã€‚
- pseTypeåªèƒ½ä¸º0æˆ–è€…1ã€‚
- realShiftOptionalå¿…é¡»ä¸ºç©ºã€‚
- dropMaskOptionalå¿…é¡»ä¸ºç©ºã€‚
- attenMaskOptionalä¸èƒ½ä¸ºç©ºã€‚


## è°ƒç”¨ç¤ºä¾‹

è°ƒç”¨ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼Œä»…ä¾›å‚è€ƒï¼Œå…·ä½“ç¼–è¯‘å’Œæ‰§è¡Œè¿‡ç¨‹è¯·å‚è€ƒ[ç¼–è¯‘ä¸è¿è¡Œæ ·ä¾‹](../../../docs/zh/context/ç¼–è¯‘ä¸è¿è¡Œæ ·ä¾‹.md)ã€‚

```C++
#include <iostream>
#include <vector>
#include "acl/acl.h"
#include "aclnnop/aclnn_flash_attention_score.h"

#define CHECK_RET(cond, return_expr) \
  do {                               \
    if (!(cond)) {                   \
      return_expr;                   \
    }                                \
  } while (0)

#define LOG_PRINT(message, ...)     \
  do {                              \
    printf(message, ##__VA_ARGS__); \
  } while (0)

int64_t GetShapeSize(const std::vector<int64_t>& shape) {
  int64_t shapeSize = 1;
  for (auto i : shape) {
    shapeSize *= i;
  }
  return shapeSize;
}

void PrintOutResult(std::vector<int64_t> &shape, void** deviceAddr) {
  auto size = GetShapeSize(shape);
  std::vector<float> resultData(size, 0);
  auto ret = aclrtMemcpy(resultData.data(), resultData.size() * sizeof(resultData[0]),
                         *deviceAddr, size * sizeof(resultData[0]), ACL_MEMCPY_DEVICE_TO_HOST);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("copy result from device to host failed. ERROR: %d\n", ret); return);
  for (int64_t i = 0; i < size; i++) {
    LOG_PRINT("mean result[%ld] is: %f\n", i, resultData[i]);
  }
}

int Init(int32_t deviceId, aclrtStream* stream) {
  // å›ºå®šå†™æ³•ï¼Œèµ„æºåˆå§‹åŒ–
  auto ret = aclInit(nullptr);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclInit failed. ERROR: %d\n", ret); return ret);
  ret = aclrtSetDevice(deviceId);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtSetDevice failed. ERROR: %d\n", ret); return ret);
  ret = aclrtCreateStream(stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtCreateStream failed. ERROR: %d\n", ret); return ret);
  return 0;
}

template <typename T>
int CreateAclTensor(const std::vector<T>& hostData, const std::vector<int64_t>& shape, void** deviceAddr,
                    aclDataType dataType, aclTensor** tensor) {
  auto size = GetShapeSize(shape) * sizeof(T);
  // è°ƒç”¨aclrtMallocç”³è¯·deviceä¾§å†…å­˜
  auto ret = aclrtMalloc(deviceAddr, size, ACL_MEM_MALLOC_HUGE_FIRST);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtMalloc failed. ERROR: %d\n", ret); return ret);
  // è°ƒç”¨aclrtMemcpyå°†hostä¾§æ•°æ®æ‹·è´åˆ°deviceä¾§å†…å­˜ä¸Š
  ret = aclrtMemcpy(*deviceAddr, size, hostData.data(), size, ACL_MEMCPY_HOST_TO_DEVICE);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtMemcpy failed. ERROR: %d\n", ret); return ret);

  // è®¡ç®—è¿ç»­tensorçš„strides
  std::vector<int64_t> strides(shape.size(), 1);
  for (int64_t i = shape.size() - 2; i >= 0; i--) {
    strides[i] = shape[i + 1] * strides[i + 1];
  }

  // è°ƒç”¨aclCreateTensoræ¥å£åˆ›å»ºaclTensor
  *tensor = aclCreateTensor(shape.data(), shape.size(), dataType, strides.data(), 0, aclFormat::ACL_FORMAT_ND,
                            shape.data(), shape.size(), *deviceAddr);
  return 0;
}

int main() {
  // 1. ï¼ˆå›ºå®šå†™æ³•ï¼‰device/streamåˆå§‹åŒ–ï¼Œå‚è€ƒacl APIæ‰‹å†Œ
  // æ ¹æ®è‡ªå·±çš„å®é™…deviceå¡«å†™deviceId
  int32_t deviceId = 0;
  aclrtStream stream;
  auto ret = Init(deviceId, &stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("Init acl failed. ERROR: %d\n", ret); return ret);

  // 2. æ„é€ è¾“å…¥ä¸è¾“å‡ºï¼Œéœ€è¦æ ¹æ®APIçš„æ¥å£è‡ªå®šä¹‰æ„é€ 
  std::vector<int64_t> qShape = {256, 1, 128};
  std::vector<int64_t> qRopeShape = {256, 1, 64};
  std::vector<int64_t> kShape = {256, 1, 128};
  std::vector<int64_t> kRopeShape = {256, 1, 64};
  std::vector<int64_t> vShape = {256, 1, 128};
  std::vector<int64_t> attenmaskShape = {256, 256};

  std::vector<int64_t> attentionOutShape = {256, 1, 128};
  std::vector<int64_t> softmaxMaxShape = {256, 1, 8};
  std::vector<int64_t> softmaxSumShape = {256, 1, 8};

  void* qDeviceAddr = nullptr;
  void* qRopeDeviceAddr = nullptr;
  void* kDeviceAddr = nullptr;
  void* kRopeDeviceAddr = nullptr;
  void* vDeviceAddr = nullptr;
  void* attenmaskDeviceAddr = nullptr;
  void* attentionOutDeviceAddr = nullptr;
  void* softmaxMaxDeviceAddr = nullptr;
  void* softmaxSumDeviceAddr = nullptr;

  aclTensor* q = nullptr;
  aclTensor* qRope = nullptr;
  aclTensor* k = nullptr;
  aclTensor* kRope = nullptr;
  aclTensor* v = nullptr;
  aclTensor* pse = nullptr;
  aclTensor* dropMask = nullptr;
  aclTensor* padding = nullptr;
  aclTensor* attenmask = nullptr;
  aclTensor* attentionOut = nullptr;
  aclTensor* softmaxMax = nullptr;
  aclTensor* softmaxSum = nullptr;
  aclTensor* softmaxOut = nullptr;

  std::vector<float> qHostData(32768, 1);
  std::vector<float> qRopeHostData(16384, 1);
  std::vector<float> kHostData(32768, 1);
  std::vector<float> kRopeHostData(16384, 1);
  std::vector<float> vHostData(32768, 1);
  std::vector<uint8_t> attenmaskHostData(65536, 0);
  std::vector<float> attentionOutHostData(32768, 0);
  std::vector<float> softmaxMaxHostData(2048, 3.0);
  std::vector<float> softmaxSumHostData(2048, 3.0);

  ret = CreateAclTensor(qHostData, qShape, &qDeviceAddr, aclDataType::ACL_BF16, &q);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(qRopeHostData, qRopeShape, &qRopeDeviceAddr, aclDataType::ACL_BF16, &qRope);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(kHostData, kShape, &kDeviceAddr, aclDataType::ACL_BF16, &k);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(kRopeHostData, kRopeShape, &kRopeDeviceAddr, aclDataType::ACL_BF16, &kRope);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(vHostData, vShape, &vDeviceAddr, aclDataType::ACL_BF16, &v);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(attenmaskHostData, attenmaskShape, &attenmaskDeviceAddr, aclDataType::ACL_UINT8, &attenmask);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(attentionOutHostData, attentionOutShape, &attentionOutDeviceAddr, aclDataType::ACL_BF16, &attentionOut);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(softmaxMaxHostData, softmaxMaxShape, &softmaxMaxDeviceAddr, aclDataType::ACL_FLOAT, &softmaxMax);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(softmaxSumHostData, softmaxSumShape, &softmaxSumDeviceAddr, aclDataType::ACL_FLOAT, &softmaxSum);
  CHECK_RET(ret == ACL_SUCCESS, return ret);

  std::vector<int64_t> prefixOp = {0};
  aclIntArray *prefix = aclCreateIntArray(prefixOp.data(), 1);
  std::vector<int64_t> qStartIdxOp = {0};
  std::vector<int64_t> kvStartIdxOp = {0};
  aclIntArray *qStartIdx = aclCreateIntArray(qStartIdxOp.data(), 1);
  aclIntArray *kvStartIdx = aclCreateIntArray(kvStartIdxOp.data(), 1);
  std::vector<int64_t>  acSeqQLenOp = {256};
  std::vector<int64_t>  acSeqKvLenOp = {256};
  aclIntArray* acSeqQLen = aclCreateIntArray(acSeqQLenOp.data(), acSeqQLenOp.size());
  aclIntArray* acSeqKvLen = aclCreateIntArray(acSeqKvLenOp.data(), acSeqKvLenOp.size());
  double scaleValue = 0.088388;
  double keepProb = 1;
  int64_t preTokens = 65536;
  int64_t nextTokens = 65536;
  int64_t headNum = 1;
  int64_t innerPrecise = 0;
  int64_t sparseMode = 0;
  int64_t pseType = 1;

  char layOut[5] = {'T', 'N', 'D', 0};

  // 3. è°ƒç”¨CANNç®—å­åº“APIï¼Œéœ€è¦ä¿®æ”¹ä¸ºå…·ä½“çš„Apiåç§°
  uint64_t workspaceSize = 0;
  aclOpExecutor* executor;

  // è°ƒç”¨aclnnFlashAttentionVarLenScoreV3ç¬¬ä¸€æ®µæ¥å£
  ret = aclnnFlashAttentionVarLenScoreV3GetWorkspaceSize(
            q, qRope, k, kRope, v, pse, dropMask, padding, attenmask, prefix, acSeqQLen, acSeqKvLen, qStartIdx, kvStartIdx,
            scaleValue, keepProb, preTokens, nextTokens, headNum, layOut, innerPrecise,
            sparseMode, pseType, softmaxMax, softmaxSum, softmaxOut, attentionOut, &workspaceSize, &executor);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclnnFlashAttentionVarLenScoreV3GetWorkspaceSize failed. ERROR: %d\n", ret);
            return ret);

  // æ ¹æ®ç¬¬ä¸€æ®µæ¥å£è®¡ç®—å‡ºçš„workspaceSizeç”³è¯·deviceå†…å­˜
  void* workspaceAddr = nullptr;
  if (workspaceSize > 0) {
    ret = aclrtMalloc(&workspaceAddr, workspaceSize, ACL_MEM_MALLOC_HUGE_FIRST);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("allocate workspace failed. ERROR: %d\n", ret); return ret);
  }

  // è°ƒç”¨aclnnFlashAttentionVarLenScoreV3ç¬¬äºŒæ®µæ¥å£
  ret = aclnnFlashAttentionVarLenScoreV3(workspaceAddr, workspaceSize, executor, stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclnnFlashAttentionVarLenScoreV3 failed. ERROR: %d\n", ret); return ret);

  // 4. ï¼ˆå›ºå®šå†™æ³•ï¼‰åŒæ­¥ç­‰å¾…ä»»åŠ¡æ‰§è¡Œç»“æŸ
  ret = aclrtSynchronizeStream(stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtSynchronizeStream failed. ERROR: %d\n", ret); return ret);

  // 5. è·å–è¾“å‡ºçš„å€¼ï¼Œå°†deviceä¾§å†…å­˜ä¸Šçš„ç»“æœæ‹·è´è‡³hostä¾§ï¼Œéœ€è¦æ ¹æ®å…·ä½“APIçš„æ¥å£å®šä¹‰ä¿®æ”¹
  PrintOutResult(attentionOutShape, &attentionOutDeviceAddr);
  PrintOutResult(softmaxMaxShape, &softmaxMaxDeviceAddr);
  PrintOutResult(softmaxSumShape, &softmaxSumDeviceAddr);

  // 6. é‡Šæ”¾aclTensorå’ŒaclScalarï¼Œéœ€è¦æ ¹æ®å…·ä½“APIçš„æ¥å£å®šä¹‰ä¿®æ”¹
  aclDestroyTensor(q);
  aclDestroyTensor(qRope);
  aclDestroyTensor(k);
  aclDestroyTensor(kRope);
  aclDestroyTensor(v);
  aclDestroyTensor(attenmask);
  aclDestroyTensor(attentionOut);
  aclDestroyTensor(softmaxMax);
  aclDestroyTensor(softmaxSum);

  // 7. é‡Šæ”¾deviceèµ„æº
  aclrtFree(qDeviceAddr);
  aclrtFree(qRopeDeviceAddr);
  aclrtFree(kDeviceAddr);
  aclrtFree(kRopeDeviceAddr);
  aclrtFree(vDeviceAddr);
  aclrtFree(attenmaskDeviceAddr);
  aclrtFree(attentionOutDeviceAddr);
  aclrtFree(softmaxMaxDeviceAddr);
  aclrtFree(softmaxSumDeviceAddr);
  if (workspaceSize > 0) {
    aclrtFree(workspaceAddr);
  }
  aclrtDestroyStream(stream);
  aclrtResetDevice(deviceId);
  aclFinalize();

  return 0;
}

```