# aclnnFlashAttentionScore

[ğŸ“„ æŸ¥çœ‹æºç ](https://gitcode.com/cann/ops-transformer/tree/master/attention/flash_attention_score)

## äº§å“æ”¯æŒæƒ…å†µ

|äº§å“      | æ˜¯å¦æ”¯æŒ |
|:----------------------------|:-----------:|
|<term>æ˜‡è…¾ 950PR/950DT AIå¤„ç†å™¨</term>|      Ã—     |
|<term>Atlas A3 è®­ç»ƒç³»åˆ—äº§å“/Atlas A3 æ¨ç†ç³»åˆ—äº§å“</term>|     âˆš      |
|<term>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</term>|      âˆš     |
|<term>Atlas 800I A2 æ¨ç†äº§å“</term>|      Ã—     |
|<term>A200I A2 Box å¼‚æ„ç»„ä»¶</term>|      Ã—     |


## åŠŸèƒ½è¯´æ˜

- æ¥å£åŠŸèƒ½ï¼šè®­ç»ƒåœºæ™¯ä¸‹ï¼Œä½¿ç”¨FlashAttentionç®—æ³•å®ç°self-attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰çš„è®¡ç®—ã€‚

- è®¡ç®—å…¬å¼ï¼š

  æ³¨æ„åŠ›çš„æ­£å‘è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

  $$
  attention\_out = Dropout(Softmax(Mask(scale*(pse+query*key^T), atten\_mask)), keep\_prob)*value
  $$


## å‡½æ•°åŸå‹

æ¯ä¸ªç®—å­åˆ†ä¸º[ä¸¤æ®µå¼æ¥å£](../../../docs/zh/context/ä¸¤æ®µå¼æ¥å£.md)ï¼Œå¿…é¡»å…ˆè°ƒç”¨â€œaclnnFlashAttentionScoreGetWorkspaceSizeâ€æ¥å£è·å–è®¡ç®—æ‰€éœ€workspaceå¤§å°ä»¥åŠåŒ…å«äº†ç®—å­è®¡ç®—æµç¨‹çš„æ‰§è¡Œå™¨ï¼Œå†è°ƒç”¨â€œaclnnFlashAttentionScoreâ€æ¥å£æ‰§è¡Œè®¡ç®—ã€‚

```c++
aclnnStatus aclnnFlashAttentionScoreGetWorkspaceSize(
  const aclTensor   *query, 
  const aclTensor   *key, 
  const aclTensor   *value, 
  const aclTensor   *realShiftOptional, 
  const aclTensor   *dropMaskOptional, 
  const aclTensor   *paddingMaskOptional, 
  const aclTensor   *attenMaskOptional, 
  const aclIntArray *prefixOptional, 
  double             scaleValue, 
  double             keepProb, 
  int64_t            preTokens, 
  int64_t            nextTokens, 
  int64_t            headNum, 
  char              *inputLayout, 
  int64_t            innerPrecise, 
  int64_t            sparseMode, 
  const aclTensor   *softmaxMaxOut, 
  const aclTensor   *softmaxSumOut, 
  const aclTensor   *softmaxOutOut, 
  const aclTensor   *attentionOutOut, 
  uint64_t          *workspaceSize, 
  aclOpExecutor    **executor)
```
```c++
aclnnStatus aclnnFlashAttentionScore(
  void              *workspace, 
  uint64_t           workspaceSize, 
  aclOpExecutor     *executor, 
  const aclrtStream  stream)
```


## aclnnFlashAttentionScoreGetWorkspaceSize

- **å‚æ•°è¯´æ˜ï¼š**

  <table style="undefined;table-layout: fixed; width: 1452px"><colgroup>
    <col style="width: 174px">
    <col style="width: 121px">
    <col style="width: 253px">
    <col style="width: 262px">
    <col style="width: 213px">
    <col style="width: 115px">
    <col style="width: 169px">
    <col style="width: 145px">
    </colgroup>
    <thead>
      <tr>
        <th>å‚æ•°å</th>
        <th>è¾“å…¥/è¾“å‡º</th>
        <th>æè¿°</th>
        <th>ä½¿ç”¨è¯´æ˜</th>
        <th>æ•°æ®ç±»å‹</th>
        <th>æ•°æ®æ ¼å¼</th>
        <th>ç»´åº¦(shape)</th>
        <th>éè¿ç»­Tensor</th>
      </tr></thead>
    <tbody>
      <tr>
        <td>query</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„queryã€‚</td>
        <td>æ•°æ®ç±»å‹ä¸key/valueçš„æ•°æ®ç±»å‹ä¸€è‡´ã€‚</td>
        <td>FLOAT16ã€BFLOAT16ã€FLOAT32</td>
        <td>ND</td>
        <td>[BNSD]ã€[BSND]ã€[BSH]ã€[SBH]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>key</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„keyã€‚</td>
        <td>æ•°æ®ç±»å‹ä¸query/valueçš„æ•°æ®ç±»å‹ä¸€è‡´ã€‚</td>
        <td>FLOAT16ã€BFLOAT16ã€FLOAT32</td>
        <td>ND</td>
        <td>[BNSD]ã€[BSND]ã€[BSH]ã€[SBH]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>value</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„valueã€‚</td>
        <td>æ•°æ®ç±»å‹ä¸query/keyçš„æ•°æ®ç±»å‹ä¸€è‡´ã€‚</td>
        <td>FLOAT16ã€BFLOAT16ã€FLOAT32</td>
        <td>ND</td>
        <td>[BNSD]ã€[BSND]ã€[BSH]ã€[SBH]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>realShiftOptional</td>
        <td>å¯é€‰è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„pseã€‚</td>
        <td>æ•°æ®ç±»å‹ä¸queryçš„æ•°æ®ç±»å‹ä¸€è‡´ã€‚</td>
        <td>FLOAT16ã€BFLOAT16ã€FLOAT32</td>
        <td>ND</td>
        <td>[B,N,Sq,Skv]ã€[B,N,1,Skv]ã€[1,N,Sq,Skv]ã€[B,N,1024,Skv]ã€[1,N,1024,Skv]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>dropMaskOptional</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„Dropoutã€‚</td>
        <td>-</td>
        <td>UINT8</td>
        <td>ND</td>
        <td>0ã€1</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>attenMaskOptional</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„atten_maskã€‚</td>
        <td>å–å€¼ä¸º1ä»£è¡¨è¯¥ä½ä¸å‚ä¸è®¡ç®—ï¼Œä¸º0ä»£è¡¨è¯¥ä½å‚ä¸è®¡ç®—ã€‚</td>
        <td>BOOLã€UINT8</td>
        <td>ND</td>
        <td>[B,N,Sq,Skv]ã€[B,1,Sq,Skv]ã€[1,1,Sq,Skv]ã€[Sq,Skv] </td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>prefixOptional</td>
        <td>è¾“å…¥</td>
        <td>
          <ul>
            <li>prefixç¨€ç–è®¡ç®—åœºæ™¯</li>
            <li>æ¯ä¸ªBatchçš„Nå€¼ã€‚</li>
          </ul>
        </td>
        <td>-</td>
        <td>INT64</td>
        <td>ND</td>
        <td>0ã€1</td>
        <td>-</td>
      </tr>
      <tr>
        <td>scaleValue</td>
        <td>è¾“å…¥</td>
        <td>å…¬å¼ä¸­çš„scaleï¼Œä»£è¡¨ç¼©æ”¾ç³»æ•°ã€‚</td>
        <td>-</td>
        <td>DOUBLE</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>keepProb</td>
        <td>è¾“å…¥</td>
        <td>ä»£è¡¨dropMaskOptionalä¸­1çš„æ¯”ä¾‹ã€‚</td>
        <td>å–å€¼èŒƒå›´ä¸º(0, 1]ã€‚</td>
        <td>DOUBLE</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>preTokens</td>
        <td>è¾“å…¥</td>
        <td>ç”¨äºç¨€ç–è®¡ç®— ï¼Œè¡¨ç¤ºslides windowçš„å·¦è¾¹ç•Œã€‚</td>
        <td>-</td>
        <td>INT64</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>nextTokens</td>
        <td>è¾“å…¥</td>
        <td>ç”¨äºç¨€ç–è®¡ç®—ï¼Œè¡¨ç¤ºslides windowçš„å³è¾¹ç•Œã€‚</td>
        <td>-</td>
        <td>INT64</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>headNum</td>
        <td>è¾“å…¥</td>
        <td>ä»£è¡¨å•å¡çš„headä¸ªæ•°ï¼Œå³è¾“å…¥queryçš„Nè½´é•¿åº¦ã€‚</td>
        <td>-</td>
        <td>INT64</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>inputLayout</td>
        <td>è¾“å…¥</td>
        <td>ä»£è¡¨è¾“å…¥queryã€keyã€valueçš„æ•°æ®æ’å¸ƒæ ¼å¼ã€‚</td>
        <td>æ”¯æŒBSHã€SBHã€BSNDã€BNSDã€‚</td>
        <td>String</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>innerPrecise</td>
        <td>è¾“å…¥</td>
        <td>ç”¨äºæå‡ç²¾åº¦ã€‚</td>
        <td>-</td>
        <td>INT64</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>sparseMode</td>
        <td>è¾“å…¥</td>
        <td>è¡¨ç¤ºsparseçš„æ¨¡å¼ã€‚</td>
        <td>æ”¯æŒé…ç½®å€¼ä¸º0ã€1ã€2ã€3ã€4ã€5ã€6ã€‚</td>
        <td>INT64</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>softmaxMaxOut</td>
        <td>è¾“å‡º</td>
        <td>Softmaxè®¡ç®—çš„Maxä¸­é—´ç»“æœï¼Œç”¨äºåå‘è®¡ç®—ã€‚</td>
        <td>-</td>
        <td>FLOAT</td>
        <td>ND</td>
        <td>[B,N,Sq,8]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>softmaxSumOut</td>
        <td>è¾“å‡º</td>
        <td>Softmaxè®¡ç®—çš„Sumä¸­é—´ç»“æœï¼Œç”¨äºåå‘è®¡ç®—ã€‚</td>
        <td>-</td>
        <td>FLOAT</td>
        <td>ND</td>
        <td>[B,N,Sq,8]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>attentionOutOut</td>
        <td>è¾“å‡º</td>
        <td>è®¡ç®—å…¬å¼çš„æœ€ç»ˆè¾“å‡ºã€‚</td>
        <td>æ•°æ®ç±»å‹å’Œshapeç±»å‹ä¸queryä¿æŒä¸€è‡´ã€‚</td>
        <td>FLOAT16ã€BFLOAT16ã€FLOAT32</td>
        <td>ND</td>
        <td>[BNSD]ã€[BSND]ã€[BSH]ã€[SBH]</td>
        <td>âˆš</td>
      </tr>
      <tr>
        <td>workspaceSize</td>
        <td>è¾“å‡º</td>
        <td>è¿”å›éœ€è¦åœ¨Deviceä¾§ç”³è¯·çš„workspaceå¤§å°ã€‚</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
      <tr>
        <td>executor</td>
        <td>è¾“å‡º</td>
        <td>è¿”å›opæ‰§è¡Œå™¨ï¼ŒåŒ…å«äº†ç®—å­è®¡ç®—æµç¨‹ã€‚</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
      </tr>
    </tbody>
  </table>

- **è¿”å›å€¼ï¼š**

  aclnnStatusï¼šè¿”å›çŠ¶æ€ç ï¼Œå…·ä½“å‚è§[aclnnè¿”å›ç ](../../../docs/zh/context/aclnnè¿”å›ç .md)ã€‚

  ç¬¬ä¸€æ®µæ¥å£å®Œæˆå…¥å‚æ ¡éªŒï¼Œå‡ºç°ä»¥ä¸‹åœºæ™¯æ—¶æŠ¥é”™ï¼š
  
  <table style="undefined;table-layout: fixed;width: 1202px"><colgroup>
  <col style="width: 262px">
  <col style="width: 121px">
  <col style="width: 819px">
  </colgroup>
  <thead>
    <tr>
      <th>è¿”å›ç </th>
      <th>é”™è¯¯ç </th>
      <th>æè¿°</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ACLNN_ERR_PARAM_NULLPTR</td>
      <td>161001</td>
      <td>ä¼ å…¥å‚æ•°æ˜¯å¿…é€‰è¾“å…¥ï¼Œè¾“å‡ºæˆ–è€…å¿…é€‰å±æ€§ï¼Œä¸”æ˜¯ç©ºæŒ‡é’ˆã€‚</td>
    </tr>
    <tr>
      <td rowspan="2">ACLNN_ERR_PARAM_INVALID</td>
      <td rowspan="2">161002</td>
      <td>queryã€keyã€valueã€realShiftOptionalã€dropMaskOptionalã€paddingMaskOptionalã€attenMaskOptionalã€softmaxMaxOutã€softmaxSumOutã€softmaxOutOutã€attentionOutOutçš„æ•°æ®ç±»å‹ä¸åœ¨æ”¯æŒçš„èŒƒå›´å†…ã€‚</td>
    </tr>
    <tr>
      <td>queryã€keyã€valueã€realShiftOptionalã€dropMaskOptionalã€paddingMaskOptionalã€attenMaskOptionalã€softmaxMaxOutã€softmaxSumOutã€softmaxOutOutã€attentionOutOutçš„æ•°æ®æ ¼å¼ä¸åœ¨æ”¯æŒçš„èŒƒå›´å†…ã€‚</td>
    </tr>
  </tbody>
  </table>


## aclnnFlashAttentionScore

- **å‚æ•°è¯´æ˜ï¼š**
  <table style="undefined;table-layout: fixed; width: 1154px"><colgroup>
  <col style="width: 153px">
  <col style="width: 121px">
  <col style="width: 880px">
  </colgroup>
  <thead>
    <tr>
      <th>å‚æ•°å</th>
      <th>è¾“å…¥/è¾“å‡º</th>
      <th>æè¿°</th>
    </tr></thead>
  <tbody>
    <tr>
      <td>workspace</td>
      <td>è¾“å…¥</td>
      <td>åœ¨Deviceä¾§ç”³è¯·çš„workspaceå†…å­˜åœ°å€ã€‚</td>
    </tr>
    <tr>
      <td>workspaceSize</td>
      <td>è¾“å…¥</td>
      <td>åœ¨Deviceä¾§ç”³è¯·çš„workspaceå¤§å°ï¼Œç”±ç¬¬ä¸€æ®µæ¥å£aclnnFlashAttentionScoreGetWorkspaceSizeè·å–ã€‚</td>
    </tr>
    <tr>
      <td>executor</td>
      <td>è¾“å…¥</td>
      <td>opæ‰§è¡Œå™¨ï¼ŒåŒ…å«äº†ç®—å­è®¡ç®—æµç¨‹ã€‚</td>
    </tr>
    <tr>
      <td>stream</td>
      <td>è¾“å…¥</td>
      <td>æŒ‡å®šæ‰§è¡Œä»»åŠ¡çš„Streamã€‚</td>
    </tr>
  </tbody>
  </table>

-   **è¿”å›å€¼ï¼š**

    è¿”å›aclnnStatusçŠ¶æ€ç ï¼Œå…·ä½“å‚è§[aclnnè¿”å›ç ](../../../docs/zh/context/aclnnè¿”å›ç .md)ã€‚

## çº¦æŸè¯´æ˜

- ç¡®å®šæ€§è®¡ç®—ï¼š
  - aclnnFlashAttentionScoreé»˜è®¤ç¡®å®šæ€§å®ç°ã€‚
- è¯¥æ¥å£ä¸PyTorché…åˆä½¿ç”¨æ—¶ï¼Œéœ€è¦ä¿è¯CANNç›¸å…³åŒ…ä¸PyTorchç›¸å…³åŒ…çš„ç‰ˆæœ¬åŒ¹é…ã€‚
- è¾“å…¥queryã€keyã€valueçš„çº¦æŸå¦‚ä¸‹ï¼š
  - æ•°æ®æ’å¸ƒæ ¼å¼æ”¯æŒä»å¤šç§ç»´åº¦è§£è¯»ï¼Œå…¶ä¸­Bï¼ˆBatchï¼‰è¡¨ç¤ºè¾“å…¥æ ·æœ¬æ‰¹é‡å¤§å°ã€Sï¼ˆSeq-Lengthï¼‰è¡¨ç¤ºè¾“å…¥æ ·æœ¬åºåˆ—é•¿åº¦ã€Hï¼ˆHead-Sizeï¼‰è¡¨ç¤ºéšè—å±‚çš„å¤§å°ã€Nï¼ˆHead-Numï¼‰è¡¨ç¤ºå¤šå¤´æ•°ã€Dï¼ˆHead-Dimï¼‰è¡¨ç¤ºéšè—å±‚æœ€å°çš„å•å…ƒå°ºå¯¸ï¼Œä¸”æ»¡è¶³D=H/Nã€‚
  - Bï¼šbatchsizeå¿…é¡»ç›¸ç­‰ã€‚
  - Dï¼šHead-Dimå¿…é¡»æ»¡è¶³(qD == kD && kD >= vD)ã€‚
  - inputLayoutå¿…é¡»ä¸€è‡´ã€‚
  - ä¸realShiftOptionalçš„æ•°æ®ç±»å‹å¿…é¡»ä¸€è‡´ã€‚
- è¾“å…¥key/valueçš„shapeé™¤Då¤–å¿…é¡»ä¸€è‡´ã€‚
- æ”¯æŒè¾“å…¥queryçš„Nå’Œkey/valueçš„Nä¸ç›¸ç­‰ï¼Œä½†å¿…é¡»æˆæ¯”ä¾‹å…³ç³»ï¼Œå³Nq/Nkvå¿…é¡»æ˜¯é0æ•´æ•°ï¼ŒNqå–å€¼èŒƒå›´1~256ã€‚å½“Nq/Nkv > 1æ—¶ï¼Œå³ä¸ºGQA(grouped-query attention)ï¼›å½“Nkv=1æ—¶ï¼Œå³ä¸ºMQA(multi-query attention)ã€‚æœ¬æ–‡å¦‚æ— ç‰¹æ®Šè¯´æ˜ï¼ŒNè¡¨ç¤ºçš„æ˜¯Nqã€‚
- å…³äºæ•°æ®shapeçš„çº¦æŸï¼Œä»¥inputLayoutçš„BSNDã€BNSDä¸ºä¾‹ï¼ˆBSHã€SBHä¸‹H=N\*Dï¼‰ï¼Œå…¶ä¸­ï¼š
    - Bï¼šå–å€¼èŒƒå›´ä¸º1\~2Mã€‚å¸¦prefixOptionalçš„æ—¶å€™Bæœ€å¤§æ”¯æŒ2Kã€‚
    - Nï¼šå–å€¼èŒƒå›´ä¸º1\~256ã€‚
    - Sï¼šå–å€¼èŒƒå›´ä¸º1\~1Mã€‚
    - Dï¼šå–å€¼èŒƒå›´ä¸º1\~768ã€‚
- realShiftOptionalï¼šå¦‚æœSqå¤§äº1024çš„æ¯ä¸ªbatchçš„Sqä¸Skvç­‰é•¿ä¸”æ˜¯sparseModeä¸º0ã€2ã€3çš„ä¸‹ä¸‰è§’æ©ç åœºæ™¯ï¼Œå¯ä½¿èƒ½alibiä½ç½®ç¼–ç å‹ç¼©ï¼Œæ­¤æ—¶åªéœ€è¦è¾“å…¥åŸå§‹PSEæœ€å1024è¡Œè¿›è¡Œå†…å­˜ä¼˜åŒ–ï¼Œå³alibi_compress = ori_pse[:, :, -1024:, :]ï¼Œå…·ä½“å¦‚ä¸‹ï¼š
  - å‚æ•°æ¯ä¸ªbatchä¸ç›¸åŒæ—¶ï¼Œshapeä¸ºBNHSkv(H=1024)ã€‚
  - æ¯ä¸ªbatchç›¸åŒæ—¶ï¼Œshapeä¸º1NHSkv(H=1024)ã€‚
  - å¦‚ä¸ä½¿ç”¨è¯¥å‚æ•°å¯ä¼ å…¥nullptrã€‚
- innerPreciseï¼šå½“å‰0ã€1ä¸ºä¿ç•™é…ç½®å€¼ï¼Œ2ä¸ºä½¿èƒ½æ— æ•ˆè¡Œè®¡ç®—ï¼Œå…¶åŠŸèƒ½æ˜¯é¿å…åœ¨è®¡ç®—è¿‡ç¨‹ä¸­å­˜åœ¨æ•´è¡Œmaskè¿›è€Œå¯¼è‡´ç²¾åº¦æœ‰æŸå¤±ï¼Œä½†æ˜¯è¯¥é…ç½®ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ å¦‚æœç®—å­å¯åˆ¤æ–­å‡ºå­˜åœ¨æ— æ•ˆè¡Œåœºæ™¯ï¼Œä¼šè‡ªåŠ¨ä½¿èƒ½æ— æ•ˆè¡Œè®¡ç®—ï¼Œä¾‹å¦‚sparseModeä¸º3ï¼ŒSq > Skvåœºæ™¯ã€‚
- sparseModeçš„çº¦æŸå¦‚ä¸‹ï¼š
  - å½“æ‰€æœ‰çš„attenMaskOptionalçš„shapeå°äº2048ä¸”ç›¸åŒçš„æ—¶å€™ï¼Œå»ºè®®ä½¿ç”¨defaultæ¨¡å¼ï¼Œæ¥å‡å°‘å†…å­˜ä½¿ç”¨é‡ã€‚
  - é…ç½®ä¸º0ã€4æ—¶ï¼Œé¡»ä¿è¯attenMaskOptionalä¸preTokensã€nextTokensçš„èŒƒå›´ä¸€è‡´ã€‚
  - é…ç½®ä¸º1ã€2ã€3ã€5ã€6æ—¶ï¼Œç”¨æˆ·é…ç½®çš„preTokensã€nextTokensä¸ä¼šç”Ÿæ•ˆã€‚
  - ä¸º1ã€2ã€3ã€4ã€5ã€6æ—¶ï¼Œåº”ä¼ å…¥å¯¹åº”æ­£ç¡®çš„attenMaskOptionalï¼Œå¦åˆ™å°†å¯¼è‡´è®¡ç®—ç»“æœé”™è¯¯ã€‚å½“attenMaskOptionalè¾“å…¥ä¸ºNoneæ—¶ï¼ŒsparseModeã€preTokensã€nextTokenså‚æ•°ä¸ç”Ÿæ•ˆï¼Œå›ºå®šä¸ºå…¨è®¡ç®—ã€‚
  - ç”¨æˆ·ä¸ç‰¹æ„æŒ‡å®šæ—¶å»ºè®®ä¼ å…¥0ã€‚
  - sparseä¸åŒæ¨¡å¼çš„è¯¦ç»†è¯´æ˜è¯·å‚è§[sparseæ¨¡å¼è¯´æ˜](../../../docs/zh/context/sparse_modeå‚æ•°è¯´æ˜.md)ã€‚
- éƒ¨åˆ†åœºæ™¯ä¸‹ï¼Œå¦‚æœè®¡ç®—é‡è¿‡å¤§å¯èƒ½ä¼šå¯¼è‡´ç®—å­æ‰§è¡Œè¶…æ—¶ï¼ˆaicore errorç±»å‹æŠ¥é”™ï¼ŒerrorSträ¸ºï¼štimeout or trap errorï¼‰ï¼Œæ­¤æ—¶å»ºè®®åšè½´åˆ‡åˆ†å¤„ç†ï¼Œæ³¨ï¼šè¿™é‡Œçš„è®¡ç®—é‡ä¼šå—Bã€Sã€Nã€Dç­‰å‚æ•°çš„å½±å“ï¼Œå€¼è¶Šå¤§è®¡ç®—é‡è¶Šå¤§ã€‚
- prefixOptionalç¨€ç–è®¡ç®—åœºæ™¯å³sparseMode=5æˆ–6ï¼Œå½“Sq > Skvæ—¶ï¼Œprefixçš„Nå€¼å–å€¼èŒƒå›´\[0, Skv\]ï¼Œå½“Sq <= Skvæ—¶ï¼Œprefixçš„Nå€¼å–å€¼èŒƒå›´\[Skv-Sq, Skv\]ã€‚
- bandåœºæ™¯ï¼ŒpreTokenså’ŒnextTokensä¹‹é—´å¿…é¡»è¦æœ‰äº¤é›†ã€‚
- realShiftOptional Sqå¤§äº1024æ—¶ï¼Œå¦‚æœé…ç½®BNHSã€1NHSï¼Œéœ€è¦Sqå’ŒSkvç­‰é•¿ã€‚

## è°ƒç”¨ç¤ºä¾‹

è°ƒç”¨ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼Œä»…ä¾›å‚è€ƒï¼Œå…·ä½“ç¼–è¯‘å’Œæ‰§è¡Œè¿‡ç¨‹è¯·å‚è€ƒ[ç¼–è¯‘ä¸è¿è¡Œæ ·ä¾‹](../../../docs/zh/context/ç¼–è¯‘ä¸è¿è¡Œæ ·ä¾‹.md)ã€‚

```c++
#include <iostream>
#include <vector>
#include "acl/acl.h"
#include "aclnnop/aclnn_flash_attention_score.h"

#define CHECK_RET(cond, return_expr) \
  do {                               \
    if (!(cond)) {                   \
      return_expr;                   \
    }                                \
  } while (0)

#define LOG_PRINT(message, ...)     \
  do {                              \
    printf(message, ##__VA_ARGS__); \
  } while (0)

int64_t GetShapeSize(const std::vector<int64_t>& shape) {
  int64_t shapeSize = 1;
  for (auto i : shape) {
    shapeSize *= i;
  }
  return shapeSize;
}

void PrintOutResult(std::vector<int64_t> &shape, void** deviceAddr) {
  auto size = GetShapeSize(shape);
  std::vector<float> resultData(size, 0);
  auto ret = aclrtMemcpy(resultData.data(), resultData.size() * sizeof(resultData[0]),
                         *deviceAddr, size * sizeof(resultData[0]), ACL_MEMCPY_DEVICE_TO_HOST);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("copy result from device to host failed. ERROR: %d\n", ret); return);
  for (int64_t i = 0; i < size; i++) {
    LOG_PRINT("mean result[%ld] is: %f\n", i, resultData[i]);
  }
}

int Init(int32_t deviceId, aclrtContext* context, aclrtStream* stream) {
  // å›ºå®šå†™æ³•ï¼Œèµ„æºåˆå§‹åŒ–
  auto ret = aclInit(nullptr);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclInit failed. ERROR: %d\n", ret); return ret);
  ret = aclrtSetDevice(deviceId);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtSetDevice failed. ERROR: %d\n", ret); return ret);
  ret = aclrtCreateContext(context, deviceId);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtCreateContext failed. ERROR: %d\n", ret); return ret);
  ret = aclrtSetCurrentContext(*context);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtSetCurrentContext failed. ERROR: %d\n", ret); return ret);
  ret = aclrtCreateStream(stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtCreateStream failed. ERROR: %d\n", ret); return ret);
  return 0;
}

template <typename T>
int CreateAclTensor(const std::vector<T>& hostData, const std::vector<int64_t>& shape, void** deviceAddr,
                    aclDataType dataType, aclTensor** tensor) {
  auto size = GetShapeSize(shape) * sizeof(T);
  // è°ƒç”¨aclrtMallocç”³è¯·deviceä¾§å†…å­˜
  auto ret = aclrtMalloc(deviceAddr, size, ACL_MEM_MALLOC_HUGE_FIRST);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtMalloc failed. ERROR: %d\n", ret); return ret);
  // è°ƒç”¨aclrtMemcpyå°†hostä¾§æ•°æ®æ‹·è´åˆ°deviceä¾§å†…å­˜ä¸Š
  ret = aclrtMemcpy(*deviceAddr, size, hostData.data(), size, ACL_MEMCPY_HOST_TO_DEVICE);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtMemcpy failed. ERROR: %d\n", ret); return ret);

  // è®¡ç®—è¿ç»­tensorçš„strides
  std::vector<int64_t> strides(shape.size(), 1);
  for (int64_t i = shape.size() - 2; i >= 0; i--) {
    strides[i] = shape[i + 1] * strides[i + 1];
  }

  // è°ƒç”¨aclCreateTensoræ¥å£åˆ›å»ºaclTensor
  *tensor = aclCreateTensor(shape.data(), shape.size(), dataType, strides.data(), 0, aclFormat::ACL_FORMAT_ND,
                            shape.data(), shape.size(), *deviceAddr);
  return 0;
}

int main() {
  // 1. ï¼ˆå›ºå®šå†™æ³•ï¼‰device/streamåˆå§‹åŒ–ï¼Œå‚è€ƒacl APIæ‰‹å†Œ 
  // æ ¹æ®è‡ªå·±çš„å®é™…deviceå¡«å†™deviceId
  int32_t deviceId = 0;
  aclrtContext context;
  aclrtStream stream;
  auto ret = Init(deviceId, &context, &stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("Init acl failed. ERROR: %d\n", ret); return ret);

  // 2. æ„é€ è¾“å…¥ä¸è¾“å‡ºï¼Œéœ€è¦æ ¹æ®APIçš„æ¥å£è‡ªå®šä¹‰æ„é€ 
  std::vector<int64_t> qShape = {256, 1, 128};
  std::vector<int64_t> kShape = {256, 1, 128};
  std::vector<int64_t> vShape = {256, 1, 128};
  std::vector<int64_t> attenmaskShape = {256, 256};

  std::vector<int64_t> attentionOutShape = {256, 1, 128};
  std::vector<int64_t> softmaxMaxShape = {1, 1, 256, 8};
  std::vector<int64_t> softmaxSumShape = {1, 1, 256, 8};

  void* qDeviceAddr = nullptr;
  void* kDeviceAddr = nullptr;
  void* vDeviceAddr = nullptr;
  void* attenmaskDeviceAddr = nullptr;
  void* attentionOutDeviceAddr = nullptr;
  void* softmaxMaxDeviceAddr = nullptr;
  void* softmaxSumDeviceAddr = nullptr;

  aclTensor* q = nullptr;
  aclTensor* k = nullptr;
  aclTensor* v = nullptr;
  aclTensor* pse = nullptr;
  aclTensor* dropMask = nullptr;
  aclTensor* padding = nullptr;
  aclTensor* attenmask = nullptr;
  aclTensor* attentionOut = nullptr;
  aclTensor* softmaxMax = nullptr;
  aclTensor* softmaxSum = nullptr;
  aclTensor* softmaxOut = nullptr;

  std::vector<float> qHostData(32768, 1);
  std::vector<float> kHostData(32768, 1);
  std::vector<float> vHostData(32768, 1);
  std::vector<uint8_t> attenmaskHostData(65536, 0);
  std::vector<float> attentionOutHostData(32768, 0);
  std::vector<float> softmaxMaxHostData(2048, 3.0);
  std::vector<float> softmaxSumHostData(2048, 3.0);

  ret = CreateAclTensor(qHostData, qShape, &qDeviceAddr, aclDataType::ACL_FLOAT16, &q);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(kHostData, kShape, &kDeviceAddr, aclDataType::ACL_FLOAT16, &k);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(vHostData, vShape, &vDeviceAddr, aclDataType::ACL_FLOAT16, &v);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(attenmaskHostData, attenmaskShape, &attenmaskDeviceAddr, aclDataType::ACL_UINT8, &attenmask);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(attentionOutHostData, attentionOutShape, &attentionOutDeviceAddr, aclDataType::ACL_FLOAT16, &attentionOut);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(softmaxMaxHostData, softmaxMaxShape, &softmaxMaxDeviceAddr, aclDataType::ACL_FLOAT, &softmaxMax);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(softmaxSumHostData, softmaxSumShape, &softmaxSumDeviceAddr, aclDataType::ACL_FLOAT, &softmaxSum);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  
  std::vector<int64_t> prefixOp = {0};
  aclIntArray *prefix = aclCreateIntArray(prefixOp.data(), 1);
  double scaleValue = 0.088388;
  double keepProb = 1;
  int64_t preTokens = 65536;
  int64_t nextTokens = 65536;
  int64_t headNum = 1;
  int64_t innerPrecise = 0;
  int64_t sparseMode = 0;
  
  char layOut[5] = {'S', 'B', 'H', 0};
  
  // 3. è°ƒç”¨CANNç®—å­åº“APIï¼Œéœ€è¦ä¿®æ”¹ä¸ºå…·ä½“çš„Apiåç§°
  uint64_t workspaceSize = 0;
  aclOpExecutor* executor;
  
  // è°ƒç”¨aclnnFlashAttentionScoreç¬¬ä¸€æ®µæ¥å£
  ret = aclnnFlashAttentionScoreGetWorkspaceSize(
            q, k, v, pse, dropMask, padding, attenmask, prefix, scaleValue,
            keepProb, preTokens, nextTokens, headNum, layOut, innerPrecise,
            sparseMode, softmaxMax, softmaxSum, softmaxOut, attentionOut, &workspaceSize, &executor);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclnnFlashAttentionScoreGetWorkspaceSize failed. ERROR: %d\n", ret); return ret);
  
  // æ ¹æ®ç¬¬ä¸€æ®µæ¥å£è®¡ç®—å‡ºçš„workspaceSizeç”³è¯·deviceå†…å­˜
  void* workspaceAddr = nullptr;
  if (workspaceSize > 0) {
    ret = aclrtMalloc(&workspaceAddr, workspaceSize, ACL_MEM_MALLOC_HUGE_FIRST);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("allocate workspace failed. ERROR: %d\n", ret); return ret);
  }
  
  // è°ƒç”¨aclnnFlashAttentionScoreç¬¬äºŒæ®µæ¥å£
  ret = aclnnFlashAttentionScore(workspaceAddr, workspaceSize, executor, stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclnnFlashAttentionScore failed. ERROR: %d\n", ret); return ret);
  
  // 4. ï¼ˆå›ºå®šå†™æ³•ï¼‰åŒæ­¥ç­‰å¾…ä»»åŠ¡æ‰§è¡Œç»“æŸ
  ret = aclrtSynchronizeStream(stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtSynchronizeStream failed. ERROR: %d\n", ret); return ret);
  
  // 5. è·å–è¾“å‡ºçš„å€¼ï¼Œå°†deviceä¾§å†…å­˜ä¸Šçš„ç»“æœæ‹·è´è‡³hostä¾§ï¼Œéœ€è¦æ ¹æ®å…·ä½“APIçš„æ¥å£å®šä¹‰ä¿®æ”¹
  PrintOutResult(attentionOutShape, &attentionOutDeviceAddr);
  PrintOutResult(softmaxMaxShape, &softmaxMaxDeviceAddr);
  PrintOutResult(softmaxSumShape, &softmaxSumDeviceAddr);
  
  // 6. é‡Šæ”¾aclTensorå’ŒaclScalarï¼Œéœ€è¦æ ¹æ®å…·ä½“APIçš„æ¥å£å®šä¹‰ä¿®æ”¹
  aclDestroyTensor(q);
  aclDestroyTensor(k);
  aclDestroyTensor(v);
  aclDestroyTensor(attenmask);
  aclDestroyTensor(attentionOut);
  aclDestroyTensor(softmaxMax);
  aclDestroyTensor(softmaxSum);
  
  // 7. é‡Šæ”¾deviceèµ„æº
  aclrtFree(qDeviceAddr);
  aclrtFree(kDeviceAddr);
  aclrtFree(vDeviceAddr);
  aclrtFree(attenmaskDeviceAddr);
  aclrtFree(attentionOutDeviceAddr);
  aclrtFree(softmaxMaxDeviceAddr);
  aclrtFree(softmaxSumDeviceAddr);
  if (workspaceSize > 0) {
    aclrtFree(workspaceAddr);
  }
  aclrtDestroyStream(stream);
  aclrtDestroyContext(context);
  aclrtResetDevice(deviceId);
  aclFinalize();
  
  return 0;
}

```
