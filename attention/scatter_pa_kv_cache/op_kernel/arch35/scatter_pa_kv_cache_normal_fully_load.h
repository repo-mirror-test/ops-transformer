/**
 * Copyright (c) 2025 Huawei Technologies Co., Ltd.
 * This program is free software, you can redistribute it and/or modify it under the terms and conditions of
 * CANN Open Software License Agreement Version 2.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */
/*!
 * \file scatter_pa_kv_cache_normal_fully_load.h
 * \brief
 */

#ifndef SCATTER_PA_KV_CACHE_NORMAL_FULLY_LOAD_H_
#define SCATTER_PA_KV_CACHE_NORMAL_FULLY_LOAD_H_

#include "kernel_operator.h"
#include "common.h"

namespace ScatterPaKvCache {
using namespace AscendC;

template <typename T, typename IndexDtype, int64_t InOutMode>
class ScatterPaKvCacheNormalFullyLoad {
public:
    __aicore__ inline ScatterPaKvCacheNormalFullyLoad(TPipe *pipe, const ScatterPaKvCacheTilingData *__restrict tiling)
        : pipe_(pipe), tilingData_(tiling){};
    __aicore__ inline void Init(GM_ADDR key, GM_ADDR key_cache_in, GM_ADDR slot_mapping, GM_ADDR value,
                                GM_ADDR value_cache_in, GM_ADDR compress_lens, GM_ADDR compress_seq_offset,
                                GM_ADDR seq_lens, GM_ADDR key_cache_out, GM_ADDR value_cache_out);
    __aicore__ inline void Process();

private:
    __aicore__ inline void CopyIn(int64_t curBlockFactor);
    __aicore__ inline void CopyOut(int64_t curBlockFactor);
    __aicore__ inline int64_t RoundUp(int64_t x);
    __aicore__ inline void CalcStartIdx(LocalTensor<IndexDtype> slotMappingLocal, int64_t curBlockFactor,
                                        int64_t handleNumPerCore);

private:
    TPipe *pipe_;
    TQueBind<QuePosition::VECIN, QuePosition::VECOUT, 1> inputKeyQueue_;
    TQueBind<QuePosition::VECIN, QuePosition::VECOUT, 1> inputValueQueue_;

    GlobalTensor<T> inputKeyGm_;
    GlobalTensor<T> inputValueGm_;
    GlobalTensor<T> outputKeyCacheGm_;
    GlobalTensor<T> outputValueCacheGm_;
    GlobalTensor<IndexDtype> slotMappingGm_;

    const ScatterPaKvCacheTilingData *tilingData_;

    int64_t blockIdx_{0};

    TBuf<TPosition::VECCALC> kSlotMappingBuf_;
    TBuf<TPosition::VECCALC> vSlotMappingBuf_;
};

template <typename T, typename IndexDtype, int64_t InOutMode>
__aicore__ inline void ScatterPaKvCacheNormalFullyLoad<T, IndexDtype, InOutMode>::Init(
    GM_ADDR key, GM_ADDR key_cache_in, GM_ADDR slot_mapping, GM_ADDR value, GM_ADDR value_cache_in,
    GM_ADDR compress_lens, GM_ADDR compress_seq_offset, GM_ADDR seq_lens, GM_ADDR key_cache_out,
    GM_ADDR value_cache_out)
{
    blockIdx_ = GetBlockIdx();
    inputKeyGm_.SetGlobalBuffer((__gm__ T *)(key) +
                                GetBlockIdx() * tilingData_->blockFactor * tilingData_->kHandleNumPerCore);
    slotMappingGm_.SetGlobalBuffer((__gm__ IndexDtype *)(slot_mapping) + GetBlockIdx() * tilingData_->blockFactor);
    outputKeyCacheGm_.SetGlobalBuffer((__gm__ T *)(key_cache_out));
    int64_t maxBlockFactor = tilingData_->blockFactor > tilingData_->tailBlockFactor ? tilingData_->blockFactor :
                                                                                       tilingData_->tailBlockFactor;
    pipe_->InitBuffer(kSlotMappingBuf_, RoundUp(maxBlockFactor) * sizeof(IndexDtype));
    pipe_->InitBuffer(inputKeyQueue_, 1, maxBlockFactor * RoundUp(tilingData_->kHandleNumPerCore) * sizeof(T));
    if constexpr (InOutMode == DUAL_IN_OUT) {
        inputValueGm_.SetGlobalBuffer((__gm__ T *)(value) +
                                      GetBlockIdx() * tilingData_->blockFactor * tilingData_->vHandleNumPerCore);
        outputValueCacheGm_.SetGlobalBuffer((__gm__ T *)(value_cache_out));
        pipe_->InitBuffer(vSlotMappingBuf_, RoundUp(maxBlockFactor) * sizeof(IndexDtype));
        pipe_->InitBuffer(inputValueQueue_, 1, maxBlockFactor * RoundUp(tilingData_->vHandleNumPerCore) * sizeof(T));
    }
}

template <typename T, typename IndexDtype, int64_t InOutMode>
__aicore__ inline int64_t ScatterPaKvCacheNormalFullyLoad<T, IndexDtype, InOutMode>::RoundUp(int64_t x)
{
    int64_t elemNum = ONE_BLK_SIZE / sizeof(T);
    return (x + elemNum - 1) / elemNum * elemNum;
}

template <typename T, typename IndexDtype, int64_t InOutMode>
__aicore__ inline void ScatterPaKvCacheNormalFullyLoad<T, IndexDtype, InOutMode>::CalcStartIdx(
    LocalTensor<IndexDtype> slotMappingLocal, int64_t curBlockFactor, int64_t handleNumPerCore)
{
    DataCopyExtParams slotMappingParams{1, static_cast<uint32_t>(curBlockFactor * sizeof(IndexDtype)), 0, 0, 0};
    DataCopyPadExtParams<IndexDtype> padParamIdx = {false, static_cast<uint8_t>(0), static_cast<uint8_t>(0),
                                                    static_cast<IndexDtype>(0)};
    DataCopyPad(slotMappingLocal, slotMappingGm_, slotMappingParams, padParamIdx);
    event_t eventIdMTE2ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_V));
    SetFlag<HardEvent::MTE2_V>(eventIdMTE2ToV);
    WaitFlag<HardEvent::MTE2_V>(eventIdMTE2ToV);
    Muls(slotMappingLocal, slotMappingLocal, handleNumPerCore, curBlockFactor);
}

template <typename T, typename IndexDtype, int64_t InOutMode>
__aicore__ inline void ScatterPaKvCacheNormalFullyLoad<T, IndexDtype, InOutMode>::CopyIn(int64_t curBlockFactor)
{
    LocalTensor<T> inputKeyLocal = inputKeyQueue_.AllocTensor<T>();
    for (int64_t i = 0; i < curBlockFactor; i++) {
        DataCopy(inputKeyLocal[i * RoundUp(tilingData_->kHandleNumPerCore)],
                 inputKeyGm_[i * tilingData_->kHandleNumPerCore], RoundUp(tilingData_->kHandleNumPerCore));
    }
    inputKeyQueue_.EnQue(inputKeyLocal);
    if constexpr (InOutMode == DUAL_IN_OUT) {
        LocalTensor<T> inputValueLocal = inputValueQueue_.AllocTensor<T>();
        for (int64_t i = 0; i < curBlockFactor; i++) {
            DataCopy(inputValueLocal[i * RoundUp(tilingData_->vHandleNumPerCore)],
                     inputValueGm_[i * tilingData_->vHandleNumPerCore], RoundUp(tilingData_->vHandleNumPerCore));
        }
        inputValueQueue_.EnQue(inputValueLocal);
    }
}

template <typename T, typename IndexDtype, int64_t InOutMode>
__aicore__ inline void ScatterPaKvCacheNormalFullyLoad<T, IndexDtype, InOutMode>::CopyOut(int64_t curBlockFactor)
{
    LocalTensor<T> inputKeyLocal = inputKeyQueue_.DeQue<T>();
    LocalTensor<IndexDtype> kSlotMappingLocal = kSlotMappingBuf_.Get<IndexDtype>();

    DataCopyExtParams outKeyCacheParams = {
        static_cast<uint16_t>(1), static_cast<uint32_t>(tilingData_->kHandleNumPerCore * sizeof(T)),
        static_cast<uint32_t>(0), static_cast<uint32_t>(0), static_cast<uint32_t>(0)};

    event_t eventIdV2ToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdV2ToS);
    WaitFlag<HardEvent::V_S>(eventIdV2ToS);

    for (int64_t i = 0; i < curBlockFactor; i++) {
        int64_t kStartIdx = kSlotMappingLocal.GetValue(i);
        DataCopyPad(outputKeyCacheGm_[kStartIdx], inputKeyLocal[i * RoundUp(tilingData_->kHandleNumPerCore)],
                    outKeyCacheParams);
    }

    inputKeyQueue_.FreeTensor(inputKeyLocal);

    if constexpr (InOutMode == DUAL_IN_OUT) {
        LocalTensor<T> inputValueLocal = inputValueQueue_.DeQue<T>();
        LocalTensor<IndexDtype> vSlotMappingLocal = vSlotMappingBuf_.Get<IndexDtype>();
        DataCopyExtParams outValueCacheParams = {
            static_cast<uint16_t>(1), static_cast<uint32_t>(tilingData_->vHandleNumPerCore * sizeof(T)),
            static_cast<uint32_t>(0), static_cast<uint32_t>(0), static_cast<uint32_t>(0)};
        for (int64_t i = 0; i < curBlockFactor; i++) {
            int64_t vStartIdx = vSlotMappingLocal.GetValue(i);
            DataCopyPad(outputValueCacheGm_[vStartIdx], inputValueLocal[i * RoundUp(tilingData_->vHandleNumPerCore)],
                        outValueCacheParams);
        }
        inputValueQueue_.FreeTensor(inputValueLocal);
    }
}

template <typename T, typename IndexDtype, int64_t InOutMode>
__aicore__ inline void ScatterPaKvCacheNormalFullyLoad<T, IndexDtype, InOutMode>::Process()
{
    if (blockIdx_ >= tilingData_->usedCoreNum) {
        return;
    }
    int64_t curBlockFactor =
        (blockIdx_ == tilingData_->usedCoreNum - 1) ? tilingData_->tailBlockFactor : tilingData_->blockFactor;
    LocalTensor<IndexDtype> kSlotMappingLocal = kSlotMappingBuf_.Get<IndexDtype>();
    CalcStartIdx(kSlotMappingLocal, curBlockFactor, tilingData_->kHandleNumPerCore);
    if constexpr (InOutMode == DUAL_IN_OUT) {
        LocalTensor<IndexDtype> vSlotMappingLocal = vSlotMappingBuf_.Get<IndexDtype>();
        CalcStartIdx(vSlotMappingLocal, curBlockFactor, tilingData_->vHandleNumPerCore);
    }
    CopyIn(curBlockFactor);
    CopyOut(curBlockFactor);
}

} // namespace ScatterPaKvCache

#endif // SCATTER_PA_KV_CACHE_NORMAL_FULLY_LOAD_H_
