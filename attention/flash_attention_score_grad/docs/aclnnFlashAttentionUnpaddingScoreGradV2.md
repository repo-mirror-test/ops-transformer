# aclnnFlashAttentionUnpaddingScoreGradV2

[ğŸ“„ æŸ¥çœ‹æºç ](https://gitcode.com/cann/ops-transformer/tree/master/attention/flash_attention_score_grad)


## äº§å“æ”¯æŒæƒ…å†µ
|äº§å“      | æ˜¯å¦æ”¯æŒ |
|:----------------------------|:-----------:|
|<term>Atlas A3 è®­ç»ƒç³»åˆ—äº§å“/Atlas A3 æ¨ç†ç³»åˆ—äº§å“</term>|      âˆš     |
|<term>Atlas A2 è®­ç»ƒç³»åˆ—äº§å“</term>|      âˆš     |
|<term>Atlas A2 æ¨ç†ç³»åˆ—äº§å“</term>|      Ã—     |


## åŠŸèƒ½è¯´æ˜

- æ¥å£åŠŸèƒ½ï¼šè®­ç»ƒåœºæ™¯ä¸‹è®¡ç®—æ³¨æ„åŠ›çš„åå‘è¾“å‡ºï¼Œå³[aclnnFlashAttentionVarLenScoreV2](../../flash_attention_score/docs/aclnnFlashAttentionVarLenScoreV2.md)çš„åå‘è®¡ç®—ã€‚**è¯¥æ¥å£ç›¸è¾ƒäº[aclnnFlashAttentionUnpaddingScoreGrad](./aclnnFlashAttentionUnpaddingScoreGrad.md)æ¥å£ï¼Œæ–°å¢psetypeå‚æ•°**ï¼š

  -   psetype=1æ—¶ï¼Œä¸[aclnnFlashAttentionUnpaddingScoreGrad](./aclnnFlashAttentionUnpaddingScoreGrad.md)å®ç°ç›¸åŒã€‚
  -   psetype=å…¶ä»–å–å€¼æ—¶ï¼Œéœ€è¦å…ˆmulå†addã€‚

- è®¡ç®—å…¬å¼ï¼š

  å·²çŸ¥æ³¨æ„åŠ›çš„æ­£å‘è®¡ç®—å…¬å¼ï¼ˆpsetype=1æ—¶ï¼Œä¸[aclnnFlashAttentionUnpaddingScoreGrad](./aclnnFlashAttentionUnpaddingScoreGrad.md)è®¡ç®—å…¬å¼ç›¸åŒï¼Œpsetype=å…¶ä»–å–å€¼å…¬å¼å¦‚ä¸‹ï¼‰ï¼š

  $$
  Y=Dropout(Softmax(Mask(\frac{QK^T}{\sqrt{d}}+pse),atten\_mask),keep\_prob)V
  $$

  ä¸ºæ–¹ä¾¿è¡¨è¾¾ï¼Œä»¥å˜é‡$S$å’Œ$P$è¡¨ç¤ºè®¡ç®—å…¬å¼ï¼š

  $$
  S=Mask(\frac{QK^T}{\sqrt{d}}+pse),atten\_mask
  $$


  $$
  P=Dropout(Softmax(S),keep\_prob)
  $$


  $$
  Y=PV
  $$

  åˆ™æ³¨æ„åŠ›çš„åå‘è®¡ç®—å…¬å¼ä¸ºï¼š

  $$
  dV=P^TdY
  $$

  $$
  dQ=\frac{((dS)*K)}{\sqrt{d}}
  $$

  $$
  dK=\frac{((dS)^T*Q)}{\sqrt{d}}
  $$



## å‡½æ•°åŸå‹

æ¯ä¸ªç®—å­åˆ†ä¸º[ä¸¤æ®µå¼æ¥å£](../../../docs/zh/context/ä¸¤æ®µå¼æ¥å£.md)ï¼Œå¿…é¡»å…ˆè°ƒç”¨â€œaclnnFlashAttentionUnpaddingScoreGradV2GetWorkspaceSizeâ€æ¥å£è·å–è®¡ç®—æ‰€éœ€workspaceå¤§å°ä»¥åŠåŒ…å«äº†ç®—å­è®¡ç®—æµç¨‹çš„æ‰§è¡Œå™¨ï¼Œå†è°ƒç”¨â€œaclnnFlashAttentionUnpaddingScoreGradV2â€æ¥å£æ‰§è¡Œè®¡ç®—ã€‚

```c++
aclnnStatus aclnnFlashAttentionUnpaddingScoreGradV2GetWorkspaceSize(
  const aclTensor   *query,
  const aclTensor   *keyIn,
  const aclTensor   *value,
  const aclTensor   *dy,
  const aclTensor   *pseShiftOptional,
  const aclTensor   *dropMaskOptional,
  const aclTensor   *paddingMaskOptional,
  const aclTensor   *attenMaskOptional,
  const aclTensor   *softmaxMaxOptional,
  const aclTensor   *softmaxSumOptional,
  const aclTensor   *softmaxInOptional,
  const aclTensor   *attentionInOptional,
  const aclIntArray *prefixOptional,
  const aclIntArray *actualSeqQLenOptional,
  const aclIntArray *actualSeqKvLenOptional,
  const aclIntArray *qStartIdxOptional,
  const aclIntArray *kvStartIdxOptional,
  double             scaleValue,
  double             keepProb,
  int64_t            preTokens,
  int64_t            nextTokens,
  int64_t            headNum,
  char              *inputLayout,
  int64_t            innerPrecise,
  int64_t            sparseMode,
  int64_t            pseType,
  const aclTensor   *dqOut,
  const aclTensor   *dkOut,
  const aclTensor   *dvOut,
  const aclTensor   *dpseOut,
  uint64_t          *workspaceSize,
  aclOpExecutor    **executor)
```
```c++
aclnnStatus aclnnFlashAttentionUnpaddingScoreGradV2(
  void             *workspace,
  uint64_t          workspaceSize,
  aclOpExecutor    *executor,
  const aclrtStream stream)
```

## aclnnFlashAttentionUnpaddingScoreGradV2GetWorkspaceSize

- **å‚æ•°è¯´æ˜ï¼š**
  <table style="undefined;table-layout: fixed; width: 1529px"><colgroup>
    <col style="width: 198px">
    <col style="width: 120px">
    <col style="width: 289px">
    <col style="width: 302px">
    <col style="width: 238px">
    <col style="width: 106px">
    <col style="width: 130px">
    <col style="width: 146px">
  </colgroup>
  <thead>
    <tr>
      <th>å‚æ•°å</th>
      <th>è¾“å…¥/è¾“å‡º</th>
      <th>æè¿°</th>
      <th>ä½¿ç”¨è¯´æ˜</th>
      <th>æ•°æ®ç±»å‹</th>
      <th>æ•°æ®æ ¼å¼</th>
      <th>ç»´åº¦(shape)</th>
      <th>éè¿ç»­Tensor</th>
    </tr></thead>
  <tbody>
    <tr>
      <td>query</td>
      <td>è¾“å…¥</td>
      <td>å…¬å¼ä¸­çš„Qã€‚</td>
      <td>æ•°æ®ç±»å‹ä¸keyIn/valueä¸€è‡´ã€‚</td>
      <td>FLOAT16ã€BFLOAT16ã€FLOAT32</td>
      <td>ND</td>
      <td>[TND]</td>
      <td>âˆš</td>
    </tr>
    <tr>
      <td>keyIn</td>
      <td>è¾“å…¥</td>
      <td>å…¬å¼ä¸­çš„Kã€‚</td>
      <td>æ•°æ®ç±»å‹ä¸query/valueä¸€è‡´ã€‚</td>
      <td>FLOAT16ã€BFLOAT16ã€FLOAT32</td>
      <td>ND</td>
      <td>[TND]</td>
      <td>âˆš</td>
    </tr>
    <tr>
      <td>value</td>
      <td>è¾“å…¥</td>
      <td>å…¬å¼ä¸­çš„Vã€‚</td>
      <td>æ•°æ®ç±»å‹ä¸query/keyInä¸€è‡´ã€‚</td>
      <td>FLOAT16ã€BFLOAT16ã€FLOAT32</td>
      <td>ND</td>
      <td>[TND]</td>
      <td>âˆš</td>
    </tr>
    <tr>
      <td>dy</td>
      <td>è¾“å…¥</td>
      <td>å…¬å¼ä¸­çš„dYã€‚</td>
      <td>-</td>
      <td>FLOAT16ã€BFLOAT16ã€FLOAT32</td>
      <td>ND</td>
      <td>[TND]</td>
      <td>âˆš</td>
    </tr>
    <tr>
      <td>pseShiftOptional</td>
      <td>å¯é€‰è¾“å…¥</td>
      <td>å…¬å¼ä¸­çš„pseã€‚</td>
      <td>æ•°æ®ç±»å‹ä¸queryçš„æ•°æ®ç±»å‹ä¸€è‡´,è¯¥å‚æ•°éœ€è¦ä¸pseTypeé…å¥—ä½¿ç”¨ã€‚</td>
      <td>FLOAT16ã€BFLOAT16ã€FLOAT32</td>
      <td>ND</td>
      <td>[B,N,1024,Skv]ã€[1,N,1024,Skv]ã€[B,N]ã€[N]</td>
      <td>âˆš</td>
    </tr>
    <tr>
      <td>dropMaskOptional</td>
      <td>è¾“å…¥</td>
      <td>Dropoutæ©ç ã€‚</td>
      <td>-</td>
      <td>UINT8</td>
      <td>ND</td>
      <td>0ã€1</td>
      <td>âˆš</td>
    </tr>
    <tr>
      <td>paddingMaskOptional</td>
      <td>è¾“å…¥</td>
      <td>é¢„ç•™å‚æ•°ã€‚</td>
      <td>è°ƒç”¨æ—¶éœ€ä¼ ç©ºã€‚</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>qStartIdxOptional</td>
      <td>è¾“å…¥</td>
      <td>ä»£è¡¨å¤–åˆ‡åœºæ™¯ï¼Œå½“å‰åˆ†å—çš„queryçš„sequenceåœ¨å…¨å±€ä¸­çš„èµ·å§‹ç´¢å¼•ã€‚</td>
      <td>-</td>
      <td>INT64</td>
      <td>ND</td>
      <td>0ã€1</td>
      <td>-</td>
    </tr>
    <tr>
      <td>kvStartIdxOptional</td>
      <td>è¾“å…¥</td>
      <td>ä»£è¡¨å¤–åˆ‡åœºæ™¯ï¼Œå½“å‰åˆ†å—çš„queryçš„sequenceåœ¨å…¨å±€ä¸­çš„èµ·å§‹ç´¢å¼•ã€‚</td>
      <td>-</td>
      <td>INT64</td>
      <td>ND</td>
      <td>0ã€1</td>
      <td>-</td>
    </tr>
    <tr>
      <td>attenMaskOptional</td>
      <td>è¾“å…¥</td>
      <td>å…¬å¼ä¸­çš„atten_maskã€‚</td>
      <td>å–å€¼ä¸º1ä»£è¡¨è¯¥ä½ä¸å‚ä¸è®¡ç®—ï¼Œä¸º0ä»£è¡¨è¯¥ä½å‚ä¸è®¡ç®—ã€‚</td>
      <td>BOOLã€UINT8</td>
      <td>ND</td>
      <td>[B,N,Sq,Skv]ã€[B,1,Sq,Skv]ã€[1,1,Sq,Skv]ã€[Sq,Skv] </td>
      <td>âˆš</td>
    </tr>
    <tr>
      <td>softmaxMaxOptional</td>
      <td>è¾“å…¥</td>
      <td>æ­£å‘softmaxçš„ä¸­é—´è¾“å‡ºã€‚</td>
      <td>-</td>
      <td>FLOAT</td>
      <td>ND</td>
      <td>[N,T,8]</td>
      <td>âˆš</td>
    </tr>
    <tr>
      <td>softmaxSumOptional</td>
      <td>è¾“å…¥</td>
      <td>æ­£å‘softmaxçš„ä¸­é—´è¾“å‡ºã€‚</td>
      <td>-</td>
      <td>FLOAT</td>
      <td>ND</td>
      <td>[N,T,8]</td>
      <td>âˆš</td>
    </tr>
    <tr>
      <td>softmaxInOptional</td>
      <td>è¾“å…¥</td>
      <td>æ­£å‘softmaxçš„ä¸­é—´è¾“å‡ºã€‚</td>
      <td>æš‚æœªä½¿ç”¨ã€‚</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>attentionInOptional</td>
      <td>è¾“å…¥</td>
      <td>æ­£å‘æ³¨æ„åŠ›è¾“å‡ºã€‚</td>
      <td>ä¸queryæ•°æ®ç±»å‹ã€shapeä¸€è‡´ã€‚</td>
      <td>FLOAT16ã€BFLOAT16ã€FLOAT32</td>
      <td>ND</td>
      <td>[TND]</td>
      <td>âˆš</td>
    </tr>
    <tr>
      <td>prefixOptional</td>
      <td>è¾“å…¥</td>
      <td>prefixç¨€ç–åœºæ™¯æ¯ä¸ªBatchçš„Nã€‚</td>
      <td>-</td>
      <td>INT64</td>
      <td>ND</td>
      <td>0ã€1</td>
      <td>-</td>
    </tr>
    <tr>
      <td>actualSeqQLenOptional</td>
      <td>è¾“å…¥</td>
      <td>å®é™…Queryåºåˆ—é•¿åº¦ã€‚</td>
      <td>-</td>
      <td>INT64</td>
      <td>ND</td>
      <td>1</td>
      <td>-</td>
    </tr>
    <tr>
      <td>actualSeqKvLenOptional</td>
      <td>è¾“å…¥</td>
      <td>å®é™…Key/Valueåºåˆ—é•¿åº¦ã€‚</td>
      <td>-</td>
      <td>INT64</td>
      <td>ND</td>
      <td>1</td>
      <td>-</td>
    </tr>
    <tr>
      <td>dqOut</td>
      <td>è¾“å‡º</td>
      <td>å…¬å¼ä¸­çš„dQï¼ŒQueryæ¢¯åº¦ã€‚</td>
      <td>-</td>
      <td>FLOAT16ã€BFLOAT16ã€FLOAT32</td>
      <td>ND</td>
      <td>[TND]</td>
      <td>âˆš</td>
    </tr>
    <tr>
      <td>dkOut</td>
      <td>è¾“å‡º</td>
      <td>å…¬å¼ä¸­çš„dKï¼ŒKeyæ¢¯åº¦ã€‚</td>
      <td>-</td>
      <td>FLOAT16ã€BFLOAT16ã€FLOAT32</td>
      <td>ND</td>
      <td>[TND]</td>
      <td>âˆš</td>
    </tr>
    <tr>
      <td>dvOut</td>
      <td>è¾“å‡º</td>
      <td>å…¬å¼ä¸­çš„dVï¼ŒValueæ¢¯åº¦ã€‚</td>
      <td>-</td>
      <td>FLOAT16ã€BFLOAT16ã€FLOAT32</td>
      <td>ND</td>
      <td>[TND]</td>
      <td>âˆš</td>
    </tr>
    <tr>
      <td>dpseOut</td>
      <td>è¾“å‡º</td>
      <td>d(pse)æ¢¯åº¦ã€‚</td>
      <td>æš‚æœªä½¿ç”¨ã€‚</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>scaleValue</td>
      <td>è¾“å…¥</td>
      <td>scaleç¼©æ”¾ç³»æ•°ã€‚</td>
      <td>-</td>
      <td>DOUBLE</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>keepProb</td>
      <td>è¾“å…¥</td>
      <td>dropMaskOptionalä¸­1çš„æ¯”ä¾‹ã€‚</td>
      <td>-</td>
      <td>DOUBLE</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>preTokens</td>
      <td>è¾“å…¥</td>
      <td>ç¨€ç–è®¡ç®—æ—¶æ»‘çª—å·¦è¾¹ç•Œã€‚</td>
      <td>-</td>
      <td>INT64</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>nextTokens</td>
      <td>è¾“å…¥</td>
      <td>ç¨€ç–è®¡ç®—æ—¶æ»‘çª—å³è¾¹ç•Œã€‚</td>
      <td>-</td>
      <td>INT64</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>headNum</td>
      <td>è¾“å…¥</td>
      <td>å•å¡headæ•°é‡ï¼Œå³Queryçš„Nè½´é•¿åº¦ã€‚</td>
      <td>-</td>
      <td>INT64</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>inputLayout</td>
      <td>è¾“å…¥</td>
      <td>è¾“å…¥Q/K/Væ•°æ®æ’å¸ƒã€‚</td>
      <td>æ”¯æŒTNDã€‚</td>
      <td>String</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>innerPrecise</td>
      <td>è¾“å…¥</td>
      <td>å†…éƒ¨è®¡ç®—ç²¾åº¦æ§åˆ¶ã€‚</td>
      <td>æš‚æœªä½¿ç”¨ã€‚</td>
      <td>INT64</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>sparseMode</td>
      <td>è¾“å…¥</td>
      <td>ç¨€ç–æ¨¡å¼ã€‚</td>
      <td>æ”¯æŒé…ç½®0~8ã€‚ä¸æ”¯æŒ5ã€‚</td>
      <td>INT64</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>pseType</td>
      <td>è¾“å…¥</td>
      <td>æ•°æ®ç±»å‹æ”¯æŒINT64ã€‚</td>
      <td>æ”¯æŒé…ç½®å€¼ä¸º0ã€1ã€2ã€3ã€‚</td>
      <td>INT64</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>workspaceSize</td>
      <td>è¾“å‡º</td>
      <td>è¿”å›éœ€è¦Deviceä¾§ç”³è¯·çš„workspaceå¤§å°ã€‚</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td>executor</td>
      <td>è¾“å‡º</td>
      <td>è¿”å›opæ‰§è¡Œå™¨ï¼ŒåŒ…å«ç®—å­è®¡ç®—æµç¨‹ã€‚</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
      <td>-</td>
    </tr>
  </tbody>
  </table>

- **è¿”å›å€¼ï¼š**

  aclnnStatusï¼šè¿”å›çŠ¶æ€ç ï¼Œå…·ä½“å‚è§[aclnnè¿”å›ç ](../../../docs/zh/context/aclnnè¿”å›ç .md)ã€‚

  ç¬¬ä¸€æ®µæ¥å£å®Œæˆå…¥å‚æ ¡éªŒï¼Œå‡ºç°ä»¥ä¸‹åœºæ™¯æ—¶æŠ¥é”™ï¼š
  <table style="undefined;table-layout: fixed;width: 1202px"><colgroup>
  <col style="width: 262px">
  <col style="width: 121px">
  <col style="width: 819px">
  </colgroup>
  <thead>
    <tr>
      <th>è¿”å›ç </th>
      <th>é”™è¯¯ç </th>
      <th>æè¿°</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ACLNN_ERR_PARAM_NULLPTR</td>
      <td>161001</td>
      <td>ä¼ å…¥å‚æ•°æ˜¯å¿…é€‰è¾“å…¥ï¼Œè¾“å‡ºæˆ–è€…å¿…é€‰å±æ€§ï¼Œä¸”æ˜¯ç©ºæŒ‡é’ˆã€‚</td>
    </tr>
    <tr>
      <td rowspan="2">ACLNN_ERR_PARAM_INVALID</td>
      <td rowspan="2">161002</td>
      <td>queryã€keyInã€valueã€dyã€pseShiftOptionalã€dropMaskOptionalã€paddingMaskOptionalã€attenMaskOptionalã€softmaxMaxOptionalã€softmaxSumOptionalã€softmaxInOptionalã€attentionInOptionalã€dqOutã€dkOutã€dvOut çš„æ•°æ®ç±»å‹ä¸åœ¨æ”¯æŒçš„èŒƒå›´å†…ã€‚</td>
    </tr>
    <tr>
      <td>queryã€keyInã€valueã€dyã€pseShiftOptionalã€dropMaskOptionalã€paddingMaskOptionalã€attenMaskOptionalã€softmaxMaxOptionalã€softmaxSumOptionalã€softmaxInOptionalã€attentionInOptionalã€dqOutã€dkOutã€dvOut çš„æ•°æ®æ ¼å¼ä¸åœ¨æ”¯æŒçš„èŒƒå›´å†…ã€‚</td>
    </tr>
  </tbody>
  </table>

## aclnnFlashAttentionUnpaddingScoreGradV2

-   **å‚æ•°è¯´æ˜ï¼š**
    <table style="undefined;table-layout: fixed; width: 1154px"><colgroup>
    <col style="width: 153px">
    <col style="width: 121px">
    <col style="width: 880px">
    </colgroup>
    <thead>
      <tr>
        <th>å‚æ•°å</th>
        <th>è¾“å…¥/è¾“å‡º</th>
        <th>æè¿°</th>
      </tr></thead>
    <tbody>
      <tr>
        <td>workspace</td>
        <td>è¾“å…¥</td>
        <td>åœ¨Deviceä¾§ç”³è¯·çš„workspaceå†…å­˜åœ°å€ã€‚</td>
      </tr>
      <tr>
        <td>workspaceSize</td>
        <td>è¾“å…¥</td>
        <td>åœ¨Deviceä¾§ç”³è¯·çš„workspaceå¤§å°ï¼Œç”±ç¬¬ä¸€æ®µæ¥å£aclnnFlashAttentionUnpaddingScoreGradV2GetWorkspaceSizeè·å–ã€‚</td>
      </tr>
      <tr>
        <td>executor</td>
        <td>è¾“å…¥</td>
        <td>opæ‰§è¡Œå™¨ï¼ŒåŒ…å«äº†ç®—å­è®¡ç®—æµç¨‹ã€‚</td>
      </tr>
      <tr>
        <td>stream</td>
        <td>è¾“å…¥</td>
        <td>æŒ‡å®šæ‰§è¡Œä»»åŠ¡çš„Streamã€‚</td>
      </tr>
    </tbody>
    </table>

- **è¿”å›å€¼ï¼š**

  è¿”å›aclnnStatusçŠ¶æ€ç ï¼Œå…·ä½“å‚è§[aclnnè¿”å›ç ](../../../docs/zh/context/aclnnè¿”å›ç .md)ã€‚
## çº¦æŸè¯´æ˜

- ç¡®å®šæ€§è®¡ç®—ï¼š
  - aclnnFlashAttentionUnpaddingScoreGradV2é»˜è®¤éç¡®å®šæ€§å®ç°ï¼Œæ”¯æŒé€šè¿‡aclrtCtxSetSysParamOptå¼€å¯ç¡®å®šæ€§ã€‚
- è¯¥æ¥å£ä¸PyTorché…åˆä½¿ç”¨æ—¶ï¼Œéœ€è¦ä¿è¯CANNç›¸å…³åŒ…ä¸PyTorchç›¸å…³åŒ…çš„ç‰ˆæœ¬åŒ¹é…ã€‚
- è¾“å…¥queryã€keyã€valueã€dyçš„Bï¼šbatchsizeå¿…é¡»ç›¸ç­‰ï¼›inputLayoutå¿…é¡»ä¸€è‡´ã€‚
- è¾“å…¥queryã€keyã€valueçš„Dï¼šHead-Dimå¿…é¡»æ»¡è¶³(qD == kD && kD >= vD)ã€‚
- è¾“å…¥queryã€keyã€valueã€pseShiftOptionalçš„æ•°æ®ç±»å‹å¿…é¡»ä¸€è‡´ã€‚
- è¾“å…¥key/valueçš„shapeé™¤Då¤–å¿…é¡»ä¸€è‡´ï¼Œåœ¨query/key/valueçš„Då¤§å°ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œquery/dyçš„shapeå¿…é¡»ä¸€è‡´ã€‚
- æ”¯æŒè¾“å…¥queryçš„Nå’Œkey/valueçš„Nä¸ç›¸ç­‰ï¼Œä½†å¿…é¡»æˆæ¯”ä¾‹å…³ç³»ï¼Œå³Nq/Nkvå¿…é¡»æ˜¯é0æ•´æ•°ï¼ŒNqå–å€¼èŒƒå›´1~256ã€‚
- å…³äºæ•°æ®shapeçš„çº¦æŸï¼Œä»¥inputLayoutçš„TNDä¸ºä¾‹ï¼Œå…¶ä¸­ï¼š
    -   Tï¼šå–å€¼èŒƒå›´ä¸º1\~1Mã€‚
    -   Nï¼šå–å€¼èŒƒå›´ä¸º1\~256ã€‚
    -   Dï¼šå–å€¼èŒƒå›´ä¸º1\~768ã€‚
    -   KeepProb: å–å€¼èŒƒå›´ä¸º(0, 1]ã€‚
- queryã€keyã€valueæ•°æ®æ’å¸ƒæ ¼å¼ä»…æ”¯æŒTNDï¼ŒTæ˜¯Bå’ŒSåˆè½´ç´§å¯†æ’åˆ—çš„æ•°æ®ï¼ˆæ¯ä¸ªbatchçš„SeqLenQå’ŒSeqLenKVï¼‰ï¼Œå…¶ä¸­Bï¼ˆBatchï¼‰è¡¨ç¤ºè¾“å…¥æ ·æœ¬æ‰¹é‡å¤§å°ã€Sï¼ˆSeq-Lengthï¼‰è¡¨ç¤ºè¾“å…¥æ ·æœ¬åºåˆ—é•¿åº¦ã€Hï¼ˆHead-Sizeï¼‰è¡¨ç¤ºéšè—å±‚çš„å¤§å°ã€Nï¼ˆHead-Numï¼‰è¡¨ç¤ºå¤šå¤´æ•°ã€Dï¼ˆHead-Dimï¼‰è¡¨ç¤ºéšè—å±‚æœ€å°çš„å•å…ƒå°ºå¯¸ï¼Œä¸”æ»¡è¶³D=H/Nã€‚
- pseShiftOptionalï¼šå¦‚æœSqå¤§äº1024çš„æ¯ä¸ªbatchçš„Sqä¸Skvç­‰é•¿ä¸”æ˜¯sparseModeä¸º0ã€2ã€3çš„ä¸‹ä¸‰è§’æ©ç åœºæ™¯ï¼Œå¯ä½¿èƒ½alibiä½ç½®ç¼–ç å‹ç¼©ï¼Œæ­¤æ—¶åªéœ€è¦è¾“å…¥åŸå§‹PSEæœ€å1024è¡Œï¼Œå®ç°å†…å­˜ä¼˜åŒ–ï¼Œå³alibi_compress = ori_pse[:, :, -1024:, :]ï¼Œå…·ä½“å¦‚ä¸‹ï¼š
  - å‚æ•°æ¯ä¸ªbatchä¸ç›¸åŒæ—¶ï¼Œshapeä¸ºBNHSkv(H=1024)ã€‚
  - æ¯ä¸ªbatchç›¸åŒæ—¶ï¼Œshapeä¸º1NHSkv(H=1024)ã€‚
  - å¦‚æœpseTypeä¸º2æˆ–3çš„æ—¶å€™ï¼Œæ•°æ®ç±»å‹éœ€ä¸ºFLOAT32, å¯¹åº”shapeæ”¯æŒèŒƒå›´æ˜¯[B,N]æˆ–[N]ã€‚
  - å¦‚æœä¸ä½¿èƒ½è¯¥å‚æ•°ï¼ŒpseShiftOptionaléœ€è¦ä¼ å…¥nullptrï¼ŒpseTypeéœ€è¦ä¼ å…¥1ã€‚
- pseType å„ä¸ªå–å€¼å«ä¹‰
    | pseType     | å«ä¹‰                              |      å¤‡æ³¨   |
    | ----------- | --------------------------------- | ----------|
    | 0           | å¤–éƒ¨ä¼ å…¥pse å…ˆmulå†add              | - |
    | 1           | å¤–éƒ¨ä¼ å…¥pse å…ˆaddå†mul              | è·Ÿ[FlashAttentionUnpaddingScoreGrad](./aclnnFlashAttentionUnpaddingScoreGrad.md)å®ç°ä¸€è‡´ã€‚ |
    | 2           | å†…éƒ¨ç”Ÿæˆpse å…ˆmulå†add              | - |
    | 3           | å†…éƒ¨ç”Ÿæˆpse å…ˆmulå†addå†sqrt         | - |
- sparseModeçš„çº¦æŸå¦‚ä¸‹:
  - å½“æ‰€æœ‰çš„attenMaskOptionalçš„shapeå°äº2048ä¸”ç›¸åŒçš„æ—¶å€™ï¼Œå»ºè®®ä½¿ç”¨defaultæ¨¡å¼ï¼Œæ¥å‡å°‘å†…å­˜ä½¿ç”¨é‡ï¼›
  - é…ç½®ä¸º1ã€2ã€3ã€5æ—¶ï¼Œç”¨æˆ·é…ç½®çš„preTokensã€nextTokensä¸ä¼šç”Ÿæ•ˆï¼›
  - é…ç½®ä¸º0ã€4æ—¶ï¼Œé¡»ä¿è¯attenMaskOptionalä¸preTokensã€nextTokensçš„èŒƒå›´ä¸€è‡´ã€‚
  - ç”¨æˆ·ä¸ç‰¹æ„æŒ‡å®šæ—¶å»ºè®®ä¼ å…¥0ã€‚
  - sparseä¸åŒæ¨¡å¼çš„è¯¦ç»†è¯´æ˜è¯·å‚è§[sparseæ¨¡å¼è¯´æ˜](../../../docs/zh/context/sparse_modeå‚æ•°è¯´æ˜.md)ã€‚
  - é…ç½®ä¸º7æ—¶ï¼Œä¸æ”¯æŒå¯é€‰è¾“å…¥realShiftOptionalã€‚
  - é…ç½®ä¸º8æ—¶ï¼Œå½“æ¯ä¸ªsequenceçš„qã€kvç­‰é•¿æ—¶æ”¯æŒå¯é€‰è¾“å…¥realShiftOptionalï¼Œé’ˆå¯¹å…¨å±€åšpseç”Ÿæˆã€‚æ”¯æŒqæ–¹å‘è¿›è¡Œå¤–åˆ‡ï¼Œéœ€è¦å¤–åˆ‡å‰æ¯ä¸ªsequenceçš„qã€kvç­‰é•¿ï¼Œå¤–åˆ‡åä¼ å…¥çš„actualSeqQLenOptionalã€‚
- éƒ¨åˆ†åœºæ™¯ä¸‹ï¼Œå¦‚æœè®¡ç®—é‡è¿‡å¤§å¯èƒ½ä¼šå¯¼è‡´ç®—å­æ‰§è¡Œè¶…æ—¶(aicore errorç±»å‹æŠ¥é”™ï¼ŒerrorSträ¸ºï¼štimeout or trap error)ï¼Œæ­¤æ—¶å»ºè®®åšè½´åˆ‡åˆ†å¤„ç†ï¼Œæ³¨ï¼šè¿™é‡Œçš„è®¡ç®—é‡ä¼šå—Bã€Sã€Nã€Dç­‰å‚æ•°çš„å½±å“ï¼Œå€¼è¶Šå¤§è®¡ç®—é‡è¶Šå¤§ã€‚
- prefixOptionalç¨€ç–è®¡ç®—ä»…æ”¯æŒå‹ç¼©åœºæ™¯ï¼ŒsparseMode=6ï¼Œå½“Sq > Skvæ—¶ï¼Œprefixçš„Nå€¼å–å€¼èŒƒå›´\[0, Skv\]ï¼Œå½“Sq <= Skvæ—¶ï¼Œprefixçš„Nå€¼å–å€¼èŒƒå›´\[Skv-Sq, Skv\]ã€‚
[0] - actualSeqKvLenOptional[0] + qStartIdxOptional - kvStartIdxOptional == 0ï¼ˆæœ¬åŠŸèƒ½å±å®éªŒæ€§åŠŸèƒ½ï¼‰ã€‚
- actualSeqQLenOptionalè¾“å…¥æ”¯æŒæŸä¸ªBatchä¸Šçš„Sé•¿åº¦ä¸º0ï¼Œæ­¤æ—¶ä¸æ”¯æŒå¯é€‰è¾“å…¥pseShiftOptionalã€‚actualSeqQLenOptionalçš„é•¿åº¦å–å€¼èŒƒå›´ä¸º1\~2Kã€‚å½“å­˜åœ¨prefixOptionalè¾“å…¥çš„æ—¶å€™ï¼Œå…¶é•¿åº¦æœ€å¤§æ”¯æŒ1Kã€‚
- å…³äºsoftmaxMaxä¸softmaxSumå‚æ•°çš„çº¦æŸï¼šè¾“å…¥æ ¼å¼å›ºå®šä¸º\[B, N, S, 8\]ï¼ŒTNDçš„è¾“å…¥æ ¼å¼é™¤å¤–ï¼Œæ­¤æ—¶ä¸º\[T, N, 8\]ï¼Œæ³¨ï¼šT=B*Sã€‚
- headNumçš„å–å€¼å¿…é¡»å’Œä¼ å…¥çš„Queryä¸­çš„Nå€¼ä¿æŒä¸€è‡´ã€‚


## è°ƒç”¨ç¤ºä¾‹

è°ƒç”¨ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼Œä»…ä¾›å‚è€ƒï¼Œå…·ä½“ç¼–è¯‘å’Œæ‰§è¡Œè¿‡ç¨‹è¯·å‚è€ƒ[ç¼–è¯‘ä¸è¿è¡Œæ ·ä¾‹](../../../docs/zh/context/ç¼–è¯‘ä¸è¿è¡Œæ ·ä¾‹.md)ã€‚

```C++
#include <iostream>
#include <vector>
#include "acl/acl.h"
#include "aclnnop/aclnn_flash_attention_score_grad.h"

#define CHECK_RET(cond, return_expr) \
  do {                               \
    if (!(cond)) {                   \
      return_expr;                   \
    }                                \
  } while (0)

#define LOG_PRINT(message, ...)     \
  do {                              \
    printf(message, ##__VA_ARGS__); \
  } while (0)

int64_t GetShapeSize(const std::vector<int64_t>& shape) {
  int64_t shapeSize = 1;
  for (auto i : shape) {
    shapeSize *= i;
  }
  return shapeSize;
}

void PrintOutResult(std::vector<int64_t> &shape, void** deviceAddr) {
  auto size = GetShapeSize(shape);
  std::vector<float> resultData(size, 0);
  auto ret = aclrtMemcpy(resultData.data(), resultData.size() * sizeof(resultData[0]),
                         *deviceAddr, size * sizeof(resultData[0]), ACL_MEMCPY_DEVICE_TO_HOST);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("copy result from device to host failed. ERROR: %d\n", ret); return);
  for (int64_t i = 0; i < size; i++) {
    LOG_PRINT("mean result[%ld] is: %f\n", i, resultData[i]);
  }
}

int Init(int32_t deviceId, aclrtStream* stream) {
  // å›ºå®šå†™æ³•ï¼Œèµ„æºåˆå§‹åŒ–
  auto ret = aclInit(nullptr);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclInit failed. ERROR: %d\n", ret); return ret);
  ret = aclrtSetDevice(deviceId);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtSetDevice failed. ERROR: %d\n", ret); return ret);
  ret = aclrtCreateStream(stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtCreateStream failed. ERROR: %d\n", ret); return ret);
  return 0;
}

template <typename T>
int CreateAclTensor(const std::vector<T>& hostData, const std::vector<int64_t>& shape, void** deviceAddr,
                    aclDataType dataType, aclTensor** tensor) {
  auto size = GetShapeSize(shape) * sizeof(T);
  // è°ƒç”¨aclrtMallocç”³è¯·deviceä¾§å†…å­˜
  auto ret = aclrtMalloc(deviceAddr, size, ACL_MEM_MALLOC_HUGE_FIRST);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtMalloc failed. ERROR: %d\n", ret); return ret);
  // è°ƒç”¨aclrtMemcpyå°†hostä¾§æ•°æ®æ‹·è´åˆ°deviceä¾§å†…å­˜ä¸Š
  ret = aclrtMemcpy(*deviceAddr, size, hostData.data(), size, ACL_MEMCPY_HOST_TO_DEVICE);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtMemcpy failed. ERROR: %d\n", ret); return ret);

  // è®¡ç®—è¿ç»­tensorçš„strides
  std::vector<int64_t> strides(shape.size(), 1);
  for (int64_t i = shape.size() - 2; i >= 0; i--) {
    strides[i] = shape[i + 1] * strides[i + 1];
  }

  // è°ƒç”¨aclCreateTensoræ¥å£åˆ›å»ºaclTensor
  *tensor = aclCreateTensor(shape.data(), shape.size(), dataType, strides.data(), 0, aclFormat::ACL_FORMAT_ND,
                            shape.data(), shape.size(), *deviceAddr);
  return 0;
}

int main() {
  // 1. ï¼ˆå›ºå®šå†™æ³•ï¼‰device/streamåˆå§‹åŒ–ï¼Œå‚è€ƒacl APIæ‰‹å†Œ
  // æ ¹æ®è‡ªå·±çš„å®é™…deviceå¡«å†™deviceId
  int32_t deviceId = 0;
  aclrtStream stream;
  auto ret = Init(deviceId, &stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("Init acl failed. ERROR: %d\n", ret); return ret);

  // 2. æ„é€ è¾“å…¥ä¸è¾“å‡ºï¼Œéœ€è¦æ ¹æ®APIçš„æ¥å£è‡ªå®šä¹‰æ„é€ 
  std::vector<int64_t> qShape = {256, 1, 128};
  std::vector<int64_t> kShape = {256, 1, 128};
  std::vector<int64_t> vShape = {256, 1, 128};
  std::vector<int64_t> dxShape = {256, 1, 128};
  std::vector<int64_t> attenmaskShape = {256, 256};
  std::vector<int64_t> softmaxMaxShape = {256, 1, 8};
  std::vector<int64_t> softmaxSumShape = {256, 1, 8};
  std::vector<int64_t> attentionInShape = {256, 1, 128};

  std::vector<int64_t> dqShape = {256, 1, 128};
  std::vector<int64_t> dkShape = {256, 1, 128};
  std::vector<int64_t> dvShape = {256, 1, 128};

  void* qDeviceAddr = nullptr;
  void* kDeviceAddr = nullptr;
  void* vDeviceAddr = nullptr;
  void* dxDeviceAddr = nullptr;
  void* attenmaskDeviceAddr = nullptr;
  void* softmaxMaxDeviceAddr = nullptr;
  void* softmaxSumDeviceAddr = nullptr;
  void* attentionInDeviceAddr = nullptr;
  void* dqDeviceAddr = nullptr;
  void* dkDeviceAddr = nullptr;
  void* dvDeviceAddr = nullptr;

  aclTensor* q = nullptr;
  aclTensor* k = nullptr;
  aclTensor* v = nullptr;
  aclTensor* dx = nullptr;
  aclTensor* pse = nullptr;
  aclTensor* dropMask = nullptr;
  aclTensor* padding = nullptr;
  aclTensor* attenmask = nullptr;
  aclTensor* softmaxMax = nullptr;
  aclTensor* softmaxSum = nullptr;
  aclTensor* softmaxIn = nullptr;
  aclTensor* attentionIn = nullptr;
  aclTensor* dq = nullptr;
  aclTensor* dk = nullptr;
  aclTensor* dv = nullptr;
  aclTensor* dpse = nullptr;

  std::vector<float> qHostData(32768, 1);
  std::vector<float> kHostData(32768, 1);
  std::vector<float> vHostData(32768, 1);
  std::vector<float> dxHostData(32768, 1);
  std::vector<uint8_t> attenmaskHostData(65536, 0);
  std::vector<float> softmaxMaxHostData(2048, 3.0);
  std::vector<float> softmaxSumHostData(2048, 3.0);
  std::vector<float> attentionInHostData(32768, 1);
  std::vector<float> dqHostData(32768, 0);
  std::vector<float> dkHostData(32768, 0);
  std::vector<float> dvHostData(32768, 0);

  ret = CreateAclTensor(qHostData, qShape, &qDeviceAddr, aclDataType::ACL_FLOAT16, &q);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(kHostData, kShape, &kDeviceAddr, aclDataType::ACL_FLOAT16, &k);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(vHostData, vShape, &vDeviceAddr, aclDataType::ACL_FLOAT16, &v);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(dxHostData, dxShape, &dxDeviceAddr, aclDataType::ACL_FLOAT16, &dx);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(attenmaskHostData, attenmaskShape, &attenmaskDeviceAddr, aclDataType::ACL_UINT8, &attenmask);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(softmaxMaxHostData, softmaxMaxShape, &softmaxMaxDeviceAddr, aclDataType::ACL_FLOAT, &softmaxMax);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(softmaxSumHostData, softmaxSumShape, &softmaxSumDeviceAddr, aclDataType::ACL_FLOAT, &softmaxSum);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(attentionInHostData, attentionInShape, &attentionInDeviceAddr, aclDataType::ACL_FLOAT16, &attentionIn);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(dqHostData, dqShape, &dqDeviceAddr, aclDataType::ACL_FLOAT16, &dq);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(dkHostData, dkShape, &dkDeviceAddr, aclDataType::ACL_FLOAT16, &dk);
  CHECK_RET(ret == ACL_SUCCESS, return ret);
  ret = CreateAclTensor(dvHostData, dvShape, &dvDeviceAddr, aclDataType::ACL_FLOAT16, &dv);
  CHECK_RET(ret == ACL_SUCCESS, return ret);

  std::vector<int64_t> prefixOp = {0};
  aclIntArray* prefix = aclCreateIntArray(prefixOp.data(), 1);
  std::vector<int64_t>  acSeqQLenOp = {256};
  std::vector<int64_t>  acSeqKvLenOp = {256};
  aclIntArray* acSeqQLen = aclCreateIntArray(acSeqQLenOp.data(), acSeqQLenOp.size());
  aclIntArray* acSeqKvLen = aclCreateIntArray(acSeqKvLenOp.data(), acSeqKvLenOp.size());
  std::vector<int64_t> qStartIdxOp = {0};
  std::vector<int64_t> kvStartIdxOp = {0};
  aclIntArray *qStartIdx = aclCreateIntArray(qStartIdxOp.data(), 1);
  aclIntArray *kvStartIdx = aclCreateIntArray(kvStartIdxOp.data(), 1);
  double scaleValue = 0.088388;
  double keepProb = 1;
  int64_t preTokens = 65536;
  int64_t nextTokens = 65536;
  int64_t headNum = 1;
  int64_t innerPrecise = 0;
  int64_t sparseMode = 0;
  int64_t pseType = 1;
  char layOut[5] = {'T', 'N', 'D', 0};

  // 3. è°ƒç”¨CANNç®—å­åº“APIï¼Œéœ€è¦ä¿®æ”¹ä¸ºå…·ä½“çš„Apiåç§°
  uint64_t workspaceSize = 0;
  aclOpExecutor* executor;

  // è°ƒç”¨aclnnFlashAttentionUnpaddingScoreGradV2ç¬¬ä¸€æ®µæ¥å£
  ret = aclnnFlashAttentionUnpaddingScoreGradV2GetWorkspaceSize(q, k, v, dx, pse, dropMask, padding,
            attenmask, softmaxMax, softmaxSum, softmaxIn, attentionIn, prefix, acSeqQLen, acSeqKvLen, qStartIdx, kvStartIdx,
            scaleValue, keepProb, preTokens, nextTokens, headNum, layOut, innerPrecise, sparseMode, pseType,
            dq, dk, dv, dpse, &workspaceSize, &executor);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclnnFlashAttentionUnpaddingScoreGradV2GetWorkspaceSize failed. ERROR: %d\n", ret); return ret);

  // æ ¹æ®ç¬¬ä¸€æ®µæ¥å£è®¡ç®—å‡ºçš„workspaceSizeç”³è¯·deviceå†…å­˜
  void* workspaceAddr = nullptr;
  if (workspaceSize > 0) {
    ret = aclrtMalloc(&workspaceAddr, workspaceSize, ACL_MEM_MALLOC_HUGE_FIRST);
    CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("allocate workspace failed. ERROR: %d\n", ret); return ret);
  }

  // è°ƒç”¨aclnnFlashAttentionUnpaddingScoreGradV2ç¬¬äºŒæ®µæ¥å£
  ret = aclnnFlashAttentionUnpaddingScoreGradV2(workspaceAddr, workspaceSize, executor, stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclnnFlashAttentionUnpaddingScoreGradV2 failed. ERROR: %d\n", ret); return ret);

  // 4. ï¼ˆå›ºå®šå†™æ³•ï¼‰åŒæ­¥ç­‰å¾…ä»»åŠ¡æ‰§è¡Œç»“æŸ
  ret = aclrtSynchronizeStream(stream);
  CHECK_RET(ret == ACL_SUCCESS, LOG_PRINT("aclrtSynchronizeStream failed. ERROR: %d\n", ret); return ret);

  // 5. è·å–è¾“å‡ºçš„å€¼ï¼Œå°†deviceä¾§å†…å­˜ä¸Šçš„ç»“æœæ‹·è´è‡³hostä¾§ï¼Œéœ€è¦æ ¹æ®å…·ä½“APIçš„æ¥å£å®šä¹‰ä¿®æ”¹
  PrintOutResult(dqShape, &dqDeviceAddr);
  PrintOutResult(dkShape, &dkDeviceAddr);
  PrintOutResult(dvShape, &dvDeviceAddr);

  // 6. é‡Šæ”¾aclTensorå’ŒaclScalarï¼Œéœ€è¦æ ¹æ®å…·ä½“APIçš„æ¥å£å®šä¹‰ä¿®æ”¹
  aclDestroyTensor(q);
  aclDestroyTensor(k);
  aclDestroyTensor(v);
  aclDestroyTensor(dx);
  aclDestroyTensor(attenmask);
  aclDestroyTensor(softmaxMax);
  aclDestroyTensor(softmaxSum);
  aclDestroyTensor(attentionIn);
  aclDestroyTensor(dq);
  aclDestroyTensor(dk);
  aclDestroyTensor(dv);

  // 7. é‡Šæ”¾deviceèµ„æº
  aclrtFree(qDeviceAddr);
  aclrtFree(kDeviceAddr);
  aclrtFree(vDeviceAddr);
  aclrtFree(dxDeviceAddr);
  aclrtFree(attenmaskDeviceAddr);
  aclrtFree(softmaxMaxDeviceAddr);
  aclrtFree(softmaxSumDeviceAddr);
  aclrtFree(attentionInDeviceAddr);
  aclrtFree(dqDeviceAddr);
  aclrtFree(dkDeviceAddr);
  aclrtFree(dvDeviceAddr);
  if (workspaceSize > 0) {
    aclrtFree(workspaceAddr);
  }
  aclrtDestroyStream(stream);
  aclrtResetDevice(deviceId);
  aclFinalize();

  return 0;
}

```