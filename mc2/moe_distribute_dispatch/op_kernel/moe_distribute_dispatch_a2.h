/**
 * Copyright (c) 2025 Huawei Technologies Co., Ltd.
 * This program is free software, you can redistribute it and/or modify it under the terms and conditions of
 * CANN Open Software License Agreement Version 2.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

/*!
 * \file moe_distribute_dispatch_a2.h
 * \brief
 */

#ifndef MOE_DISTRIBUTE_DISPATCH_A2_H
#define MOE_DISTRIBUTE_DISPATCH_A2_H
#include <climits>
#include "kernel_operator.h"
#include "kernel_tiling/kernel_tiling.h"
#include "moe_distribute_dispatch_tiling.h"
#include "../common/inc/kernel/moe_distribute_base.h"

namespace MoeDistributeDispatchA2Impl {
constexpr static uint8_t BUFFER_NUM = 2;
constexpr static uint32_t PING_IDX = 0;
constexpr static uint32_t PONG_IDX = 1;
constexpr static uint32_t DATA_OFFSET = 512; // 数据空间起始偏移
constexpr static uint32_t STATE_SIZE = 1024 * 1024; // 1M
constexpr static uint32_t UB_ALIGN = 32; // UB按32字节对齐
constexpr static uint32_t BITS32_PER_BLOCK = 8;
constexpr static uint32_t BITS16_PER_BLOCK = 16;
constexpr static uint32_t BITS32_PER_REPEAT = 64;
constexpr static uint32_t BITS16_PER_REPEAT = 128;
constexpr static uint32_t BW_ITEM_SIZE = 32; // = sizeof(BatchWriteItem)
constexpr static uint32_t U64_PER_ITEM = BW_ITEM_SIZE / sizeof(uint64_t);
constexpr static uint32_t U32_PER_ITEM = BW_ITEM_SIZE / sizeof(uint32_t);
constexpr static uint32_t SKIP_OFFSET = 512;
constexpr static int32_t FLAG_VALUE = 0xFFFFFFFF;

template <typename T1, typename T2>
__aicore__ inline T2 RoundUp(const T1 val, const T2 align) {
    return (static_cast<T2>(val) + align - 1) / align * align;
}

template<AscendC::HardEvent event>
__aicore__ inline void SyncFunc() {
    int32_t eventID = static_cast<int32_t>(GetTPipePtr()->FetchEventID(event));
    AscendC::SetFlag<event>(eventID);
    AscendC::WaitFlag<event>(eventID);
}

struct TaskInfo {
    uint32_t startTaskId{0};
    uint32_t endTaskId{0};
    uint32_t taskNum{0};

    __aicore__ inline void SplitCore(uint32_t taskNumTotal, uint32_t aivNum, uint32_t aivId) {
        if (unlikely(aivNum == 0)) {
            startTaskId = endTaskId = taskNum = 0;
            return;
        }

        const uint32_t baseNum = taskNumTotal / aivNum;
        const uint32_t remainder = taskNumTotal % aivNum;

        const bool hasExtraTask = aivId < remainder;
        taskNum = baseNum + static_cast<uint32_t>(hasExtraTask);
        startTaskId = baseNum * aivId + (hasExtraTask ? aivId : remainder);
        endTaskId = startTaskId + taskNum;
    }
};

#define TemplateMC2TypeA2Class typename XType, typename ExpandXOutType, bool StaticQuant, bool DynamicQuant, bool IsSmoothScaleExist
#define TemplateMC2TypeA2Func XType, ExpandXOutType, StaticQuant, DynamicQuant, IsSmoothScaleExist

using namespace AscendC;
template <TemplateMC2TypeA2Class>
class MoeDistributeDispatchA2 {
public:
    __aicore__ inline MoeDistributeDispatchA2() {};
    __aicore__ inline void Init(GM_ADDR x, GM_ADDR expertIds, GM_ADDR scales, GM_ADDR xActiveMask, GM_ADDR performanceInfo,
        GM_ADDR expandXOut, GM_ADDR dynamicScalesOut, GM_ADDR expandIdxOut, GM_ADDR expertTokenNumsOut, GM_ADDR epRecvCountsOut,
        GM_ADDR workspaceGM, TPipe *pipe, GM_ADDR tilingGM);
    __aicore__ inline void Process();
private:
    __aicore__ inline void AllocTensor();
    __aicore__ inline void InitStatusTensor();
    __aicore__ inline void IndexSort();
    __aicore__ inline void SendToMoeExpert();
    __aicore__ inline void LocalWindowCopy();
    __aicore__ inline void GetStatusCumSum();
    __aicore__ inline void WaitDispatch();
    __aicore__ inline void ConstructBatchWriteInfo();
    __aicore__ inline void ReorderTokens();
    __aicore__ inline void QuantProcess(uint32_t expertIndex, TEventID eventId);
    __aicore__ inline void TokenActiveMaskCal();
    __aicore__ inline void ExpertActiveMaskCal();
    __aicore__ inline void ZeroComputeExpertMaskCal();
    __aicore__ inline void CalValidTokenCount();
    __aicore__ inline void CleanUpFlags();
    __aicore__ inline void CopyPerformanceInfo();
    TPipe *tpipe_{nullptr};
    GlobalTensor<XType> xGMTensor_;
    GlobalTensor<int32_t> expertIdsGMTensor_;
    GlobalTensor<float> scalesGMTensor_;
    GlobalTensor<ExpandXOutType> expandXOutGMTensor_;
    GlobalTensor<float> dynamicScalesOutGMTensor_;
    GlobalTensor<int32_t> windowInstatusTensor_;
    GlobalTensor<uint32_t> batchWriteInfoTensor_;
    GlobalTensor<int32_t> sendStatusTensor_;
    GlobalTensor<uint32_t> bufferChosenGlobal_;
    GlobalTensor<int8_t> xActiveMaskGMTensor_;
    GlobalTensor<int32_t> performanceInfoI32GMTensor_;

    LocalTensor<XType> xTensor_[BUFFER_NUM];
    LocalTensor<ExpandXOutType> xOutTensor_[BUFFER_NUM];
    LocalTensor<int32_t> expertCountTensor_;
    LocalTensor<int32_t> expertIdsTensor_;
    LocalTensor<int32_t> statusTensor_;
    LocalTensor<uint64_t> batchWriteU64Tensor_;
    LocalTensor<uint32_t> batchWriteU32Tensor_;
    LocalTensor<uint32_t> expertCumsumTensor_;
    LocalTensor<int32_t> validExpIndexTensor_;
    LocalTensor<uint16_t> zeroExpertMaskTensor_;
    LocalTensor<float> smoothScalesTensor_;
    LocalTensor<float> xFloatTensor_;
    LocalTensor<int32_t> flagTensor_;
    LocalTensor<int8_t> xActiveMaskTensor_;
    LocalTensor<half> xActiveMaskHalfTensor_;
    LocalTensor<int32_t> epRecvCountsTempLocal_;
    LocalTensor<int32_t> epRecvCountsOutLocal_;
    LocalTensor<int32_t> performanceInfoI32Tensor_;

    GM_ADDR expandIdxOutGM_;
    GM_ADDR expertTokenNumsOutGM_;
    GM_ADDR epRecvCountsGM_;
    GM_ADDR windowInGM_;
    GM_ADDR windowOutGM_;
    GM_ADDR batchWriteInfo_;

    // tiling侧已确保数据上限，相乘不会越界，因此统一采用uint32_t进行处理
    uint32_t axisBS_{0};
    uint32_t axisH_{0};
    uint32_t axisK_{0};
    uint32_t aivNum_{0};
    uint32_t expertIdsCnt_{0};
    uint32_t worldSize_{0};
    uint32_t rankId_{0};
    uint32_t aivId_{0};
    uint32_t moeExpertNum_{0}; // moe专家卡数, 等于worldSize_ - 共享专家卡数
    uint32_t hCommuSize_{0};
    uint32_t axisHCommu_{0};
    uint32_t localMoeExpertNum_{0};
    uint32_t localMoeExpertNumAlign_{0};
    uint32_t dataSizePerRank_{0};
    uint32_t dataSize_{0};
    uint32_t bufferChosen_{0};
    uint32_t totalSize_{0};
    uint64_t activeMaskBsCnt_{0};
    uint32_t expertTokenNumsType_{0};
    int32_t zeroComputeExpertNum_{0};
    uint64_t sendToMoeExpTokenCnt_{0};
    uint32_t statusEntryCount_{0};
    bool isTokenMaskFlag_{false};
    bool isExpertMaskFlag_{false};
    uint32_t performanceInfoSize_{0};
    bool needPerformanceInfo_{false};
    TaskInfo worldTaskInfo_;
    Hccl<HCCL_SERVER_TYPE_AICPU> hccl_;
    __gm__ HcclOpResParam *winContext_{nullptr};
};

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::Init(
    GM_ADDR x, GM_ADDR expertIds, GM_ADDR scales, GM_ADDR xActiveMask, GM_ADDR performanceInfo, GM_ADDR expandXOut,
    GM_ADDR dynamicScalesOut, GM_ADDR expandIdxOut, GM_ADDR expertTokenNumsOut, GM_ADDR epRecvCountsOut,
    GM_ADDR workspaceGM, TPipe *pipe, GM_ADDR tilingGM)
{
    tpipe_ = pipe;
    GET_TILING_DATA_WITH_STRUCT(MoeDistributeDispatchA2TilingData, tilingData, tilingGM);

    auto contextGM0 = AscendC::GetHcclContext<HCCL_GROUP_ID_0>();
    hccl_.InitV2(contextGM0, &tilingData);
    hccl_.SetCcTilingV2(offsetof(MoeDistributeDispatchA2TilingData, mc2CcTiling));

    winContext_ = (__gm__ HcclOpResParam *)contextGM0;
    rankId_ = tilingData.moeDistributeDispatchInfo.epRankId;
    windowInGM_ = hccl_.GetWindowsInAddr(rankId_);
    windowOutGM_ = hccl_.GetWindowsOutAddr(rankId_);

    axisBS_ = tilingData.moeDistributeDispatchInfo.bs;
    axisH_ = tilingData.moeDistributeDispatchInfo.h;
    axisK_ = tilingData.moeDistributeDispatchInfo.k;
    aivNum_ = tilingData.moeDistributeDispatchInfo.aivNum;
    worldSize_ = tilingData.moeDistributeDispatchInfo.epWorldSize;
    expertTokenNumsType_ = tilingData.moeDistributeDispatchInfo.expertTokenNumsType;
    isTokenMaskFlag_ = tilingData.moeDistributeDispatchInfo.isTokenMask;
    isExpertMaskFlag_ = tilingData.moeDistributeDispatchInfo.isExpertMask;
    zeroComputeExpertNum_ = tilingData.moeDistributeDispatchInfo.zeroComputeExpertNum;
    totalSize_ = winContext_->winSize / BUFFER_NUM;
    dataSize_ = totalSize_ - STATE_SIZE;
    dataSizePerRank_ = dataSize_ / worldSize_;
    moeExpertNum_ = tilingData.moeDistributeDispatchInfo.moeExpertNum;
    localMoeExpertNum_ = moeExpertNum_ / worldSize_;
    aivId_ = GetBlockIdx();
    expertIdsCnt_ = axisBS_ * axisK_;
    localMoeExpertNumAlign_ = (localMoeExpertNum_ + BITS32_PER_BLOCK - 1) / BITS32_PER_BLOCK * BITS32_PER_BLOCK;

    bufferChosenGlobal_.SetGlobalBuffer((__gm__ uint32_t*)(windowInGM_ + dataSize_));
    bufferChosen_ = bufferChosenGlobal_(0);

    windowInGM_ = windowInGM_ + totalSize_ * bufferChosen_;
    windowOutGM_ = windowOutGM_ + totalSize_ * bufferChosen_;

    xGMTensor_.SetGlobalBuffer((__gm__ XType*)x);
    expertIdsGMTensor_.SetGlobalBuffer((__gm__ int32_t*)expertIds);
    expandXOutGMTensor_.SetGlobalBuffer((__gm__ ExpandXOutType*)(expandXOut), worldSize_ * axisBS_ * localMoeExpertNum_ * axisH_);
    dynamicScalesOutGMTensor_.SetGlobalBuffer((__gm__ float*)(dynamicScalesOut));
    windowInstatusTensor_.SetGlobalBuffer((__gm__ int32_t*)(windowInGM_));
    sendStatusTensor_.SetGlobalBuffer((__gm__ int32_t*)(windowOutGM_));

    xActiveMaskGMTensor_.SetGlobalBuffer((__gm__ int8_t*)xActiveMask);
    expandIdxOutGM_ = expandIdxOut;
    expertTokenNumsOutGM_ = expertTokenNumsOut;
    epRecvCountsGM_ = epRecvCountsOut;

    hCommuSize_ = axisH_ * sizeof(ExpandXOutType) + ((StaticQuant || DynamicQuant) ? UB_ALIGN: 0);
    axisHCommu_ = hCommuSize_ / sizeof(ExpandXOutType);

    batchWriteInfo_ = workspaceGM;
    batchWriteInfoTensor_.SetGlobalBuffer((__gm__ uint32_t*)(batchWriteInfo_),
                                        worldSize_ * U32_PER_ITEM);

    if constexpr (StaticQuant || DynamicQuant) {
        scalesGMTensor_.SetGlobalBuffer((__gm__ float*)scales);
    }

    needPerformanceInfo_ = performanceInfo != nullptr;
    if (unlikely(needPerformanceInfo_)) {
        performanceInfoSize_ = worldSize_;
        performanceInfoI32GMTensor_.SetGlobalBuffer((__gm__ int32_t*)performanceInfo);
    }

    uint64_t stateSizeMaxSize = 2 * STATE_SIZE; // 2: 实际上是(DATA_OFFSET+SKIP_OFFSET+sizeof(uint32)) + STATE_SIZE，近似计算使用2 * STATE_SIZE
    uint64_t winSizeMin = (axisBS_ * worldSize_ * (localMoeExpertNum_ > axisK_ ? axisK_ : localMoeExpertNum_) *
        axisH_ * sizeof(uint16_t) + stateSizeMaxSize) * BUFFER_NUM; // 考虑负载极其不均衡时，HCCL BUFFSIZE需要开的大小

    worldTaskInfo_.SplitCore(worldSize_, aivNum_, aivId_);
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::AllocTensor() {
    uint32_t xLength = axisH_ * sizeof(XType) + axisHCommu_ * sizeof(ExpandXOutType);
    uint32_t xPongAddr = axisH_ * sizeof(XType);
    xTensor_[PING_IDX] = LocalTensor<XType>{TPosition::LCM, 0, axisH_};
    xTensor_[PONG_IDX] = LocalTensor<XType>{TPosition::LCM, xPongAddr, axisH_};
    xOutTensor_[PING_IDX] = LocalTensor<ExpandXOutType>{TPosition::LCM, 0, axisHCommu_};
    xOutTensor_[PONG_IDX] = LocalTensor<ExpandXOutType>{TPosition::LCM, xPongAddr, axisHCommu_};
    // statusEntry contains the number of tokens received by each expert
    // and a flag indicating whether the recv buffer has received data.
    statusEntryCount_ = RoundUp(localMoeExpertNum_ + 1, BITS32_PER_BLOCK);
    uint32_t statusWorldSize = statusEntryCount_ * worldSize_;
    statusTensor_ = LocalTensor<int32_t>{TPosition::LCM, 0, statusWorldSize};

    uint32_t batchWriteU32Size = U32_PER_ITEM * worldTaskInfo_.taskNum;
    uint32_t batchWriteU64Size = U64_PER_ITEM * worldTaskInfo_.taskNum;
    batchWriteU32Tensor_ = LocalTensor<uint32_t>{TPosition::LCM, 0, batchWriteU32Size};
    batchWriteU64Tensor_ = LocalTensor<uint64_t>{TPosition::LCM, 0, batchWriteU64Size};

    uint32_t statusWorldLength = statusWorldSize * sizeof(int32_t);
    uint32_t batchWriteLength = batchWriteU32Size * sizeof(uint32_t);
    uint32_t expertCumsumAddr = Std::max(xLength, Std::max(statusWorldLength, batchWriteLength));
    uint32_t expertCumsumSize = RoundUp(moeExpertNum_ + 1, BITS32_PER_BLOCK);
    expertCumsumTensor_ = LocalTensor<uint32_t>{TPosition::LCM, expertCumsumAddr, expertCumsumSize};

    uint32_t expertIdsAddr = expertCumsumAddr + expertCumsumSize * sizeof(uint32_t);
    uint32_t expertIdsSize = RoundUp(expertIdsCnt_, BITS32_PER_BLOCK);
    uint32_t expertIdsLength = expertIdsSize * sizeof(int32_t);
    expertIdsTensor_ = LocalTensor<int32_t>{TPosition::LCM, expertIdsAddr, expertIdsSize};

    uint32_t expertCountAddr = expertIdsAddr + expertIdsLength;
    expertCountTensor_ = LocalTensor<int32_t>{TPosition::LCM, expertCountAddr, expertIdsSize};

    uint32_t xfloatOrFlagAddr = expertCountAddr + expertIdsLength;
    flagTensor_ = LocalTensor<int32_t>{TPosition::LCM, xfloatOrFlagAddr, BITS32_PER_BLOCK};
    xFloatTensor_ = LocalTensor<float>{TPosition::LCM, xfloatOrFlagAddr, axisH_};

    uint32_t smoothScalesAddr = xfloatOrFlagAddr + axisH_ * sizeof(float);
    smoothScalesTensor_ = LocalTensor<float>{TPosition::LCM, smoothScalesAddr, axisH_};

    if (unlikely(needPerformanceInfo_)) {
        uint32_t performanceInfoI32Addr = smoothScalesAddr + axisH_;
        performanceInfoI32Tensor_ = LocalTensor<int32_t>{TPosition::LCM, performanceInfoI32Addr, performanceInfoSize_ * sizeof(int64_t) / sizeof(int32_t)};
    }

    uint32_t validExpIndexAddr = AscendC::TOTAL_UB_SIZE - expertIdsLength;
    validExpIndexTensor_ = LocalTensor<int32_t>{TPosition::LCM, validExpIndexAddr, expertIdsSize};

    uint32_t xActiveMaskAlignSize = RoundUp(isExpertMaskFlag_ ? expertIdsCnt_ : axisBS_, 128);
    xActiveMaskTensor_ = LocalTensor<int8_t>{TPosition::LCM, expertCountAddr, xActiveMaskAlignSize};
    uint32_t xActiveMaskHalfAddr = expertCountAddr + xActiveMaskAlignSize;
    xActiveMaskHalfTensor_ = LocalTensor<half>{TPosition::LCM, xActiveMaskHalfAddr, xActiveMaskAlignSize};

    uint32_t zeroExpertMaskSize = RoundUp(expertIdsCnt_, BITS32_PER_REPEAT) / (CHAR_BIT * sizeof(uint16_t));
    zeroExpertMaskTensor_ = LocalTensor<uint16_t>{TPosition::LCM, xActiveMaskHalfAddr, zeroExpertMaskSize};
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::CalValidTokenCount()
{
    activeMaskBsCnt_ = axisBS_;
    sendToMoeExpTokenCnt_ = axisBS_ * axisK_;
    if (isTokenMaskFlag_) {
        TokenActiveMaskCal();
    } else if (isExpertMaskFlag_) {
        ExpertActiveMaskCal();
    }
    CreateVecIndex(validExpIndexTensor_, 0, RoundUp(expertIdsCnt_, BITS32_PER_BLOCK));
    if (sendToMoeExpTokenCnt_ > 0 && zeroComputeExpertNum_ > 0) {
        ZeroComputeExpertMaskCal();
    }
    PipeBarrier<PIPE_V>();
    if (sendToMoeExpTokenCnt_ == 0 || (!isExpertMaskFlag_ && zeroComputeExpertNum_ <= 0)) {
        return;
    }
    if (isExpertMaskFlag_) {
        if (zeroComputeExpertNum_ > 0) {
            auto &&xActiveMaskU16Tensor_ = xActiveMaskTensor_.ReinterpretCast<uint16_t>();
            And(xActiveMaskU16Tensor_, xActiveMaskU16Tensor_, zeroExpertMaskTensor_,
                Ceil(sendToMoeExpTokenCnt_, BITS16_PER_BLOCK));
            PipeBarrier<PIPE_V>();
        }
        GatherMask(validExpIndexTensor_, validExpIndexTensor_, xActiveMaskTensor_.ReinterpretCast<uint32_t>(), true,
                   sendToMoeExpTokenCnt_, {1, 1, 0, 0}, sendToMoeExpTokenCnt_);
    } else { // !isExpertMaskFlag_ && zeroComputeExpertNum_ > 0
        GatherMask(validExpIndexTensor_, validExpIndexTensor_, zeroExpertMaskTensor_.ReinterpretCast<uint32_t>(), true,
                   sendToMoeExpTokenCnt_, {1, 1, 0, 0}, sendToMoeExpTokenCnt_);
    }
    PipeBarrier<PIPE_V>();
    SyncFunc<AscendC::HardEvent::V_MTE2>();
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::TokenActiveMaskCal()
{
    auto&& sharedTmpBuffer = xActiveMaskTensor_.ReinterpretCast<half>();
    auto xActiveMaskParams = DataCopyExtParams{1U, axisBS_, 0U, 0U, 0U};
    auto xActiveMaskCopyPadParams = DataCopyPadExtParams<int8_t>{false, 0U, 0U, 0U};
    DataCopyPad(xActiveMaskTensor_, xActiveMaskGMTensor_, xActiveMaskParams, xActiveMaskCopyPadParams);
    SyncFunc<AscendC::HardEvent::MTE2_V>();
    Cast(xActiveMaskHalfTensor_, xActiveMaskTensor_, RoundMode::CAST_NONE, axisBS_);
    PipeBarrier<PIPE_V>();
    ReduceSum(sharedTmpBuffer, xActiveMaskHalfTensor_, sharedTmpBuffer, axisBS_);
    activeMaskBsCnt_ = static_cast<int32_t>(AscendC::GetAccVal<half>());
    sendToMoeExpTokenCnt_ = activeMaskBsCnt_ * axisK_;
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::ExpertActiveMaskCal()
{
    auto maskCopyPadParams = DataCopyPadExtParams<int8_t>{false, 0U, 0U, 0U};
    auto maskParams = DataCopyExtParams{1U, expertIdsCnt_, 0U, 0U, 0U};
    DataCopyPad(xActiveMaskTensor_, xActiveMaskGMTensor_, maskParams, maskCopyPadParams);
    SyncFunc<AscendC::HardEvent::MTE2_V>();
    Cast(xActiveMaskHalfTensor_, xActiveMaskTensor_, RoundMode::CAST_NONE, expertIdsCnt_);
    PipeBarrier<PIPE_V>();
    CompareScalar(xActiveMaskTensor_, xActiveMaskHalfTensor_, static_cast<half>(1), AscendC::CMPMODE::EQ,
                  RoundUp(expertIdsCnt_, BITS16_PER_REPEAT));
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::ZeroComputeExpertMaskCal()
{
    auto expertIdsCntParams = DataCopyExtParams{1, sendToMoeExpTokenCnt_ * sizeof(int32_t), 0U, 0U, 0U};
    auto expertIdsCntCopyPadParams = DataCopyPadExtParams<int32_t>{false, 0U, 0U, 0U};
    DataCopyPad(expertIdsTensor_, expertIdsGMTensor_, expertIdsCntParams, expertIdsCntCopyPadParams);
    SyncFunc<AscendC::HardEvent::MTE2_V>();
    Mins(expertIdsTensor_, expertIdsTensor_, static_cast<int32_t>(moeExpertNum_), sendToMoeExpTokenCnt_);
    PipeBarrier<PIPE_V>();
    CompareScalar(zeroExpertMaskTensor_.template ReinterpretCast<uint8_t>(), expertIdsTensor_,
                  static_cast<int32_t>(moeExpertNum_), AscendC::CMPMODE::EQ,
                  RoundUp(sendToMoeExpTokenCnt_, BITS32_PER_REPEAT));
    PipeBarrier<PIPE_V>();
    Not(zeroExpertMaskTensor_, zeroExpertMaskTensor_, Ceil(sendToMoeExpTokenCnt_, BITS16_PER_BLOCK));
}


template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::QuantProcess(uint32_t expertIndex, TEventID eventId) {
    SyncFunc<AscendC::HardEvent::MTE3_V>();
    WaitFlag<HardEvent::MTE2_V>(eventId);
    Cast(xFloatTensor_, xTensor_[PING_IDX], RoundMode::CAST_NONE, axisH_);
    SetFlag<HardEvent::V_MTE2>(eventId);
    PipeBarrier<PIPE_V>();
    if constexpr (IsSmoothScaleExist) {
        DataCopy(smoothScalesTensor_, scalesGMTensor_[expertIndex * axisH_], axisH_);
        SyncFunc<AscendC::HardEvent::MTE2_V>();
        Mul(xFloatTensor_, xFloatTensor_, smoothScalesTensor_, axisH_);
        PipeBarrier<PIPE_V>();
    }
    LocalTensor<float>& absTensor = smoothScalesTensor_;
    LocalTensor<float>& maxTensor = smoothScalesTensor_;
    Abs(absTensor, xFloatTensor_, axisH_);
    PipeBarrier<PIPE_V>();
    ReduceMax(maxTensor, absTensor, absTensor, axisH_, false);
    SyncFunc<AscendC::HardEvent::V_S>();
    float dynamicScale = float(127.0) / maxTensor.GetValue(0);
    SyncFunc<AscendC::HardEvent::S_V>();

    Muls(xFloatTensor_, xFloatTensor_, dynamicScale, axisH_);
    PipeBarrier<PIPE_V>();
    auto &&xHalfTensor = xFloatTensor_.ReinterpretCast<half>();
    auto &&xIntTensor = xFloatTensor_.ReinterpretCast<int32_t>();
    Cast(xIntTensor, xFloatTensor_, RoundMode::CAST_RINT, axisH_);
    SetDeqScale((half)1.0f);
    PipeBarrier<PIPE_V>();
    Cast(xHalfTensor, xIntTensor, RoundMode::CAST_ROUND, axisH_);
    PipeBarrier<PIPE_V>();
    Cast(xOutTensor_[1], xHalfTensor, RoundMode::CAST_TRUNC, axisH_);
    SetFlag<HardEvent::V_MTE3>(eventId);
    auto &&xOutFloatTensor = xOutTensor_[PONG_IDX].template ReinterpretCast<float>();
    dynamicScale = 1 / dynamicScale;
    xOutFloatTensor.SetValue(axisH_ / sizeof(float), dynamicScale);
    SetFlag<HardEvent::S_MTE3>(eventId);
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::InitStatusTensor()
{
    Duplicate<int32_t>(statusTensor_, 0, worldSize_ * statusEntryCount_);
    uint32_t statusEntryBlockNum = statusEntryCount_ / BITS32_PER_BLOCK;
    uint32_t statusEntryNumPerRepeat = AscendC::DEFAULT_REPEAT_STRIDE / statusEntryBlockNum;
    constexpr uint64_t maskTemplate[] = {
        0x0ULL,                 // placeholder
        0x8080808080808080ULL,  // Flag last element of each block when StatusEntry size is 1 blocks
        0x8000800080008000ULL,  // Flag last element of every 2 blocks when StatusEntry size is 2 blocks
        0x800000800000ULL,      // Flag last element of blocks 3 & 6 when StatusEntry size is 3 blocks
        0x8000000080000000ULL,  // Flag last element of blocks 4 & 8 when StatusEntry size is 4 blocks
        0x8000000000ULL,        // Flag last element of 5th block when StatusEntry size is 5 blocks
        0x800000000000ULL,      // Flag last element of 6th block when StatusEntry size is 6 blocks
        0x80000000000000ULL,    // Flag last element of 7th block when StatusEntry size is 7 blocks
        0x8000000000000000ULL}; // Flag last element of 8th block when StatusEntry size is 8 blocks
    uint32_t repeatTime, dstRepeatStride, statusEntryOffset;
    uint64_t mask[2] = {0};
    if (statusEntryNumPerRepeat > 0) {
        repeatTime = Ceil(worldSize_, statusEntryNumPerRepeat);
        dstRepeatStride = statusEntryNumPerRepeat * statusEntryBlockNum;
        statusEntryOffset = 0;
        mask[0] = maskTemplate[statusEntryBlockNum];
    } else {
        repeatTime = worldSize_;
        dstRepeatStride = statusEntryBlockNum;
        statusEntryOffset = (statusEntryBlockNum - AscendC::DEFAULT_REPEAT_STRIDE) * BITS32_PER_BLOCK;
        mask[0] = maskTemplate[AscendC::DEFAULT_REPEAT_STRIDE];
    }
    PipeBarrier<PIPE_V>();
    Duplicate<int32_t>(statusTensor_[statusEntryOffset], FLAG_VALUE, mask, repeatTime, 1, dstRepeatStride);
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::IndexSort()
{
    uint32_t activeExpertIds = activeMaskBsCnt_ * axisK_;
    auto copyExpertIdsParams = DataCopyExtParams{1, static_cast<uint32_t>(activeExpertIds * sizeof(int32_t)), 0, 0, 0};
    auto padParams = DataCopyPadExtParams<int32_t>{false, 0, 0, 0};
    DataCopyPad(expertIdsTensor_, expertIdsGMTensor_, copyExpertIdsParams, padParams);
    Duplicate(expertCountTensor_, 0, RoundUp(activeExpertIds, BITS32_PER_BLOCK));
    InitStatusTensor();
    SyncFunc<AscendC::HardEvent::MTE2_S>();
    SyncFunc<AscendC::HardEvent::V_S>();
    // 统计给每个专家发的token数
    for (uint32_t tokenIndex = 0; tokenIndex < sendToMoeExpTokenCnt_; ++tokenIndex) {
        int32_t expertIdx = validExpIndexTensor_(tokenIndex);
        int32_t expertId = expertIdsTensor_(expertIdx);
        int32_t rankId = expertId / localMoeExpertNum_;
        int32_t expertOffsetInRank = expertId % localMoeExpertNum_;
        expertCountTensor_(expertIdx) = statusTensor_(rankId * statusEntryCount_ + expertOffsetInRank);
        statusTensor_(rankId * statusEntryCount_ + expertOffsetInRank)++;
    }
    expertCumsumTensor_(0) = 0;
    for (uint32_t expertId = 1; expertId < moeExpertNum_; expertId++) {
        int32_t rankId = (expertId - 1) / localMoeExpertNum_;
        int32_t expertOffsetInRank = (expertId - 1) % localMoeExpertNum_;
        uint32_t count = statusTensor_(rankId * statusEntryCount_ + expertOffsetInRank);
        uint32_t preSum = expertCumsumTensor_(expertId - 1);
        expertCumsumTensor_(expertId) = count + preSum;
    }
    expertCumsumTensor_(moeExpertNum_) = sendToMoeExpTokenCnt_;

    if (aivId_ == aivNum_ - 1) {
        SyncFunc<AscendC::HardEvent::S_MTE3>();
        GlobalTensor<int32_t> expandIdxGMTensor;
        expandIdxGMTensor.SetGlobalBuffer((__gm__ int32_t *)expandIdxOutGM_);
        DataCopyPad(expandIdxGMTensor, expertCountTensor_, copyExpertIdsParams);
        DataCopy(windowInstatusTensor_[rankId_ * dataSizePerRank_ / sizeof(int32_t)],
                 statusTensor_[rankId_ * statusEntryCount_], statusEntryCount_);
        Duplicate<int32_t>(flagTensor_, FLAG_VALUE, BITS32_PER_BLOCK);
        SyncFunc<AscendC::HardEvent::V_MTE3>();
        for (uint32_t rankId = 0; rankId < worldSize_; rankId++) {
            uint64_t rankOffset = rankId * dataSizePerRank_ / sizeof(int32_t);
            DataCopy(sendStatusTensor_[rankOffset], statusTensor_[rankId * statusEntryCount_], statusEntryCount_);
            uint32_t startExpertId = rankId * localMoeExpertNum_;
            uint32_t tokenCount = expertCumsumTensor_(startExpertId + localMoeExpertNum_) - expertCumsumTensor_(startExpertId);
            uint64_t dataFlagOffset =rankOffset + (DATA_OFFSET + tokenCount * hCommuSize_ + SKIP_OFFSET) / sizeof(int32_t);
            DataCopyExtParams copyFlagParams{1, static_cast<uint32_t>(sizeof(int32_t)), 0, 0, 0};
            DataCopyPad(sendStatusTensor_[dataFlagOffset], flagTensor_, copyFlagParams);
        }
        SyncFunc<AscendC::HardEvent::MTE3_MTE2>();
    }
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::ReorderTokens()
{
    TaskInfo tokenTaskInfo;
    tokenTaskInfo.SplitCore(sendToMoeExpTokenCnt_, aivNum_, aivId_);
    GlobalTensor<ExpandXOutType> sendTokensGlobal;
    constexpr auto event = (StaticQuant || DynamicQuant) ? HardEvent::V_MTE2 : HardEvent::MTE3_MTE2;
    SetFlag<event>(EVENT_ID0);
    SetFlag<event>(EVENT_ID1);
    for (uint32_t tokenIndex = tokenTaskInfo.startTaskId; tokenIndex < tokenTaskInfo.endTaskId; ++tokenIndex) {
        int32_t eventId = (tokenIndex & 1) ? EVENT_ID0 : EVENT_ID1;
        int32_t expertIdx = validExpIndexTensor_(tokenIndex);
        int32_t expertId = expertIdsTensor_(expertIdx);
        int32_t rankId = expertId / localMoeExpertNum_;
        int32_t startExpertId = rankId * localMoeExpertNum_;
        uint32_t expertOffset = expertCumsumTensor_(expertId) - expertCumsumTensor_(startExpertId);
        int32_t tokenOffset = expertCountTensor_(expertIdx);
        sendTokensGlobal.SetGlobalBuffer((__gm__ ExpandXOutType*)(windowOutGM_ + rankId * dataSizePerRank_ + DATA_OFFSET));
        if constexpr (StaticQuant || DynamicQuant) {
            WaitFlag<HardEvent::V_MTE2>(eventId);
            DataCopy(xTensor_[PING_IDX], xGMTensor_[expertIdx / axisK_ * axisH_], axisH_);
            SetFlag<HardEvent::MTE2_V>(eventId);
            QuantProcess(expertId, eventId);
            WaitFlag<HardEvent::V_MTE3>(eventId);
            WaitFlag<HardEvent::S_MTE3>(eventId);
            DataCopy(sendTokensGlobal[(expertOffset + tokenOffset) * axisHCommu_], xOutTensor_[PONG_IDX], axisHCommu_);
        } else {
            WaitFlag<HardEvent::MTE3_MTE2>(eventId);
            DataCopy(xTensor_[eventId], xGMTensor_[expertIdx / axisK_ * axisH_], axisH_); // 约束对齐 expertIdx / axisK_ * axisH_
            SetFlag<HardEvent::MTE2_MTE3>(eventId);
            WaitFlag<HardEvent::MTE2_MTE3>(eventId);
            DataCopy(sendTokensGlobal[(expertOffset + tokenOffset) * axisHCommu_], xTensor_[eventId], axisHCommu_);
            SetFlag<HardEvent::MTE3_MTE2>(eventId);
        }
    }
    WaitFlag<event>(EVENT_ID0);
    WaitFlag<event>(EVENT_ID1);
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::ConstructBatchWriteInfo()
{
    uint32_t batchWriteDataType = static_cast<uint32_t>(AscendC::HcclDataType::HCCL_DATA_TYPE_INT8);
    SyncFunc<AscendC::HardEvent::MTE3_S>();
    for (uint32_t rankIndex = worldTaskInfo_.startTaskId; rankIndex < worldTaskInfo_.endTaskId; ++rankIndex) {
        uint32_t startExpertId = rankIndex * localMoeExpertNum_;
        uint32_t currentIndex = rankIndex - worldTaskInfo_.startTaskId;
        uint32_t tokenCount = expertCumsumTensor_(startExpertId + localMoeExpertNum_) - expertCumsumTensor_(startExpertId);
        GM_ADDR rankGM = (__gm__ uint8_t*)(hccl_.GetWindowsInAddr(rankIndex) + totalSize_ * bufferChosen_ + (dataSizePerRank_ * rankId_));
        GM_ADDR localBuf = (__gm__ uint8_t*)(windowOutGM_ + dataSizePerRank_ * rankIndex);
        uint64_t batchWriteDataSize = DATA_OFFSET + tokenCount * hCommuSize_ + sizeof(int32_t) + SKIP_OFFSET;
        batchWriteU64Tensor_(currentIndex * U64_PER_ITEM) = (uint64_t)localBuf;
        batchWriteU64Tensor_(currentIndex * U64_PER_ITEM + 1) = (uint64_t)rankGM;
        batchWriteU64Tensor_(currentIndex * U64_PER_ITEM + 2) = batchWriteDataSize;
        batchWriteU32Tensor_(currentIndex * U32_PER_ITEM + 6) = batchWriteDataType;
        batchWriteU32Tensor_(currentIndex * U32_PER_ITEM + 7) = rankIndex;
    }

    SyncFunc<AscendC::HardEvent::S_MTE3>();
    DataCopy(batchWriteInfoTensor_[worldTaskInfo_.startTaskId * U32_PER_ITEM], batchWriteU32Tensor_,
        worldTaskInfo_.taskNum * U32_PER_ITEM);
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::SendToMoeExpert()
{
    if (aivId_ == 0) {
        HcclHandle batchWriteResult = hccl_.BatchWrite<true>(batchWriteInfo_, worldSize_);
        bufferChosenGlobal_(0) = bufferChosen_ ^ 1;
    }
    if (aivId_ == aivNum_ - 1) {
        uint32_t startExpertId = rankId_ * localMoeExpertNum_;
        uint32_t tokenCount = expertCumsumTensor_(startExpertId + localMoeExpertNum_) - expertCumsumTensor_(startExpertId);
        GlobalTensor<ExpandXOutType> currRankWindowInGlobal;
        GlobalTensor<ExpandXOutType> currRankWindowOutGlobal;
        currRankWindowInGlobal.SetGlobalBuffer((__gm__ ExpandXOutType*)(windowInGM_ + rankId_ * dataSizePerRank_ + DATA_OFFSET));
        currRankWindowOutGlobal.SetGlobalBuffer((__gm__ ExpandXOutType*)(windowOutGM_ + rankId_ * dataSizePerRank_ + DATA_OFFSET));
        SyncFunc<AscendC::HardEvent::S_MTE2>();
        SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
        SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
        for (uint32_t currTokenIdx = 0; currTokenIdx < tokenCount; currTokenIdx++) {
            int32_t eventId = (currTokenIdx & 1) ? EVENT_ID0 : EVENT_ID1;
            WaitFlag<HardEvent::MTE3_MTE2>(eventId);
            DataCopy(xOutTensor_[eventId], currRankWindowOutGlobal[currTokenIdx * axisHCommu_], axisHCommu_);
            SetFlag<HardEvent::MTE2_MTE3>(eventId);
            WaitFlag<HardEvent::MTE2_MTE3>(eventId);
            DataCopy(currRankWindowInGlobal[currTokenIdx * axisHCommu_], xOutTensor_[eventId], axisHCommu_);
            SetFlag<HardEvent::MTE3_MTE2>(eventId);
        }
        WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
        WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
        uint64_t dataFlagOffset = (rankId_ * dataSizePerRank_ + DATA_OFFSET + tokenCount * hCommuSize_ + SKIP_OFFSET) / sizeof(int32_t);
        SyncFunc<AscendC::HardEvent::MTE3_S>();
        windowInstatusTensor_(dataFlagOffset) = FLAG_VALUE;
        DataCacheCleanAndInvalid<int32_t, AscendC::CacheLine::SINGLE_CACHE_LINE, AscendC::DcciDst::CACHELINE_OUT>(windowInstatusTensor_[dataFlagOffset]);
    }
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::WaitDispatch()
{
    if (unlikely(needPerformanceInfo_)) {
        // 避免没有被分配任务的核未初始化performanceInfoI32Tensor_
        Duplicate<int32_t>(performanceInfoI32Tensor_, 0, performanceInfoSize_ * sizeof(int64_t) / sizeof(int32_t));
        SyncFunc<AscendC::HardEvent::V_S>();
    }

    if (worldTaskInfo_.taskNum == 0) {
        SyncAll<true>();
        return;
    }

    DataCopyExtParams copyFlagParams{1, static_cast<uint32_t>(sizeof(int32_t)), 0, 0, 0};
    DataCopyPadExtParams<int32_t> padParams{false, 0, 0, 0};
    auto dataFlagLocal =
        LocalTensor<int32_t>{TPosition::LCM, statusTensor_.GetSize() * sizeof(int32_t), BITS32_PER_BLOCK};
    SyncFunc<AscendC::HardEvent::S_MTE2>();

    uint32_t recvFlagNum = 0;
    int64_t startTime = GetCurrentTimestampUs();
    while (recvFlagNum < worldTaskInfo_.taskNum) {
        for (uint32_t rankId = worldTaskInfo_.startTaskId; rankId < worldTaskInfo_.endTaskId; rankId++) {
            DataCopy(statusTensor_[rankId * statusEntryCount_], windowInstatusTensor_[rankId * dataSizePerRank_ / sizeof(int32_t)], statusEntryCount_);
            SyncFunc<AscendC::HardEvent::MTE2_S>();
            int32_t statusFlag = statusTensor_(rankId * statusEntryCount_ + statusEntryCount_ - 1);
            if (statusFlag != FLAG_VALUE) {
                continue;
            }
            uint32_t tokenCount = 0;
            for (int32_t expertOffset = 0; expertOffset < localMoeExpertNum_; expertOffset++) {
                tokenCount += statusTensor_(rankId * statusEntryCount_ + expertOffset);
            }
            uint64_t dataFlagOffset = (rankId * dataSizePerRank_ + DATA_OFFSET + tokenCount * hCommuSize_ + SKIP_OFFSET) / sizeof(int32_t);
            DataCopyPad(dataFlagLocal, windowInstatusTensor_[dataFlagOffset], copyFlagParams, padParams);
            SyncFunc<AscendC::HardEvent::MTE2_S>();
            int32_t dataFlag = dataFlagLocal(0);
            if (dataFlag != FLAG_VALUE) {
                continue;
            }
            recvFlagNum++;
            windowInstatusTensor_(dataFlagOffset) = 0;
            // 重要：要下DCCI保证清零写进去，避免下一次判断时又判断生效，重复累计recvFlagNum
            DataCacheCleanAndInvalid<int32_t, AscendC::CacheLine::SINGLE_CACHE_LINE, AscendC::DcciDst::CACHELINE_OUT>(windowInstatusTensor_[dataFlagOffset]);
            if (unlikely(needPerformanceInfo_)) {
                auto srcRankId = rankId;
                RecordRankCommDuration(performanceInfoI32Tensor_, srcRankId, startTime);
            }
        }
    }
    SyncAll<true>();
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::GetStatusCumSum()
{
    const uint32_t statusSize = statusEntryCount_ * sizeof(int32_t);
    uint32_t srcStrideU32 = dataSizePerRank_ - statusSize;
    DataCopyExtParams copyStatusParams{static_cast<uint16_t>(worldSize_), statusSize, srcStrideU32, 0, 0};
    DataCopyPadExtParams<int32_t> padParams{false, 0, 0, 0};
    DataCopyPad(statusTensor_, windowInstatusTensor_, copyStatusParams, padParams);
    SyncFunc<AscendC::HardEvent::MTE2_V>();
    uint32_t epRecvCountsTempAddr = statusTensor_.GetSize() * sizeof(int32_t);
    uint32_t epRecvCountsTempSize = localMoeExpertNumAlign_ * worldSize_;
    epRecvCountsTempLocal_ = LocalTensor<int32_t>{TPosition::LCM, epRecvCountsTempAddr, epRecvCountsTempSize};
    uint32_t epRecvCountsOutAddr = epRecvCountsTempAddr + epRecvCountsTempSize * sizeof(int32_t);
    uint32_t epRecvCountsOutSize = RoundUp(moeExpertNum_, BITS32_PER_BLOCK);
    epRecvCountsOutLocal_ = LocalTensor<int32_t>{TPosition::LCM, epRecvCountsOutAddr, epRecvCountsOutSize};
    uint16_t srcStrideU16 = (statusEntryCount_ - localMoeExpertNumAlign_) / BITS32_PER_BLOCK;
    uint16_t worldSizeU16 = (uint16_t)worldSize_;
    DataCopyParams copyParamsMultiple{worldSizeU16, static_cast<uint16_t>(localMoeExpertNumAlign_ / BITS32_PER_BLOCK), srcStrideU16, 0};
    DataCopy(epRecvCountsTempLocal_, statusTensor_, copyParamsMultiple);
    uint64_t mask4Adds = localMoeExpertNum_;
    PipeBarrier<PIPE_V>();
    for (uint32_t rankIndex = 1; rankIndex < worldSize_; ++rankIndex) {
        uint32_t statusOffset = rankIndex * localMoeExpertNumAlign_;
        Add(epRecvCountsTempLocal_[statusOffset], epRecvCountsTempLocal_[statusOffset - localMoeExpertNumAlign_],
            epRecvCountsTempLocal_[statusOffset], mask4Adds, 1, {1, 1, 1, 8, 8, 8});
        PipeBarrier<PIPE_V>();
    }
    uint32_t patternAddr = epRecvCountsOutAddr + RoundUp(moeExpertNum_ * sizeof(int32_t), UB_ALIGN);
    auto patternLocal = LocalTensor<uint32_t>{TPosition::LCM, patternAddr, localMoeExpertNumAlign_};
    Duplicate<uint32_t>(patternLocal, 0, localMoeExpertNumAlign_);
    SyncFunc<AscendC::HardEvent::V_S>();
    patternLocal(0) = 1;
    srcStrideU16 = localMoeExpertNumAlign_ * sizeof(int32_t) / UB_ALIGN;
    int32_t previousSum = 0;
    uint64_t rsvdCnt = 0;
    mask4Adds = worldSize_;
    uint32_t mask4Gather = localMoeExpertNumAlign_;
    for (uint32_t expertIndex = 0; expertIndex < localMoeExpertNum_; expertIndex++) {
        SyncFunc<AscendC::HardEvent::S_V>();
        GatherMask(epRecvCountsOutLocal_[expertIndex * worldSize_], epRecvCountsTempLocal_, patternLocal, true, mask4Gather,
            {1, worldSizeU16, srcStrideU16, 0}, rsvdCnt);
        PipeBarrier<PIPE_V>();
        Adds(epRecvCountsOutLocal_[expertIndex * worldSize_], epRecvCountsOutLocal_[expertIndex * worldSize_], previousSum,
            worldSize_);
        SyncFunc<AscendC::HardEvent::V_S>();
        previousSum = epRecvCountsOutLocal_(expertIndex * worldSize_ + worldSize_ - 1);
        patternLocal(0) = patternLocal(0) << 1;
    }
    if (aivId_ == aivNum_ - 1) {
        uint32_t expertTokenNumsW64Addr = patternAddr + localMoeExpertNumAlign_ * sizeof(uint32_t);
        auto expertTokenNumsW64Local = LocalTensor<int32_t>{TPosition::LCM, expertTokenNumsW64Addr,
                                                            localMoeExpertNum_ * (sizeof(int64_t) / sizeof(int32_t))};
        if (expertTokenNumsType_ == 0) {
            mask4Gather = worldSize_;
            patternLocal(0) = 0;
            patternLocal((worldSize_ - 1) / 32) = 1 << ((worldSize_ - 1) % 32);
            srcStrideU16 = worldSize_ * sizeof(int32_t) / UB_ALIGN;
            SyncFunc<AscendC::HardEvent::S_V>();
            GatherMask(epRecvCountsTempLocal_, epRecvCountsOutLocal_, patternLocal, true, mask4Gather,
                {1, static_cast<uint16_t>(localMoeExpertNum_), srcStrideU16, 0}, rsvdCnt);
            SyncFunc<AscendC::HardEvent::V_S>();
            for (int i = 0; i < localMoeExpertNum_; i++) {
                expertTokenNumsW64Local(i * 2) = epRecvCountsTempLocal_(i);
                expertTokenNumsW64Local(i * 2 + 1) = 0;
            }
        } else {
            uint32_t tokenCountOffset = (worldSize_ - 1) * localMoeExpertNumAlign_;
            for (int i = 0; i < localMoeExpertNum_; i++) {
                expertTokenNumsW64Local(i * 2) = epRecvCountsTempLocal_(tokenCountOffset + i);
                expertTokenNumsW64Local(i * 2 + 1) = 0;
            }
        }
        SyncFunc<AscendC::HardEvent::S_MTE3>();
        GlobalTensor<int32_t> expertTokenNumsGlobal;
        expertTokenNumsGlobal.SetGlobalBuffer((__gm__ int32_t*)(expertTokenNumsOutGM_));
        DataCopyExtParams copyPadParams{1, static_cast<uint32_t>(localMoeExpertNum_ * sizeof(int64_t)), 0, 0, 0};
        DataCopyPad(expertTokenNumsGlobal, expertTokenNumsW64Local, copyPadParams);

        GlobalTensor<int32_t> epRecvCountsGlobal;
        epRecvCountsGlobal.SetGlobalBuffer((__gm__ int32_t*)(epRecvCountsGM_));
        SyncFunc<AscendC::HardEvent::V_MTE3>();
        DataCopy(epRecvCountsGlobal, epRecvCountsOutLocal_, moeExpertNum_);
    }
}


template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::LocalWindowCopy()
{
    GlobalTensor<ExpandXOutType> currRankWindowGlobal;
    uint32_t xOutTensor0Addr = epRecvCountsOutLocal_.GetPhyAddr() + epRecvCountsOutLocal_.GetSize() * sizeof(int32_t);
    uint32_t xOutTensor1Addr = xOutTensor0Addr + axisHCommu_ * sizeof(ExpandXOutType);
    uint32_t dynamicScalesAddr = xOutTensor1Addr + axisHCommu_ * sizeof(ExpandXOutType);
    uint32_t tokenNum = epRecvCountsOutLocal_(localMoeExpertNum_ * worldSize_ - 1);
    auto dynamicScalesTensor = LocalTensor<float>{TPosition::LCM, dynamicScalesAddr, tokenNum};
    xOutTensor_[0] = LocalTensor<ExpandXOutType>{TPosition::LCM, xOutTensor0Addr, axisHCommu_};
    xOutTensor_[1] = LocalTensor<ExpandXOutType>{TPosition::LCM, xOutTensor1Addr, axisHCommu_};
    for (uint32_t rankId = worldTaskInfo_.startTaskId; rankId < worldTaskInfo_.endTaskId; rankId++) {
        GM_ADDR wAddr = (__gm__ uint8_t*)(windowInGM_) + rankId * dataSizePerRank_ + DATA_OFFSET;
        currRankWindowGlobal.SetGlobalBuffer((__gm__ ExpandXOutType*)(wAddr));
        uint32_t currRankDataOffset = 0;
        uint32_t currRankStatusOffset = rankId * statusEntryCount_;

        for (uint32_t j = 0; j < localMoeExpertNum_; j++) {
            // 将数据从Window拷贝到UB
            uint32_t currTokensCount = statusTensor_(currRankStatusOffset + j);
            uint32_t currTokensOffset = epRecvCountsOutLocal_(j * worldSize_ + rankId) - currTokensCount;
            uint32_t dynamicScalesLocalIdx = 0;
            SyncFunc<AscendC::HardEvent::S_MTE2>();
            SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
            SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
            for (uint32_t k = 0; k < currTokensCount; k++) {
                int32_t eventId = (k & 1) ? EVENT_ID0 : EVENT_ID1;
                WaitFlag<HardEvent::MTE3_MTE2>(eventId);
                DataCopy(xOutTensor_[eventId], currRankWindowGlobal[(currRankDataOffset + k) * axisHCommu_], axisHCommu_);
                SetFlag<HardEvent::MTE2_MTE3>(eventId);
                if constexpr (DynamicQuant) {
                    PipeBarrier<PIPE_ALL>();
                    auto &&dynamicScalesLocal = xOutTensor_[eventId].template ReinterpretCast<float>();
                    dynamicScalesTensor.SetValue(dynamicScalesLocalIdx++, dynamicScalesLocal(axisH_ / sizeof(float)));
                    PipeBarrier<PIPE_ALL>();
                }
                WaitFlag<HardEvent::MTE2_MTE3>(eventId);
                DataCopy(expandXOutGMTensor_[(currTokensOffset + k) * axisH_], xOutTensor_[eventId], axisH_);
                SetFlag<HardEvent::MTE3_MTE2>(eventId);
            }
            WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
            WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
            currRankDataOffset += currTokensCount;
            PipeBarrier<PIPE_ALL>();
            if constexpr (DynamicQuant) {
                DataCopyExtParams scalesCopyParams{1U, static_cast<uint32_t>(dynamicScalesLocalIdx * sizeof(float)), 0U, 0U, 0U};
                DataCopyPad(dynamicScalesOutGMTensor_[currTokensOffset], dynamicScalesTensor, scalesCopyParams);
            }
        }
    }
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::CleanUpFlags()
{
    if (aivId_ == 0) {
        Duplicate<int32_t>(statusTensor_, 0, worldSize_ * DATA_OFFSET);
        SyncFunc<AscendC::HardEvent::V_MTE3>();
        uint32_t dstStrideU32 = dataSizePerRank_ - DATA_OFFSET;
        auto copyStatusParams = DataCopyExtParams{static_cast<uint16_t>(worldSize_), DATA_OFFSET, 0, dstStrideU32, 0};
        DataCopyPad(windowInstatusTensor_, statusTensor_, copyStatusParams);
    }
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::CopyPerformanceInfo()
{
    if (unlikely(needPerformanceInfo_)) {
        AscendC::SetAtomicAdd<int32_t>();
        AscendC::DataCopy(performanceInfoI32GMTensor_, performanceInfoI32Tensor_, performanceInfoSize_ * sizeof(int64_t) / sizeof(int32_t));
        AscendC::SetAtomicNone();
        PipeBarrier<PIPE_ALL>();
    }
}

template <TemplateMC2TypeA2Class>
__aicore__ inline void MoeDistributeDispatchA2<TemplateMC2TypeA2Func>::Process()
{
    if ASCEND_IS_AIV {
        AllocTensor();
        CalValidTokenCount();
        IndexSort();
        ReorderTokens();
        ConstructBatchWriteInfo();
        SyncAll<true>();
        SendToMoeExpert();
        WaitDispatch();
        CopyPerformanceInfo(); // 避免performanceInfoI32Tensor_被复用，提前搬出性能打点数据
        GetStatusCumSum();
        LocalWindowCopy();
        SyncAll<true>();
        CleanUpFlags();
        hccl_.Finalize();
    }
}
} // MoeDistributeDispatchA2Impl
#endif // MOE_DISTRIBUTE_DISPATCH_A2_H
